{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db0cc1d0-620d-491b-a2c8-f24d3b84e25a",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Tomographic reconstruction in four dimensions\"\n",
    "date: 2021-10-16\n",
    "author: Austin Hoover\n",
    "categories: \n",
    "  - tomography\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc8f28f-1c45-45a3-9970-3099cfff410f",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddcdab5-7858-4122-94e8-b7c55b7d98cb",
   "metadata": {},
   "source": [
    "A beam of particles in an accelerator is characterized by its distribution in phase space — the space of positions $\\{x, y, z\\}$ and momenta $\\{x', y', z'\\}$. I'm currently working on a project in which we'd like to measure the four-dimensional (4D) phase space distribution $f(x, x', y, y')$ of a fully-accumulated beam in the Spallation Neutron Source (SNS). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ae237d-b3f4-4c41-8bb1-6316af404e78",
   "metadata": {},
   "source": [
    "A direct way to do this is to use a series of slits to block all particles outside a small region $\\mathbf{x} \\pm \\Delta\\mathbf{x}$, where $\\mathbf{x} = (x, x', y, y')$, and move $\\mathbf{x}$ through phase space. This method is accurate but slow and not available for our beam ($10^{14}$ protons moving at 90% the speed of light). The best we can do is to measure projections of the distribution; i.e., lower-dimensional views such as\n",
    "\n",
    "$$\n",
    "f(x) = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}{f(x, x', y, y') dx' dy dy'}\n",
    "$$\n",
    "\n",
    "and estimate the distribution from these projections. This is called *tomographic reconstruction*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8449b95c-069f-42b3-80df-d8207c613a50",
   "metadata": {},
   "source": [
    "Tomographic reconstruction is used in a wide variety of fields, particularly in medical imaging where X-rays are used to generate 1D projections of a 2D slice of organic material at various angles. The same idea can be applied to a phase space distribution. Consider $f(x, x')$. It's straightforward to measure a 1D projection along the $x$ axis by sweeping a vertical conducting wire across the beam path; the problem is that this only corresponds to one projection angle. The trick is to approximate motion in an accelerator as a series of linear transformations of the phase space coordinates: shearing, scaling, and rotation. Thus, the projection of a beam at one location onto the $x$ axis is a scaled projection of a projection onto a rotated axis at a different location. The projection angle can be controlled by varying the electromagnetic fields between the two locations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1e8e5a-e1cb-4763-b57f-a033b1745ef9",
   "metadata": {},
   "source": [
    "It's more challenging to find $f(x, x', y, y')$. It's simple to reconstruct the $4 \\times 4$ covariance matrix from 1D projections; I've implemented this method in the SNS and won't discuss it here. To obtain the 4D *distribution*, we generally need at least 2D projections. I'm interested in this approach because the final destination of the SNS beam — the target — is coated with a luminescent material which gives the 2D projection onto the $x$-$y$ plane. The idea that I'll begin to explore in this post is whether these 2D projections can be used to reconstruct the 4D phase space distribution. I'll first go over the common reconstruction algorithms in 2D and mention if they could be extended to 4D."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9eec21-0847-4d0d-9631-6a3c2061f8ff",
   "metadata": {},
   "source": [
    "## 2D reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56242f42-12cf-470a-b158-126bc2598b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "import sys\n",
    "sys.path.append('/Users/austin/repo/psdist/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2516723f-a02b-46f5-a271-4d984dd368a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import proplot as pplt\n",
    "from skimage import transform\n",
    "import psdist.visualization as psv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02dff430-4f63-485b-a227-dcf23fe7df21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "pplt.rc['animation.html'] = 'jshtml'\n",
    "pplt.rc['grid'] = False\n",
    "pplt.rc['cmap.discrete'] = False\n",
    "pplt.rc['cmap.sequential'] = 'dusk_r'\n",
    "pplt.rc['figure.facecolor'] = 'white'\n",
    "pplt.rc['savefig.dpi'] = 'figure'\n",
    "savefig_kws = dict(dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3e9f3b-e3fa-49db-90b4-ff443acab2ac",
   "metadata": {},
   "source": [
    "I'll use the following distribution to test the reconstruction algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcd32160-3f7f-4250-aa07-f64451225d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1.75\n",
    "n = 300000\n",
    "X = np.vstack([\n",
    "    np.random.normal(scale=[1.0, 1.0], loc=[0.0, 0.0], size=(n, 2)),\n",
    "    np.random.normal(scale=0.6, loc=[+a, +a], size=(n//5, 2)),\n",
    "    np.random.normal(scale=0.6, loc=[+a, -a], size=(n//5, 2)),\n",
    "    np.random.normal(scale=0.6, loc=[-a, +a], size=(n//5, 2)),\n",
    "    np.random.normal(scale=0.6, loc=[-a, -a], size=(n//5, 2)),\n",
    "])\n",
    "\n",
    "n_bins = 60\n",
    "xmax = 5.0\n",
    "limits = [(-xmax, xmax), (-xmax, xmax)]\n",
    "\n",
    "Z_true, xedges, yedges = np.histogram2d(X[:, 0], X[:, 1], n_bins, limits, density=True)\n",
    "xcenters = 0.5 * (xedges[:-1] + xedges[1:])\n",
    "ycenters = 0.5 * (yedges[:-1] + yedges[1:])\n",
    "\n",
    "fig, ax = pplt.subplots()\n",
    "ax.pcolormesh(xcenters, ycenters, Z_true.T)\n",
    "ax.format(xlabel=\"x\", ylabel=\"x'\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5be1b498-f71b-4f71-828e-4b4bcfeb6d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "fig.save('_output_dist2d.png', dpi=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da91233f-8aab-4a47-9b63-e539161eee13",
   "metadata": {},
   "source": [
    "![](_output_dist2d.png){width=40% fig-align=center}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ef637d-81ef-4b65-a8b3-a29bfb501905",
   "metadata": {},
   "source": [
    "We now simulate the measurements. Assume we have the ability to rotate the distribution without any shearing or scaling. We then rotate the distribution by different angles and project it onto the $x$ axis. It will turn out that measurement time limits the number of projections we can use; for now, we'll limit ourselves to 15 projections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f110dd4-d299-4133-82fd-caaa17f8de99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation_matrix(angle):\n",
    "    cs, sn = np.cos(angle), np.sin(angle)\n",
    "    return np.array([[cs, sn], [-sn, cs]])\n",
    "\n",
    "\n",
    "n_proj = 15\n",
    "angles = np.linspace(0.0, 180.0, n_proj, endpoint=False)\n",
    "transfer_matrices = [rotation_matrix(np.radians(angle)) for angle in angles]\n",
    "projections = np.zeros((n_bins, n_proj))\n",
    "for k in range(n_proj):\n",
    "    M = transfer_matrices[k]\n",
    "    X_meas = np.apply_along_axis(lambda row: np.matmul(M, row), 1, X)\n",
    "    projections[:, k], _ = np.histogram(X_meas[:, 0], n_bins, limits[0], density=True)\n",
    "\n",
    "fig, ax = pplt.subplots()\n",
    "ax.pcolormesh(projections.T)\n",
    "ax.format(xlabel=\"Projection axis\", ylabel=\"Projection number\", ytickminor=False)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfe4316b-89b9-422e-82d7-6de36d8a1ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "fig.save('_output_sinogram.png', dpi=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ef898e-aee4-46ba-8377-7e17b424119a",
   "metadata": {},
   "source": [
    "![](_output_sinogram.png){width=40% fig-align=center}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aab19f2-0974-4ef7-8aac-b157f21d2ae5",
   "metadata": {},
   "source": [
    "We'll also need some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6628ff4-6b44-41cd-b372-12b888c4f6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply(M, X):\n",
    "    return np.apply_along_axis(lambda row: np.matmul(M, row), 1, X)\n",
    "\n",
    "\n",
    "def project(Z, indices):\n",
    "    if type(indices) is int:\n",
    "        indices = [indices]\n",
    "    axis = tuple([k for k in range(4) if k not in indices])\n",
    "    return np.sum(Z, axis=axis)\n",
    "\n",
    "\n",
    "def normalize(Z, bin_volume=1.0):\n",
    "    count = np.sum(Z)\n",
    "    if count == 0.0:\n",
    "        return Z\n",
    "    return Z / count / bin_volume\n",
    "\n",
    "\n",
    "def get_bin_volume(limits, n_bins):\n",
    "    if type(n_bins) is int:\n",
    "        n_bins = len(limits) * [n_bins]\n",
    "    return np.prod([((lim[1] - lim[0]) / n) for lim, n in zip(limits, n_bins)])\n",
    "\n",
    "\n",
    "def process(Z, keep_positive=False, density=False, limits=None):\n",
    "    if keep_positive:\n",
    "        Z = np.clip(Z, 0.0, None)\n",
    "    if density:\n",
    "        bin_volume = 1.0\n",
    "        if limits is not None:\n",
    "            bin_volume = get_bin_volume(limits, Z.shape)\n",
    "        Z = normalize(Z, bin_volume)\n",
    "    return Z\n",
    "\n",
    "\n",
    "def plot_rec(Z, Z_true, suptitle=\"\"):\n",
    "    fig, axs = pplt.subplots(ncols=3, figsize=(7.25, 2.5))\n",
    "    axs[0].pcolormesh(xcenters, ycenters, Z.T, cmap=\"dusk_r\")\n",
    "    axs[1].pcolormesh(xcenters, ycenters, Z_true.T, cmap=\"dusk_r\")\n",
    "    axs[2].pcolormesh(\n",
    "        xcenters,\n",
    "        ycenters,\n",
    "        (Z - Z_true).T,\n",
    "        colorbar=True,\n",
    "        colorbar_kw=dict(width=0.075, ticklabelsize=8),\n",
    "    )\n",
    "    axs.format(xticks=[], yticks=[], suptitle=suptitle)\n",
    "    for ax, title in zip(axs, [\"Reconstructed\", \"Original\", \"Error\"]):\n",
    "        ax.set_title(title)\n",
    "    return fig, axs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70db8b4e-1347-41ae-8949-58baf990ffa9",
   "metadata": {},
   "source": [
    "### Filtered Back-Projection (FPB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced3f6b4-8d4f-4107-bc0f-e17235f2acfc",
   "metadata": {},
   "source": [
    "In filtered back-projection (FBP) each projection is Fourier transformed and then smeared back across the $x$-$x'$ plane. The Fourier transform acts as a filter and creates a sharper image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d90b222-b8ec-40ba-9645-7692cbd6d232",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = transform.iradon(projections, theta=-angles).T\n",
    "Z = process(Z, keep_positive=True, density=True, limits=limits)\n",
    "fig, axs = plot_rec(Z, Z_true, 'FBP')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05b7cb86-ef16-4175-8c2c-99ad454ea890",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "fig.save('_output_FBP.png', dpi=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c842a8d2-ed81-47b8-87a7-b1424ae5ef5f",
   "metadata": {},
   "source": [
    "![](_output_FBP.png){width=100% fig-align=center}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2769cfaf-fe24-4799-bd8d-fc30042b6f77",
   "metadata": {},
   "source": [
    "FBP generally requires a high number of projections to avoid these streaking artifacts. Let's see what happens when the number of projections is varied. In each case, we distribute the angles evenly over 180 degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae122d92-7356-4cda-9f7d-55be6c5a81de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "#| output: false\n",
    "# image = np.copy(Z_true.T)\n",
    "\n",
    "# fig, ax = pplt.subplots(figsize=(3, 3))\n",
    "# ax.axis('off')\n",
    "# plt.close()\n",
    "\n",
    "# def update(frame):\n",
    "#     n = frame + 2\n",
    "#     theta = np.linspace(0., 180., n, endpoint=False)\n",
    "#     proj = transform.radon(image, theta, circle=False)\n",
    "#     image_rec = transform.iradon(proj, theta, circle=False)\n",
    "#     ax.imshow(image_rec, cmap='dusk_r')\n",
    "#     ax.set_title('Number of angles = {}'.format(n))\n",
    "\n",
    "# anim = animation.FuncAnimation(fig, update, frames=48, interval=(1000/5))\n",
    "# anim.save('_output_vary_n_angles.gif', dpi=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5593b4-96cf-4d71-9822-e1821dbd22bb",
   "metadata": {},
   "source": [
    "![](_output_vary_n_angles.gif){width=40% fig-align=center}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c719751-a8cc-44b8-bd7d-1dd72a5408cb",
   "metadata": {},
   "source": [
    "We were also careful to choose an angular range close to 180 degrees to maximize the information carried by the projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef53af42-7b46-4578-9ac7-a0b38c124f4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "#| output: false\n",
    "# angular_range = np.linspace(10., 180., 50)\n",
    "# n = 100\n",
    "\n",
    "# fig, ax = pplt.subplots(figsize=(3, 3))\n",
    "# ax.axis('off')\n",
    "# plt.close()\n",
    "\n",
    "# def update(i):\n",
    "#     theta = np.linspace(0., angular_range[i], n)\n",
    "#     proj = transform.radon(image, theta, circle=False)\n",
    "#     image_rec = transform.iradon(proj, theta, circle=False)\n",
    "#     ax.imshow(image_rec, cmap='dusk_r')\n",
    "#     ax.set_title(r'{} angles;  {:.2f}$\\degree$ range'.format(n, angular_range[i]))\n",
    "    \n",
    "# anim = animation.FuncAnimation(fig, update, frames=len(angular_range), interval=(1000/5))\n",
    "# anim.save('_output_vary_angular_range.gif', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd509feb-2bcd-4f50-8e09-7ac576094ac1",
   "metadata": {},
   "source": [
    "![](_output_vary_angular_range.gif){width=40% fig-align=center}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840c15b2-ee3f-430c-a364-c08bb1073573",
   "metadata": {},
   "source": [
    "The angular range in an accelerator is determined by the amount of control we have over the optics between the reconstruction location and the measurement location. It's possible to run into problems here; for example, varying a magnet too far from its design value could make the beam unacceptably large at a downstream location. Magnets also have limited strengths, and sometimes they can't be controlled independently. Additionally, achieving a specific projection angle is not always straightforward; usually an optimizer is used to find the correct settings, and this takes time. I'll come back to these points later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4381e4-8744-4dbf-8f04-4e51dbf040f8",
   "metadata": {},
   "source": [
    "FBP can be generalized to 3D reconstruction from 2D projections, but it doesn't seem straightforward. I don't know if it could work for a 4D reconstruction from 2D projections. I'm just not sure what back-projection would mean in that case..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab64b30-73f6-4749-85e7-b1ca92f4962e",
   "metadata": {},
   "source": [
    "### Algebraic Reconstruction (ART) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc685b25-d790-448f-9adb-7c260abfb16f",
   "metadata": {},
   "source": [
    "Algebraic reconstruction (ART) algorithms are simpler than FBP. The reconstructed $x$-$x'$ distribution will be defined on a grid. Let $\\rho^{(k)}$ be a vector containing the $k$th projection, and let $\\psi$ be a vector of the phase space density at the reconstruction location ($N^2$ elements for an $N \\times N$ grid). Since we know the linear transformation connecting the two points, we can write the following matrix equation:\n",
    "\n",
    "$$ \\rho^{(k)} = P^{(k)}\\psi, $$\n",
    "\n",
    "Stack these equations for all the projections to get\n",
    "\n",
    "$$ \\rho = P \\psi. $$\n",
    "\n",
    "All we need to do is invert these equations. The problem is that $P$ could be huge. If there are $K$ projections, $P$ will be have $K N \\times N^2$ elements. Inverting such a matrix could be a pain, so people have developed iterative methods to find the answer. Scikit-image has one ready to go called simultaneous algebraic reconstruction (SART)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29546c4d-4a27-4a56-87d2-2373582d07f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = transform.iradon_sart(projections, theta=-angles).T\n",
    "Z = process(Z, keep_positive=True, density=True, limits=limits)\n",
    "fig, axs = plot_rec(Z, Z_true, 'SART (1 iteration)')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c470405e-f13a-4c8b-921f-f13c8c789fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "fig.save('_output_SART.png', dpi=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45ff1f6-5524-4cb4-b6b7-153285070bbc",
   "metadata": {},
   "source": [
    "![](_output_SART.png){width=100% fig-align=center}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0bc5b5-f026-4374-a319-52a8ec0fb9ec",
   "metadata": {},
   "source": [
    "The SART algorithm looks like it does better with fewer projections. SART is known to produce a good answer in only one iteration. Running it again with the original reconstruction as an initial estimate can sharpen image but may increase noise. In this case, it looks like running SART again leads to a better reconstruction without increasing noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5553c79a-27b6-40a5-923a-ae62ae31b78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = transform.iradon_sart(projections, theta=-angles).T\n",
    "Z = transform.iradon_sart(projections, theta=-angles, image=Z.T).T\n",
    "Z = process(Z, keep_positive=True, density=True, limits=limits)\n",
    "fig, axs = plot_rec(Z, Z_true, 'SART (2 iterations)')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6f9fbc-8d35-44ee-aad6-791bc2783a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "fig.save('_output_SART2.png', dpi=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba6495d-0162-4f86-8b46-916f84008e5a",
   "metadata": {},
   "source": [
    "![](_output_SART2.png){width=100% fig-align=center}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e64ae1-d227-4947-ba7e-df926bb324ad",
   "metadata": {},
   "source": [
    "ART easily generalizes to 4D, but it could become expensive since $P$ would have $K N^2 \\times N^4$ elements. This has been carried out by Wolski (2020) with $N = 69$. I'll leave investigation of 4D ART for a future post."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de08531-3f55-4757-af15-255de68adb50",
   "metadata": {},
   "source": [
    "### MENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f094681-41ad-49e4-acfb-c7e1e41a2fa0",
   "metadata": {},
   "source": [
    "There is another approach to the problem based on information theory. The idea is that the most probable distribution should be chosen, i.e., find the $f(\\mathbf{x})$ that maximizes the entropy\n",
    "\n",
    "$$ S = -\\int f(\\mathbf{x}) \\log{f(\\mathbf{x})} d\\mathbf{x}$$\n",
    "\n",
    "with the constraint that the projections of $f(\\mathbf{x}$ are consistent with measurements. The theory behind this algorithm is layed out in Minerbo (1972). The benefit to MENT is that it has a goal in mind and can do very well with only a few projections where ART might introduce significant streaking artifacts. The downside of MENT is that although it *can* work well with few projections, there is no guarantee, and it may struggle to converge when many projections are used. Unfortunately, this algorithm is a bit tricky to implement; I'm working on it.  \n",
    "\n",
    "MENT can also work in 4D with 2D projections *or 1D projections*; the math is pretty much the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9847c2-d0bc-4b32-a39f-3299c8f3556a",
   "metadata": {},
   "source": [
    "### Grid-based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f0f049-e44a-46c0-a51d-0339e84ab66e",
   "metadata": {},
   "source": [
    "There is a final method that I should mention: using multi-particle simulation to find the answer. Start with a bunch of particles and transport them to the measurement location. Throw away particles that landed in bins in which the measured projection is zero. Now, generate new particles in the viscinity of the old ones with the number of new particles proportional to the measured distribution in that bin. Repeat until convergence.\n",
    "\n",
    "The advantage of this approach is that it works for any number of dimesions and can also include nonlinear effects in the simulation such as space charge. I'm wondering, though, if this just reduces to a version of ART if the transport is linear; our beam is high energy, which means that space charge can mostly be ignored over short distances. Again, I'll leave this method for a future post."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29bbec4-acf4-4876-aaf7-bab55c19f641",
   "metadata": {},
   "source": [
    "## 4D reconstruction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df68755-646b-4271-beb2-35612c0d92c9",
   "metadata": {},
   "source": [
    "I've mentioned that at least three methods (ART, MENT, grid-based) will generalize to 4D reconstruction from 2D projections. But what I'd like to look at now is a different, hopefully easier method described in Hock (2013). The method uses 2D projections but only uses 2D reconstruction methods like those described in this post. It's sort of like a CT scan: 1D projections are used to reconstruct a 2D slice of a human body, and then the slice is moved along the body to reconstruct the 3D image. This would be nice because the conditions for a good 2D reconstruction are well-understood, but might not be clear for higher-dimensional versions of the algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9195dd11-b3f8-4ca1-8675-e9c41cc95bc0",
   "metadata": {},
   "source": [
    "First, we need a 4D distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38a29458-445b-4f6c-80b8-1869b24a41c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a rigid rotating distribution.\n",
    "X = np.random.normal(size=(400000, 4))\n",
    "X = np.apply_along_axis(lambda row: row / np.linalg.norm(row), 1, X)\n",
    "X[:, 3] = +X[:, 0]\n",
    "X[:, 1] = -X[:, 2]\n",
    "\n",
    "# Change the x-y phase difference.\n",
    "R = np.zeros((4, 4))\n",
    "R[:2, :2] = rotation_matrix(np.pi / 4)\n",
    "R[2:, 2:] = rotation_matrix(0.0)\n",
    "X = np.apply_along_axis(lambda row: np.matmul(R, row), 1, X)\n",
    "\n",
    "# Add some noise.\n",
    "X += np.random.normal(scale=0.4, size=X.shape)\n",
    "\n",
    "# Plot the 2D projections.\n",
    "n_bins = 80\n",
    "labels = [\"x\", \"x'\", \"y\", \"y'\"]\n",
    "g = psv.cloud.corner(\n",
    "    X, bins=n_bins, grid_kws=dict(figwidth=5.5), mask=False,\n",
    "    labels=labels,\n",
    ")\n",
    "limits = [ax.get_xlim() for ax in g.axs[-1, :]]\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74e7f188-d5ec-48b8-81b0-d813c5995e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "#| output: false\n",
    "g.fig.save('_output_corner.png', dpi=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d2f4dd-e814-4778-a394-97e2aca1aeb3",
   "metadata": {},
   "source": [
    "![](_output_corner.png){width=75% fig-align=center}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87767bc2-caed-47d7-bc13-9e7714a1aa5c",
   "metadata": {},
   "source": [
    "We can estimate the distribution by binning the particles on a 4D grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d950960a-741b-4c2c-9101-baf2ed0579ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_true, edges = np.histogramdd(X, n_bins, limits, density=True)\n",
    "centers = []\n",
    "for _edges in edges:\n",
    "    centers.append(0.5 * (_edges[:-1] + _edges[1:]))\n",
    "bin_volume = get_bin_volume(limits, n_bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59138130-0cc8-46b5-9aed-f0e4bf3fe3ac",
   "metadata": {},
   "source": [
    "### Description of Hock's method "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b77fb0f-1293-4a41-9048-5bd223bfdd7f",
   "metadata": {},
   "source": [
    "Now for the method. Suppose we can independently rotate the $x$-$x'$ and $y$-$y'$ projections of the phase space distribution. Let the angles in $x$-$x'$ be $\\left\\{\\mu_{x_1}, \\dots, \\mu_{x_k}, \\dots, \\mu_{x_K}\\right\\}$ and the angles in $y$-$y'$ be $\\left\\{\\mu_{y_1}, \\dots, \\mu_{y_l}, \\dots, \\mu_{y_L}\\right\\}$. For each combination of $\\mu_x$ and $\\mu_y$, we measure the beam intensity on a screen. In other words, we create a matrix $S$ such that $S_{ijkl}$ gives the intensity at point $(x_i, y_j)$ on the screen for phase advances (angles) $\\mu_{x_k}$ and $\\mu_{y_l}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "344a17c1-077b-4220-a475-d64d2739132b",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 15 # number of angles in x dimension\n",
    "L = 15 # number of angles in y dimension\n",
    "muxx = muyy = np.linspace(0., 180., K, endpoint=False)\n",
    "\n",
    "xx_list = []\n",
    "for mux in muxx:\n",
    "    Mx = rotation_matrix(np.radians(mux))\n",
    "    xx_list.append(apply(Mx, X[:, :2])[:, 0])\n",
    "    \n",
    "yy_list = []\n",
    "for muy in muyy:\n",
    "    My = rotation_matrix(np.radians(muy))\n",
    "    yy_list.append(apply(My, X[:, 2:])[:, 0])\n",
    "    \n",
    "S = np.zeros((n_bins, n_bins, K, L))\n",
    "for k, xx in enumerate(xx_list):\n",
    "    for l, yy in enumerate(yy_list):\n",
    "        S[:, :, k, l], _, _ = np.histogram2d(xx, yy, n_bins, (limits[0], limits[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "030a21d1-ffed-42be-91bd-05af1ac9a714",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "# fig, ax = pplt.subplots(figsize=(3, 3))\n",
    "# ax.format(xlabel=\"x\", ylabel=\"y\", xticks=[], yticks=[])\n",
    "# plt.close()\n",
    "\n",
    "# def update(frame):\n",
    "#     k = frame // 15\n",
    "#     l = frame % 15\n",
    "#     mux = np.degrees(muxx[k])\n",
    "#     muy = np.degrees(muyy[l])\n",
    "#     dist = S[:, :, k, l]\n",
    "#     ax.pcolormesh(centers[0], centers[2], dist)\n",
    "#     ax.set_title(r'$\\mu_x = {:.2f}\\degree$   $\\mu_y = {:.2f} \\degree$'.format(mux, muy))\n",
    "    \n",
    "# frames = 3 * L - 1\n",
    "# fps = 7.5\n",
    "# anim = animation.FuncAnimation(fig, update, frames=frames, interval=(1000.0/fps))\n",
    "# anim.save('figures/target_phase_scan.gif', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a100e5-b1c6-45a3-bf53-9aebbf80ef89",
   "metadata": {},
   "source": [
    "![](_output_target_phase_scan.gif){width=40% fig-align=center}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c01c2c5-c24b-420e-9499-c97c40838054",
   "metadata": {},
   "source": [
    "The above animation shows a few steps in the scan; since the transfer matrices are rotation matrices, only the cross-plane correlation changes.\n",
    "\n",
    "We can immediately reconstruct the 3D projection the 4D phase space distribution using this data. Consider one row of the beam image — the intensity along the row gives a 1D projection onto the $x$ axis for a vertical slice of the beam. We have a set of these 1D projections at different $\\mu_k$ which we can use to reconstruct the $x$-$x'$ distribution at this vertical slice using any 1D $\\rightarrow$ 2D reconstruction method. I'll use SART since this seemed to work well with 15 projections. We repeat this at each slice. We thus have an array $D$ such that $D_{j,l,r,s}$ gives the density at $x = x_r$, $x' = x'_s$ for $y = y_j$ and $\\mu_y = \\mu_{y_l}$.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "961e753f-ce76-4f02-8bb2-f1b1dd409a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.zeros((n_bins, L, n_bins, n_bins))\n",
    "for j in range(n_bins):\n",
    "    for l in range(L):\n",
    "        _Z = transform.iradon_sart(S[:, j, :, l], theta=-muxx).T\n",
    "        _Z = transform.iradon_sart(S[:, j, :, l], theta=-muxx, image=_Z.T).T\n",
    "        D[j, l, :, :] =_Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d66274-6797-4656-8ecc-f16c95035058",
   "metadata": {},
   "source": [
    "I should also mention that the reconstruction grid doesn't need to be the same size as the measurement grid. We can now do a similar thing in the vertical plane. For each bin in the reconstructed x-x' grid, `D[:, :, r, s]` gives the projections of $y$-$y'$ at each $\\mu_{y_l}$; thus, $y$-$y'$ can be reconstructed at each $x_r$ and $x'_s$, and we have an array $Z$ such that $Z_{r,s,t,u}$ gives the density at $x = x_r$, $x' = x'_s$, $y = y_t$, $y' = y'_u$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13dd09c1-3939-4ff7-a633-6b53cf3ab175",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.zeros((n_bins, n_bins, n_bins, n_bins))\n",
    "for r in range(n_bins):\n",
    "    for s in range(n_bins):\n",
    "        _Z = transform.iradon_sart(D[:, :, r, s], theta=-muyy).T\n",
    "        _Z = transform.iradon_sart(D[:, :, r, s], theta=-muyy, image=_Z.T).T\n",
    "        Z[r, s, :, :] = _Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c60c2f9-0066-4c99-ab00-000773a8777f",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042bb15f-73a5-4875-8425-c131694e0383",
   "metadata": {},
   "source": [
    "SART could make some bins negative, so set those to zero. We then need to normalize `Z` for comparison with `Z_true`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d2db923-5017-4f14-89d9-a10958ba81f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min(Z) = 0.0\n",
      "max(Z) = 0.17036653561113582\n",
      "sum(Z) * bin_volume = 0.9999999999999999\n",
      "\n",
      "min(Z_true) = 0.0\n",
      "max(Z_true) = 1.3628864309355835\n",
      "sum(Z_true) * bin_volume = 0.9999999999999879\n"
     ]
    }
   ],
   "source": [
    "Z = process(Z, keep_positive=True, density=True, limits=limits)\n",
    "print('min(Z) = {}'.format(np.min(Z)))\n",
    "print('max(Z) = {}'.format(np.max(Z)))\n",
    "print('sum(Z) * bin_volume = {}'.format(np.sum(Z) * bin_volume))\n",
    "print()\n",
    "print('min(Z_true) = {}'.format(np.min(Z_true)))\n",
    "print('max(Z_true) = {}'.format(np.max(Z_true)))\n",
    "print('sum(Z_true) * bin_volume = {}'.format(np.sum(Z_true) * bin_volume))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eaea03-3c77-4066-b25a-ca9a9e6d6b74",
   "metadata": {},
   "source": [
    "I'm not sure the best way to quantify the difference between the distributions. My initial thought is to subtract `Z_true` from `Z`, take the absolute value, and divide by the number of bins; this would give the average absolute error over the bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82664ac4-65c6-41ec-b78c-7f60f69d6ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average absolute error per bin = 0.002540311656083561\n"
     ]
    }
   ],
   "source": [
    "avg_abs_err_per_bin = np.sum(np.abs(Z - Z_true)) / Z.size\n",
    "print('Average absolute error per bin = {}'.format(avg_abs_err_per_bin))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382c698a-ebdf-46fb-942d-f6db5ffd94e1",
   "metadata": {},
   "source": [
    "Lots of bins are empty so this might not be the most meaningful number. For now, it's probably more helpful to view the differences between the projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5963d695-f562-485b-9f9c-134293504176",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "#| code-fold: true\n",
    "fig, axs = pplt.subplots(ncols=4, figsize=(6, 2), spanx=False)\n",
    "for i in range(4):\n",
    "    axs[i].plot(centers[i], project(Z_true, i), color='black', label='True')\n",
    "    axs[i].plot(centers[i], project(Z, i), color='red8', ls='dotted', label='Reconstructed')\n",
    "    axs[i].set_xlabel(labels[i])\n",
    "axs[0].legend(loc=(0.0, 1.02), framealpha=0.0, ncol=1)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "693ea27e-7aeb-4f21-896c-54a9597a5aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "fig.save('_output_compare_1d.png', dpi=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6771ef1-e488-4b7b-9239-788eae001433",
   "metadata": {},
   "source": [
    "![](_output_compare_1d.png){fig-align=center width=85%}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a490aa65-1fb1-46b4-8b6a-28756da177f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "#| code-fold: true\n",
    "indices = [(0, 1), (2, 3), (0, 2), (0, 3), (2, 1), (1, 3)]\n",
    "fig, axs = pplt.subplots(nrows=6, ncols=3, figwidth=5.0, sharex=False, sharey=False)\n",
    "for row, (i, j) in enumerate(indices):\n",
    "    _Z_true = project(Z_true, [i, j])\n",
    "    _Z = project(Z, [i, j])\n",
    "    axs[row, 0].pcolormesh(centers[i], centers[j], _Z.T)\n",
    "    axs[row, 1].pcolormesh(centers[i], centers[j], _Z_true.T)\n",
    "    axs[row, 2].pcolormesh(\n",
    "        centers[i],\n",
    "        centers[j],\n",
    "        (_Z - _Z_true).T,\n",
    "        colorbar=True,\n",
    "        colorbar_kw=dict(width=0.075),\n",
    "    )\n",
    "    axs[row, 0].annotate(\n",
    "        \"{}-{}\".format(labels[i], labels[j]),\n",
    "        xy=(0.02, 0.92),\n",
    "        xycoords=\"axes fraction\",\n",
    "        color=\"white\",\n",
    "    )\n",
    "for ax, title in zip(axs[0, :], [\"Reconstructed\", \"True\", \"Error\"]):\n",
    "    ax.set_title(title)\n",
    "axs.format(xticks=[], yticks=[])\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "76a6b0dc-bf5d-4c86-a656-29339daca928",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "fig.save('_output_compare_2d.png', dpi=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18be2f61-a802-4011-b9d1-6cf00cce0a31",
   "metadata": {},
   "source": [
    "![](_output_compare_2d.png){fig-align=center width=65%}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aaad38-741e-4a4f-942c-cd697a4b71f0",
   "metadata": {},
   "source": [
    "In the second plot, the error is given in number of particles. It looks like the method worked well to reconstruct this distribution. [Edit 2023-02-13: This is not a sufficient comparison in 4D: one needs to look at slices, and use more complex distribution.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d3db8c-0d7e-4274-860d-b2340b479ef5",
   "metadata": {},
   "source": [
    "## Feasibility "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b1d93f-8802-4a04-8cd6-e2bf65f66dc5",
   "metadata": {},
   "source": [
    "### Imaging system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567c9d49-0620-4ff9-976d-0a54637a10c5",
   "metadata": {},
   "source": [
    "The SNS target is a steel vessel containing liquid mercury. At the beginning of this post, I mentioned that the SNS has a target imaging system. Here is a diagram showing how the target imaging system works..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404addc9-202d-4f75-9031-877c53bd5c77",
   "metadata": {},
   "source": [
    "![](target_image_system.png){fig-align=center width=80%}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a13d9e1-ba2b-4fa0-a52b-65c21085b314",
   "metadata": {},
   "source": [
    "... and an example of an image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a6d0fb-ea0a-41bd-bee0-331f71208b9b",
   "metadata": {},
   "source": [
    "![](target_image.png){fig-align=center width=70%}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7fc324-59fb-4589-8592-15ce1939cbfe",
   "metadata": {},
   "source": [
    "To reduce noise, the image was averaged over five beam pulses and a Gaussian blur was applied with $\\sigma = 3$. There are four fiducial markers that are visible as dark spots on the corner of the beam. The black ellipse represents a measurement from wire-scanners upstream of the target. Collecting this image is easy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6df81e-7e58-4d51-9d8e-db9866d5730a",
   "metadata": {},
   "source": [
    "### Optics control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e7446d-f7de-40e1-951f-8a01553cd032",
   "metadata": {},
   "source": [
    "I've assumed that the transfer matrices connecting the coordinates are simple rotation matrices. Although that isn't true in reality, there is a trick we can play. Any transfer matrix $M$ can be writen as the product $M = V R V^{-1}$, where $V$ provides shearing + scaling and $R$ is a rotation by the phase advances $\\mu_x$ and $\\mu_y$. Applying $V^{-1}$ to the coordinates is called *normalizing* the coordinates. What we can do is normalize (scale) the measured profiles, perform the reconstruction in normalized phase space, then unnormalize the reconstructed distribution. \n",
    "\n",
    "In normalized phase space, the transfer matrices are just rotation matrices ($R$) and the projection angles are the phase advances. This is an advantage because the phase advances are easier to control than the true projection angles. Also, the reconstructed $x$-$x'$ and $y$-$y'$ projections will be somewhat circular in this space, which should reduce errors.\n",
    "\n",
    "The (un)normalization steps will involve interpolation since we're performing a transformation on an image, and I'm not yet sure how this will affect the reconstruction error. Anyhow, we don't *have* to reconstruct in normalized phase space if this turns out to be an issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e4f13e-be1c-40de-b174-3eb0e4d85e8b",
   "metadata": {},
   "source": [
    "So, the question is whether we can independently vary the $x$ and $y$ phase advances at the target, and if so, by how much. There are five independent quadrupoles before the target — three focusing and two defocusing — QH26, QV27, QH28, QV29 and QH30. Before that, there are eight quadrupoles — four focusing and four defocusing — that share two powers supplies. One power supply controls QH18, QH20, QH22, and QH24, and another controls QV19, QV21, QV23, and QV25. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc553ce-34c7-4de8-883c-7e40a342face",
   "metadata": {},
   "source": [
    "There are also a constraints. First, the beam size far away from the target should remain small. We measure the beam size using the $\\beta$ function, which should remain below 35 [m/rad]. The beam becomes much larger close to the target; there, the $\\beta$ function should remain below 100 [m/rad]. Finally, the SNS target has tight constraints on the beam size and the beam density. It's safest to keep the $\\beta$ functions within 10 or 20 percent of the design value.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fd830f-cef4-4434-9b06-a10a9e8267b6",
   "metadata": {},
   "source": [
    "I plugged all this into an optimizer and asked to vary the phase advances in a 180 degree range. After fideling with the starting and ending phases, I was able to more-or-less do this. I used 15 phase advances in $x$ and 15 phase advances in $y$, just like in this post. Each optimization took around 16 seconds, so the script took around an hour to run. Below are the computed phase advances at each step as well as the strengths of the seven magnet power supplies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2270144-3345-466c-bea7-179c3bfbfaa7",
   "metadata": {},
   "source": [
    "![](target_phase_scan1.png){fig-align=center width=75%}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2291cbd-b221-4bfd-b8fa-f7ca141e15d0",
   "metadata": {},
   "source": [
    "There are a few steps near the end of the scan in which the phase advances aren't exactly correct, but they are close. I can always decrease the phase coverage slighly, or maybe these steps just need a slightly different initial guess. Here are the beta functions for all steps in one plot. The plot ends at the target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8735d66c-b95d-405f-abb5-069ee197db2f",
   "metadata": {},
   "source": [
    "![](target_phase_scan2.png){fig-align=center width=75%}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3473eaf5-7ce3-4c27-857d-9e2173ccbcc1",
   "metadata": {},
   "source": [
    "The bottom plot is just the integral of the inverse of the top plot. The beta functions remain within their limits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e3632c-1240-4cd5-aa67-8a0247d9ad16",
   "metadata": {},
   "source": [
    "### Outlook "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7c2424-a41a-4f7b-b25c-9d706858db1e",
   "metadata": {},
   "source": [
    "Initial tests of Hock's 4D reconstruction method are promising. There are a number of things to investigate. Ideally, the minimum number of projections is used. First, I should play with the number of projections using ART to see how the reconstruction error scales. I would also like to implement MENT, which should do better with fewer projections. I should then test the \"direct\" 4D reconstruction methods such as ART, MENT, or grid-based and compare them to Hock's method. I should also investigate how the methods respond to noise, changes in the grid size, and more complex distributions.\n",
    "\n",
    "The script to perform the data collection for this method is pretty simple: compute the correct optics, update the live magnet strengths accordingly, send a beam to the target, and save the target image. The biggest issue is execution time. In our next study at the SNS, we'll have plenty of time to run the script in addition to our other tasks. Hopefully this will be a helpful diagnostic."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
