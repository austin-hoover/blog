<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.550">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Austin Hoover">
<meta name="dcterms.date" content="2021-04-29">

<title>Accelerated Inquiry - Authorship identification</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-DLC4C8LZFB"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-DLC4C8LZFB', { 'anonymize_ip': true});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Accelerated Inquiry</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about/index.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../archive.html"> 
<span class="menu-text">Posts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../notes/index.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#a-brief-history-of-stylometry" id="toc-a-brief-history-of-stylometry" class="nav-link active" data-scroll-target="#a-brief-history-of-stylometry">1. A brief history of stylometry</a></li>
  <li><a href="#support-vector-machine-svm" id="toc-support-vector-machine-svm" class="nav-link" data-scroll-target="#support-vector-machine-svm">2. Support Vector Machine (SVM)</a>
  <ul>
  <li><a href="#maximum-margin-hyperplane" id="toc-maximum-margin-hyperplane" class="nav-link" data-scroll-target="#maximum-margin-hyperplane">2.1. Maximum margin hyperplane</a></li>
  <li><a href="#kernel-trick" id="toc-kernel-trick" class="nav-link" data-scroll-target="#kernel-trick">2.2. Kernel trick</a></li>
  <li><a href="#multi-class-svm" id="toc-multi-class-svm" class="nav-link" data-scroll-target="#multi-class-svm">2.3. Multi-class SVM</a></li>
  </ul></li>
  <li><a href="#n-grams-and-feature-selection-methods" id="toc-n-grams-and-feature-selection-methods" class="nav-link" data-scroll-target="#n-grams-and-feature-selection-methods">3. N-grams and feature selection methods</a>
  <ul>
  <li><a href="#data-set-description" id="toc-data-set-description" class="nav-link" data-scroll-target="#data-set-description">3.1. Data set description</a></li>
  <li><a href="#n-grams" id="toc-n-grams" class="nav-link" data-scroll-target="#n-grams">3.2. N-grams</a></li>
  <li><a href="#feature-selection" id="toc-feature-selection" class="nav-link" data-scroll-target="#feature-selection">3.3. Feature selection</a>
  <ul>
  <li><a href="#information-gain" id="toc-information-gain" class="nav-link" data-scroll-target="#information-gain">3.3.1. Information gain</a></li>
  <li><a href="#localmaxs-algorithm" id="toc-localmaxs-algorithm" class="nav-link" data-scroll-target="#localmaxs-algorithm">3.3.2. LocalMaxs algorithm</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">4. Conclusion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Authorship identification</h1>
  </div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Austin Hoover </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 29, 2021</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>In this post, I’ll summarize a paper by John Houvardas and Efstathios Stamatatos titled <em>N-Gram Feature Selection for Authorship Identification</em><span class="citation" data-cites="Houvardas2006">&nbsp;[<a href="#ref-Houvardas2006" role="doc-biblioref">1</a>]</span>. The topic of the paper is <em>authorship identification</em>, that is, to identify the author of an unlabeled document given a list of possible authors and some sample of each author’s writing. I’ll first motivate the problem of authorship identification, then briefly introduce the relevant statistical methods, and finally, summarize and implement the methods in the paper.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="task.png" class="lightbox" data-glightbox="description: .lightbox-desc-1" data-gallery="quarto-lightbox-gallery-1" title="Fig. 1. The authorship identification task."><img src="task.png" class="img-fluid figure-img" alt="Fig. 1. The authorship identification task."></a></p>
<figcaption>Fig. 1. The authorship identification task.</figcaption>
</figure>
</div>
<section id="a-brief-history-of-stylometry" class="level2">
<h2 class="anchored" data-anchor-id="a-brief-history-of-stylometry">1. A brief history of stylometry</h2>
<p>The Federalist Papers are an important collection of 85 essays written by Hamilton, Madison, and Jay in 1787 and 1788. The essays were published under the alias “Plubious”, and although it became well-known that the three men were involved, the authorship of each paper was kept hidden for over a decade. This was actually in the interest of both Hamilton and Madison; both were politicians who had changed positions on several issues and didn’t want their political opponents to use their own words against them. Days before his death, Hamilton allegedly wrote down who he believed to be the correct author of each essay, claiming over 60 for himself. Madison waited several years before publishing his own list, and in the end, there were 12 essays claimed by both Madison and Hamilton. Details on the controversy are in<span class="citation" data-cites="Adair1944">&nbsp;[<a href="#ref-Adair1944" role="doc-biblioref">2</a>]</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Hamilton_and_Madison.png" class="lightbox" data-glightbox="description: .lightbox-desc-2" data-gallery="quarto-lightbox-gallery-2" title="Fig. 2. Alexander Hamilton (left) and James Madison (right). (Source: Wikipedia.)"><img src="Hamilton_and_Madison.png" class="img-fluid figure-img" style="width:75.0%" alt="Fig. 2. Alexander Hamilton (left) and James Madison (right). (Source: Wikipedia.)"></a></p>
<figcaption>Fig. 2. Alexander Hamilton (left) and James Madison (right). (Source: Wikipedia.)</figcaption>
</figure>
</div>
<p>There are a few ways one might go about resolving this dispute. One approach is to analyze the actual <em>content</em> of the text. For example, perhaps an essay draws from a reference with which only Madison was intimately familiar, or maybe an essay is similar to Hamilton’s previous work. This approach was used many times over the next 150 years, but perhaps the final word on the subject was by Adair, who in 1944 concluded that Madison likely wrote all 12 essays.</p>
<p>An alternative approach is to analyze the <em>style</em> of the text. For example, maybe Madison used many more commas than Hamilton. The field of <em>stylometry</em> attempts to statistically quantify these stylistic differences. David Holmes writes the following about stylometry<span class="citation" data-cites="Holmes1998">&nbsp;[<a href="#ref-Holmes1998" role="doc-biblioref">3</a>]</span>:</p>
<blockquote class="blockquote">
<p><em>At its heart lies an assumption that authors have an unconscious aspect to their style, an aspect which cannot consciously be manipulated but which possesses features which are quantifiable and which may be distinctive.</em></p>
</blockquote>
<p>I think this is a valid assumption. The question is which features best characterize the author’s style and which methods are best to use in the analysis of these features. Let’s go back in time a bit to see how stylometry has developed over the past 150 years.</p>
<p>The physicist <a href="https://en.wikipedia.org/wiki/Thomas_Corwin_Mendenhall">Thomas Mendenhall</a> is considered the first to statistically analyze large literary texts. He presented the following idea in an 1887 paper titled <a href="https://www.jstor.org/stable/pdf/1764604.pdf"><em>The Characteristic Curves of Composition</em></a><span class="citation" data-cites="Mendenhall1887">&nbsp;[<a href="#ref-Mendenhall1887" role="doc-biblioref">4</a>]</span>: it is known that each chemical element emits light with a unique distribution of wavelengths when it is heated; perhaps each author has a unique distribution of word lengths in the texts they have written. It’s a cool idea, and I recommend reading his original paper. Mendenhall tallied word lengths by hand for various books, usually in batches of 1000 words or so. Here is Fig. 2 from his paper which shows the characteristic curves for a few excerpts of <em>Oliver Twist</em>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Mendenhall_Fig2.png" class="lightbox" data-glightbox="description: .lightbox-desc-3" data-gallery="quarto-lightbox-gallery-3" title="Fig. 3. Distribution of word lengths in “Oliver Twist”. Each curve is for a different sample of 1000 words. (Source:[@Mendenhall1887]."><img src="Mendenhall_Fig2.png" class="img-fluid figure-img" style="width:75.0%" alt="Fig. 3. Distribution of word lengths in “Oliver Twist”. Each curve is for a different sample of 1000 words. (Source:&nbsp;[4]."></a></p>
<figcaption>Fig. 3. Distribution of word lengths in “Oliver Twist”. Each curve is for a different sample of 1000 words. (Source:<span class="citation" data-cites="Mendenhall1887">&nbsp;[<a href="#ref-Mendenhall1887" role="doc-biblioref">4</a>]</span>.</figcaption>
</figure>
</div>
<p>Mendenhall showed that these curves reveal similarities between different works by the same author. The use of these statistics for authorship identification was left for future work.</p>
<p>The next significant advance in the statistical analysis of text was made by Zipf in 1932. Zipf found a relationship between an integer <span class="math inline">\(k\)</span> and the frequency <span class="math inline">\(f(k)\)</span> of the <span class="math inline">\(k\)</span>th most frequent word. This is often called a <em>rank-frequency</em> relationship, where <span class="math inline">\(k\)</span> is the rank. The scaling law can be written as</p>
<p><span id="eq-1"><span class="math display">\[
f(k) \propto k^{-1}.
\tag{1}\]</span></span></p>
<p>The idea expressed by this law is that short words are much more frequent than large words. Surprisingly, the law holds up very well, albeit not perfectly, for most texts. <em>Why</em> this is the case is still unknown; a comprehensive review of the current state of the law can be found in<span class="citation" data-cites="Piantadosi2014">&nbsp;[<a href="#ref-Piantadosi2014" role="doc-biblioref">5</a>]</span>. The law also shows up in other situations such as national GDP:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Cristelli_Fig1_big.png" class="lightbox" data-glightbox="description: .lightbox-desc-4" data-gallery="quarto-lightbox-gallery-4" title="Fig. 4. National GDPs appear to be moving toward the prediction by Zipf’s Law (red line). (Source:[@Cristelli2012].)"><img src="Cristelli_Fig1_big.png" class="img-fluid figure-img" style="width:75.0%" alt="Fig. 4. National GDPs appear to be moving toward the prediction by Zipf’s Law (red line). (Source:&nbsp;[6].)"></a></p>
<figcaption>Fig. 4. National GDPs appear to be moving toward the prediction by Zipf’s Law (red line). (Source:<span class="citation" data-cites="Cristelli2012">&nbsp;[<a href="#ref-Cristelli2012" role="doc-biblioref">6</a>]</span>.)</figcaption>
</figure>
</div>
<p>The success of Zipf’s Law was very encouraging and led to a flurry of new mathematical models. Stylometry reached a landmark case in the 1960s when researchers used the frequency distributions of short function words — words we don’t think about too much like “upon” or “therefore” — to support Adair’s conclusion that Madison wrote the 12 disputed Federalist Papers. At the end of the day, however, models created in the spirit of Zipf’s Law are probably doomed to fail. The “true” underlying model must be very complex due to its dependence on human psychology. There are now many algorithms available which instead build predictive models directly from data, and these can be readily applied to the problem of authorship identification. Here we focus on the use of the Support Vector Machine (SVM).</p>
</section>
<section id="support-vector-machine-svm" class="level2">
<h2 class="anchored" data-anchor-id="support-vector-machine-svm">2. Support Vector Machine (SVM)</h2>
<p>I include here the basic idea behind the SVM approach. There are a number of resources that go into the details (such as<span class="citation" data-cites="Bishop2006">&nbsp;[<a href="#ref-Bishop2006" role="doc-biblioref">7</a>]</span>). I’ll follow the Wikipedia page since it has a nice short summary.</p>
<section id="maximum-margin-hyperplane" class="level3">
<h3 class="anchored" data-anchor-id="maximum-margin-hyperplane">2.1. Maximum margin hyperplane</h3>
<p>Consider a linear binary classifier, i.e., a plane that splits the data into two classes. The equation for a plane in any number of dimensions is</p>
<p><span id="eq-2"><span class="math display">\[
y(\mathbf{x}) = \mathbf{w}^T\mathbf{x} + w_0 = 0
\tag{2}\]</span></span></p>
<p>This plane is called the <em>decision surface</em>; points are assigned to class 1 if <span class="math inline">\(y(\mathbf{x}) &gt; 0\)</span> or class 2 when <span class="math inline">\(y(\mathbf{x}) &lt; 0\)</span>. Suppose the data is linearly separable (able to be completely split in two) and that we’ve found a plane that correctly splits the data. We could then scale the coordinates such that all points with <span class="math inline">\(y(\mathbf{x}) \ge 1\)</span> belong to class 1 and all points with <span class="math inline">\(y(\mathbf{x}) \le -1\)</span> belong to class 2. The separating plane then sits in the middle as in the following figure.</p>
<p><br></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="svm.png" class="lightbox" data-glightbox="description: .lightbox-desc-5" data-gallery="quarto-lightbox-gallery-5" title="Fig. 5. Maximum margin separating plane. (Source: Wikipedia.)"><img src="svm.png" class="img-fluid figure-img" style="width:40.0%" alt="Fig. 5. Maximum margin separating plane. (Source: Wikipedia.)"></a></p>
<figcaption>Fig. 5. Maximum margin separating plane. (Source: Wikipedia.)</figcaption>
</figure>
</div>
<p><br></p>
<p>Notice that the plane could be rotated while still correctly splitting the existing data; the SVM attempts to find the optimal plane by maximizing the orthogonal distance from the decision plane to the closest point. This is known as the <em>margin</em>, and it can be shown that it is inversely proportional to the magnitude of <span class="math inline">\(\mathbf{w}\)</span>. Thus, the SVM tries to minimize <span class="math inline">\(|\mathbf{w}|^2\)</span> subject to the constraint that all points are correctly categorized. New data is then assigned based on this optimal boundary.</p>
<p>Some datasets won’t be linearly separable, in which case we can add a penalty function in order to minimize the number of miscategorized points. So, for <em>N</em> samples we minimize</p>
<p><span id="eq-3"><span class="math display">\[
\frac{1}{2}|\mathbf{w}|^2 +  C\sum_{i=1}^{N}{\max\left[0, 1 - {t_i y(\mathbf{x}_i)}\right]}.
\tag{3}\]</span></span></p>
<p>where <span class="math inline">\(t_i\)</span> is the true class of point <span class="math inline">\(i\)</span> (<span class="math inline">\(\pm 1\)</span>) and <span class="math inline">\(C\)</span> is a positive constant. Correctly classified points don’t contribute anything to the sum since <span class="math inline">\(t_i y(\mathbf{x}_i)\)</span> will be greater than or equal to one. Let’s try this on non-linearly separable data sampled from two Gaussian distributions in 2D space. The Python package <a href="https://scikit-learn.org/stable/modules/svm.html">scikit-learn</a> has a user-friendly interface for the SVM implementation in <a href="https://www.csie.ntu.edu.tw/~cjlin/liblinear/">LIBLINEAR</a> which we use here.</p>
<div id="cell-21" class="cell" data-tags="[]" data-execution_count="3">
<details class="code-fold">
<summary>Imports</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> collections</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> string</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> plotly <span class="im">import</span> graph_objects <span class="im">as</span> go</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.io <span class="im">as</span> pio</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> proplot <span class="im">as</span> pplt</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> norm</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn.metrics</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> svm</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-23" class="cell" data-tags="[]" data-execution_count="6">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> padded_ranges(X, pad<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    xmin, ymin <span class="op">=</span> np.<span class="bu">min</span>(X, axis<span class="op">=</span><span class="dv">0</span>) <span class="op">-</span> pad</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    xmax, ymax <span class="op">=</span> np.<span class="bu">max</span>(X, axis<span class="op">=</span><span class="dv">0</span>) <span class="op">+</span> pad</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (xmin, xmax), (ymin, ymax)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_dec_boundary(clf, ax<span class="op">=</span><span class="va">None</span>, xlim<span class="op">=</span>(<span class="op">-</span><span class="fl">100.0</span>, <span class="fl">100.0</span>), i<span class="op">=</span><span class="dv">0</span>, <span class="op">**</span>kws):</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    kws.setdefault(<span class="st">'c'</span>, <span class="st">'black'</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    w0 <span class="op">=</span> clf.intercept_ <span class="cf">if</span> <span class="bu">type</span>(clf.intercept_) <span class="kw">is</span> <span class="bu">float</span> <span class="cf">else</span> clf.intercept_[i]</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    (w1, w2) <span class="op">=</span> clf.coef_[i]</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    line_x <span class="op">=</span> np.array(xlim)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    line_y <span class="op">=</span> <span class="op">-</span>(w1 <span class="op">/</span> w2) <span class="op">*</span> line_x <span class="op">-</span> (w0 <span class="op">/</span> w2)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    ax.plot(line_x, line_y, <span class="op">**</span>kws)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_dec_regions(clf, xlim<span class="op">=</span><span class="va">None</span>, ylim<span class="op">=</span><span class="va">None</span>, ax<span class="op">=</span><span class="va">None</span>, nsteps<span class="op">=</span><span class="dv">500</span>, <span class="op">**</span>kws):</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    kws.setdefault(<span class="st">'alpha'</span>, <span class="fl">0.05</span>)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    kws.setdefault(<span class="st">'zorder'</span>, <span class="dv">0</span>)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    xx, yy <span class="op">=</span> np.meshgrid(</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>        np.linspace(xlim[<span class="dv">0</span>], xlim[<span class="dv">1</span>], nsteps), </span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>        np.linspace(ylim[<span class="dv">0</span>], ylim[<span class="dv">1</span>], nsteps),</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> np.c_[xx.ravel(), yy.ravel()]</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> clf.predict(Z)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>    zz <span class="op">=</span> y_pred.reshape(xx.shape)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    ax.contourf(xx, yy, zz, <span class="op">**</span>kws)</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate two Gaussian distributions</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.vstack([</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>    np.random.normal(size<span class="op">=</span>(n, <span class="dv">2</span>), loc<span class="op">=</span>[<span class="fl">0.0</span>, <span class="fl">0.0</span>], scale<span class="op">=</span><span class="fl">2.0</span>),</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>    np.random.normal(size<span class="op">=</span>(n, <span class="dv">2</span>), loc<span class="op">=</span>[<span class="fl">5.0</span>, <span class="fl">5.0</span>], scale<span class="op">=</span><span class="fl">2.5</span>)</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> n <span class="op">*</span> [<span class="dv">1</span>] <span class="op">+</span> n <span class="op">*</span> [<span class="op">-</span><span class="dv">1</span>] </span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Find SVM decision boundary </span></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> svm.LinearSVC(C<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>clf.fit(X, y)</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> pplt.subplots(xspineloc<span class="op">=</span><span class="st">'neither'</span>, yspineloc<span class="op">=</span><span class="st">'neither'</span>)</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>ax.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], s<span class="op">=</span><span class="dv">20</span>, alpha<span class="op">=</span><span class="fl">0.35</span>, c<span class="op">=</span>y, cmap<span class="op">=</span><span class="st">'PiYG_r'</span>)    </span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>xlim, ylim <span class="op">=</span> padded_ranges(X, pad<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>plot_dec_boundary(clf, ax<span class="op">=</span>ax)</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>plot_dec_regions(clf, xlim, ylim, ax<span class="op">=</span>ax, cmap<span class="op">=</span><span class="st">'PiYG_r'</span>)</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>ax.<span class="bu">format</span>(xlim<span class="op">=</span>xlim, ylim<span class="op">=</span>ylim)</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>ax.annotate(<span class="st">'Decision</span><span class="ch">\n</span><span class="st">boundary'</span>, xy<span class="op">=</span>(<span class="fl">0.55</span>, <span class="fl">0.02</span>), xycoords<span class="op">=</span><span class="st">'axes fraction'</span>)</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>plt.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./_output_dec_boundary.png" class="lightbox" data-glightbox="description: .lightbox-desc-6" data-gallery="quarto-lightbox-gallery-6" title="Fig. 6. An SVM decision boundary for a two-class data set. Each point is colored by its class."><img src="./_output_dec_boundary.png" class="img-fluid figure-img" style="width:40.0%" alt="Fig. 6. An SVM decision boundary for a two-class data set. Each point is colored by its class."></a></p>
<figcaption>Fig. 6. An SVM decision boundary for a two-class data set. Each point is colored by its class.</figcaption>
</figure>
</div>
<p>The points are colored by their true classes, and the background is shaded according to the SVM prediction at each point. It can be important to try at least a few different values of <span class="math inline">\(C\)</span>, which determines the trade-off between correctly classifying all samples and maximizing the margin, and to observe the effect on the accuracy as well as the algorithm convergence. Parameters such as this one which change the algorithm behavior but aren’t optimized by the algorithm itself are commonly known as <em>hyperparameters</em>.</p>
</section>
<section id="kernel-trick" class="level3">
<h3 class="anchored" data-anchor-id="kernel-trick">2.2. Kernel trick</h3>
<p>In some cases the linear model is going to be bad; a frequently used example is “target” dataset.</p>
<div id="cell-29" class="cell" data-tags="[]" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">400</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>r1 <span class="op">=</span> np.sqrt(np.random.uniform(<span class="fl">0.0</span>, <span class="fl">0.2</span>, size<span class="op">=</span>(n,)))</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>r2 <span class="op">=</span> np.sqrt(np.random.uniform(<span class="fl">0.5</span>, <span class="fl">1.0</span>, size<span class="op">=</span>(n,)))</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>t1 <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, <span class="dv">2</span><span class="op">*</span>np.pi, size<span class="op">=</span>(n,))</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>t2 <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, <span class="dv">2</span><span class="op">*</span>np.pi, size<span class="op">=</span>(n,))</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.vstack([</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    np.vstack([r1 <span class="op">*</span> np.cos(t1), r1 <span class="op">*</span> np.sin(t1)]).T,</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    np.vstack([r2 <span class="op">*</span> np.cos(t2), r2 <span class="op">*</span> np.sin(t2)]).T,</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> n <span class="op">*</span> [<span class="dv">1</span>] <span class="op">+</span> n <span class="op">*</span> [<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>xlim, ylim <span class="op">=</span> padded_ranges(X, pad<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> pplt.subplots(xspineloc<span class="op">=</span><span class="st">'neither'</span>, yspineloc<span class="op">=</span><span class="st">'neither'</span>)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>ax.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], s<span class="op">=</span><span class="dv">20</span>, alpha<span class="op">=</span><span class="fl">0.35</span>, c<span class="op">=</span>y, cmap<span class="op">=</span><span class="st">'PiYG_r'</span>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>ax.<span class="bu">format</span>(xlim<span class="op">=</span>xlim, ylim<span class="op">=</span>ylim)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>plt.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./output_circles.png" class="lightbox" data-glightbox="description: .lightbox-desc-7" data-gallery="quarto-lightbox-gallery-7" title="Fig. 7. Two-dimensional data set which no linear model can classify."><img src="./output_circles.png" class="img-fluid figure-img" style="width:40.0%" alt="Fig. 7. Two-dimensional data set which no linear model can classify."></a></p>
<figcaption>Fig. 7. Two-dimensional data set which no linear model can classify.</figcaption>
</figure>
</div>
<p>A line won’t work; ideally, we would draw a circle around the inner cluster to split the data. The <a href="https://en.wikipedia.org/wiki/Kernel_method">kernel trick</a> can be used to alleviate this problem by performing a transformation to a higher dimensional space in which the data is linearly separable. For example, consider the transformation</p>
<p><span id="eq-4"><span class="math display">\[
(x_1, x_2) \rightarrow (x_1^2, x_2^2, \sqrt{2} x_1 x_2)
\tag{4}\]</span></span></p>
<div id="cell-33" class="cell" data-tags="[]" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>x1, x2 <span class="op">=</span> X.T</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>u <span class="op">=</span> x1<span class="op">**</span><span class="dv">2</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> np.sqrt(<span class="dv">2</span>) <span class="op">*</span> x1 <span class="op">*</span> x2</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> x2<span class="op">**</span><span class="dv">2</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> go.Figure(</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    data<span class="op">=</span>go.Scatter3d(</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>        x<span class="op">=</span>u, y<span class="op">=</span>v, z<span class="op">=</span>w, mode<span class="op">=</span><span class="st">'markers'</span>,</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>        marker<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span>y, size<span class="op">=</span><span class="dv">3</span>, opacity<span class="op">=</span><span class="fl">0.5</span>),</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>fig.update_scenes(xaxis_visible<span class="op">=</span><span class="va">False</span>, yaxis_visible<span class="op">=</span><span class="va">False</span>, zaxis_visible<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>fig.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<iframe scrolling="no" width="100%" height="545px" src="iframe_figures/figure_8.html" frameborder="0" allowfullscreen=""></iframe>
<p>Fig. 8. Data after applying the nonlinear transformation in <a href="#eq-4" class="quarto-xref">Equation&nbsp;4</a>. This is an interactive plot.</p>
</div>
</div>
<p>It’s clear from rotating this plot that the transformed data can be split with a 2D plane. This need not be the transformation used by the SVM — in fact, many transformations can be used — but it demonstrates the idea. The linear boundary in the transformed space can then be transformed into a nonlinear boundary in the original space. One way to plot this boundary is to predict a grid of points, then make a contour plot (the boundary is shown in grey).</p>
<div id="cell-35" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> svm.SVC(kernel<span class="op">=</span><span class="st">'rbf'</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>clf.fit(X, y)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> pplt.subplots(xspineloc<span class="op">=</span><span class="st">'neither'</span>, yspineloc<span class="op">=</span><span class="st">'neither'</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>ax.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], s<span class="op">=</span><span class="dv">20</span>, alpha<span class="op">=</span><span class="fl">0.35</span>, c<span class="op">=</span>y, cmap<span class="op">=</span><span class="st">'PiYG_r'</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>ax.<span class="bu">format</span>(xlim<span class="op">=</span>xlim, ylim<span class="op">=</span>ylim)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>plot_dec_regions(clf, xlim<span class="op">=</span>xlim, ylim<span class="op">=</span>ylim, ax<span class="op">=</span>ax, cmap<span class="op">=</span><span class="st">'PiYG_r'</span>)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>plt.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./_output_circles_dec_region.png" class="lightbox" data-glightbox="description: .lightbox-desc-8" data-gallery="quarto-lightbox-gallery-8" title="Fig. 9. Result of SVM with RBF kernel"><img src="./_output_circles_dec_region.png" class="img-fluid figure-img" style="width:40.0%" alt="Fig. 9. Result of SVM with RBF kernel"></a></p>
<figcaption>Fig. 9. Result of SVM with RBF kernel</figcaption>
</figure>
</div>
<p>There are still several advantages to the linear SVM. First, it is much faster to train, and second, the kernel trick may be unnecessary for high-dimensional data. As we’ll see, text data can involve a large number of very high-dimensional samples, so we’ll be sticking with linear kernels.</p>
</section>
<section id="multi-class-svm" class="level3">
<h3 class="anchored" data-anchor-id="multi-class-svm">2.3. Multi-class SVM</h3>
<p>A binary classifier can also be used for multi-class problems. Here we use the <em>one-versus-rest</em> (OVR) approach. Suppose we had <span class="math inline">\(N\)</span> classes denoted by <span class="math inline">\(c_1\)</span>, <span class="math inline">\(c_2\)</span> … <span class="math inline">\(c_N\)</span>. In the OVR approach, we train <span class="math inline">\(N\)</span> different classifiers; the ith classifier <span class="math inline">\(L_i\)</span> tries to split the data into two parts: <span class="math inline">\(c_i\)</span> and not <span class="math inline">\(c_i\)</span>. Then we observe a new point and ask each classifier <span class="math inline">\(L_i\)</span> how confident it is that the point belongs to <span class="math inline">\(c_i\)</span>. The point is assigned to the class with the highest score. We can extend our previous example to three Gaussian distributions to get a sense of how the decision boundaries are formed.</p>
<div id="cell-41" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create three Gaussian distributions</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.vstack([</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    np.random.normal(size<span class="op">=</span>(n, <span class="dv">2</span>), loc<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">0</span>], scale<span class="op">=</span><span class="fl">2.0</span>),</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    np.random.normal(size<span class="op">=</span>(n, <span class="dv">2</span>), loc<span class="op">=</span>[<span class="dv">5</span>, <span class="dv">5</span>], scale<span class="op">=</span><span class="fl">2.5</span>),</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    np.random.normal(size<span class="op">=</span>(n, <span class="dv">2</span>), loc<span class="op">=</span>[<span class="op">-</span><span class="dv">6</span>, <span class="dv">6</span>], scale<span class="op">=</span><span class="fl">2.5</span>),</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> n <span class="op">*</span> [<span class="dv">1</span>] <span class="op">+</span> n <span class="op">*</span> [<span class="dv">0</span>] <span class="op">+</span> n <span class="op">*</span> [<span class="op">-</span><span class="dv">1</span>] </span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Find SVM decision boundary </span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> svm.LinearSVC(C<span class="op">=</span><span class="dv">1</span>, multi_class<span class="op">=</span><span class="st">'ovr'</span>, max_iter<span class="op">=</span><span class="dv">10000</span>)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>clf.fit(X, y)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the data</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>cmap<span class="op">=</span>pplt.Colormap((<span class="st">'pink9'</span>, <span class="st">'steel'</span>, <span class="st">'darkgreen'</span>))</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> pplt.subplots(xspineloc<span class="op">=</span><span class="st">'neither'</span>, yspineloc<span class="op">=</span><span class="st">'neither'</span>)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>ax.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], s<span class="op">=</span><span class="dv">20</span>, alpha<span class="op">=</span><span class="fl">0.35</span>, c<span class="op">=</span>y, cmap<span class="op">=</span>cmap)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot decision boundary</span></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>xlim, ylim <span class="op">=</span> padded_ranges(X)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>    ls <span class="op">=</span> [<span class="st">'-'</span>, <span class="st">'--'</span>, <span class="st">'dotted'</span>][i]</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>    plot_dec_boundary(clf, ax<span class="op">=</span>ax, i<span class="op">=</span>i, ls<span class="op">=</span>ls, label<span class="op">=</span><span class="ss">f'class </span><span class="sc">{</span>i <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss"> boundary'</span>)</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>ax.<span class="bu">format</span>(xlim<span class="op">=</span>xlim, ylim<span class="op">=</span>ylim)</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>ax.legend(ncols<span class="op">=</span><span class="dv">1</span>, loc<span class="op">=</span>(<span class="fl">1.1</span>, <span class="fl">0.6</span>))</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>plot_dec_regions(clf, xlim, ylim, ax<span class="op">=</span>ax, cmap<span class="op">=</span>cmap)</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>plt.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./_output_svm_multiclass.png" class="lightbox" data-glightbox="description: .lightbox-desc-9" data-gallery="quarto-lightbox-gallery-9" title="Fig. 10. Linear SVM trained on three-class data."><img src="./_output_svm_multiclass.png" class="img-fluid figure-img" style="width:55.0%" alt="Fig. 10. Linear SVM trained on three-class data."></a></p>
<figcaption>Fig. 10. Linear SVM trained on three-class data.</figcaption>
</figure>
</div>
<p>The same idea holds with more classes and dimensions. Notice that there are some regions which are claimed by multiple classifiers, so it’s not a perfect method.</p>
</section>
</section>
<section id="n-grams-and-feature-selection-methods" class="level2">
<h2 class="anchored" data-anchor-id="n-grams-and-feature-selection-methods">3. N-grams and feature selection methods</h2>
<p>As I mentioned in the introduction, the paper I’m following is called <em>N-Gram Feature Selection for Authorship Identification</em>. In short, the paper used <em>n-gram</em> frequencies (defined in a moment) as features in the classification task and developed a new method to select the most significant or “dominant” n-grams. This was tested on a collection of short news articles. Let’s step through their method.</p>
<section id="data-set-description" class="level3">
<h3 class="anchored" data-anchor-id="data-set-description">3.1. Data set description</h3>
<p>The Reuters Corpus Volume 1 (RCV1) data set is a big collection of news articles labeled by topic. Around 100,000 of these have known authors, and there are around 2000 different authors. A specific topic was chosen, and only authors who wrote at least one article which fell under this topic were considered. From this subset of authors, the top 50 in terms of number of articles written were chosen. 100 articles from each author were selected — 5000 in total — and these were evenly split into a training and testing set. The resulting corpus is a good challenge for authorship identification because the genre is invariant across documents and because the authors write about similar topics. Hopefully this leaves the author’s style as the primary distinguishing factor. The data set can be downloaded <a href="https://archive.ics.uci.edu/ml/datasets/Reuter_50_50">here</a>. The files are organized like this:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="files.png" class="lightbox" data-glightbox="description: .lightbox-desc-10" data-gallery="quarto-lightbox-gallery-10" title="Fig. 11. Organization of RCV1 data set."><img src="files.png" class="img-fluid figure-img" alt="Fig. 11. Organization of RCV1 data set."></a></p>
<figcaption>Fig. 11. Organization of RCV1 data set.</figcaption>
</figure>
</div>
<p>There are plenty of functions available to load the data and to extract features from it, but I’ll do everything manually just for fun. To load the data, let’s first create two lists of strings, <code>texts_train</code> and <code>texts_test</code>, corresponding to the 2500 training and testing documents. The class id and author name for each document are also stored.</p>
<div id="cell-51" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> load_files(outer_path):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    texts, class_ids, class_names <span class="op">=</span> [], [], []</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> class_id, folder <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">sorted</span>(os.listdir(outer_path))):</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>        folder_path <span class="op">=</span> os.path.join(outer_path, folder)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> filename <span class="kw">in</span> os.listdir(folder_path):</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>            class_ids.append(class_id)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>            class_names.append(folder)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>            <span class="bu">file</span> <span class="op">=</span> <span class="bu">open</span>(os.path.join(folder_path, filename), <span class="st">'r'</span>)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>            text <span class="op">=</span> <span class="bu">file</span>.read().replace(<span class="st">' '</span>, <span class="st">'_'</span>)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>            texts.append(text)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>            <span class="bu">file</span>.close()</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> texts, class_ids, class_names</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>texts_train, y_train, authors_train <span class="op">=</span> load_files(<span class="st">'reuters_data/train'</span>)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>texts_test, y_test, authors_test <span class="op">=</span> load_files(<span class="st">'reuters_data/test'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-52" class="cell" data-execution_count="5">
<div class="cell-output cell-output-display" data-execution_count="5">
<div>
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Author Name</th>
<th data-quarto-table-cell-role="th">Author ID</th>
<th data-quarto-table-cell-role="th">Training Text</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>AaronPressman</td>
<td>0</td>
<td>A_group_of_leading_trademark_specialists_plans...</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>AaronPressman</td>
<td>0</td>
<td>Prospects_for_comprehensive_reform_of_U.S._ban...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>AaronPressman</td>
<td>0</td>
<td>An_influential_economic_research_group_is_prep...</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>AaronPressman</td>
<td>0</td>
<td>The_Federal_Communications_Commission_proposed...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>AaronPressman</td>
<td>0</td>
<td>An_international_task_force_charged_with_resol...</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2495</td>
<td>WilliamKazer</td>
<td>49</td>
<td>China_could_list_more_railway_companies_and_is...</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">2496</td>
<td>WilliamKazer</td>
<td>49</td>
<td>The_choice_of_Singapore_for_the_listing_of_Chi...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2497</td>
<td>WilliamKazer</td>
<td>49</td>
<td>China_ushered_in_1997,_a_year_it_has_hailed_as...</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">2498</td>
<td>WilliamKazer</td>
<td>49</td>
<td>China_on_Tuesday_announced_a_ban_on_poultry_an...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2499</td>
<td>WilliamKazer</td>
<td>49</td>
<td>China's_leaders_have_agreed_on_a_need_to_stimu...</td>
</tr>
</tbody>
</table>

<p>2500 rows × 3 columns</p>
</div>
</div>
</div>
</div>
<p>The following histogram shows the distribution of document lengths in the training set; it’s expected that the short average document length will greatly increases the difficulty of the classification task relative to longer works such as books.</p>
<div id="cell-55" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>word_counts <span class="op">=</span> [<span class="bu">len</span>(text) <span class="cf">for</span> text <span class="kw">in</span> texts_train]</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> pplt.subplots(figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="fl">1.5</span>))</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>ax.hist(word_counts, bins<span class="op">=</span><span class="st">'auto'</span>, color<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>ax.<span class="bu">format</span>(xlabel<span class="op">=</span><span class="st">'Document length (characters)'</span>, ylabel<span class="op">=</span><span class="st">'Num. docs'</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>plt.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./_output_training_docs_lengths.png" class="lightbox" data-glightbox="description: .lightbox-desc-11" data-gallery="quarto-lightbox-gallery-11" title="Fig. 11. Distribution of document lengths in training set."><img src="./_output_training_docs_lengths.png" class="img-fluid figure-img" style="width:67.0%" alt="Fig. 11. Distribution of document lengths in training set."></a></p>
<figcaption>Fig. 11. Distribution of document lengths in training set.</figcaption>
</figure>
</div>
</section>
<section id="n-grams" class="level3">
<h3 class="anchored" data-anchor-id="n-grams">3.2. N-grams</h3>
<p>An obvious feature candidate is word frequency; a less obvious one is <em>n-gram</em> frequency. A character n-gram is a string of length n.&nbsp;For example, the 3-grams contained in <em>red_bike!</em> are <em>red</em>, <em>ed_</em>, <em>d_b</em>, *_bi<em>, </em>bik<em>, </em>ike<em>, </em>ke!*. These shorter strings may be useful because they capture different aspects of style such as the use of punctuation or certain prefixes/suffixes. They also remove any ambiguities in word extraction and work for all languages. To use these features in the SVM classifier, we need to create a feature matrix <span class="math inline">\(X\)</span> where <span class="math inline">\(X_{ij}\)</span> is the frequency of the jth n-gram in the ith document. Thus, each document is represented as a vector in <span class="math inline">\(k\)</span> dimensional space, where <span class="math inline">\(k\)</span> is the number of unique n-grams selected from the training documents. We’ll also normalize each vector so that all points are mapped onto the surface of the <span class="math inline">\(k\)</span>-dimensional unit sphere while preserving the angles between the vectors; this should help the SVM performance a bit.</p>
<div id="cell-60" class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_ngrams(text, n):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [text[i <span class="op">-</span> n : i] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n, <span class="bu">len</span>(text) <span class="op">+</span> <span class="dv">1</span>)]</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_ngrams_in_range(text, min_n, max_n):</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    ngrams <span class="op">=</span> []</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> n <span class="kw">in</span> <span class="bu">range</span>(min_n, max_n <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>        ngrams.extend(get_ngrams(text, n))</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ngrams</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sort_by_val(dictionary, max_items<span class="op">=</span><span class="va">None</span>, reverse<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    n_items <span class="op">=</span> <span class="bu">len</span>(dictionary)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> max_items <span class="kw">is</span> <span class="va">None</span> <span class="kw">or</span> max_items <span class="op">&gt;</span> n_items:</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>        max_items <span class="op">=</span> n_items</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    sorted_key_val_list <span class="op">=</span> <span class="bu">sorted</span>(dictionary.items(), key<span class="op">=</span><span class="kw">lambda</span> item: item[<span class="dv">1</span>], </span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>                                 reverse<span class="op">=</span>reverse)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {k: v <span class="cf">for</span> k, v <span class="kw">in</span> sorted_key_val_list[:max_items]}</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NgramExtractor:</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, ngram_range<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">5</span>)):</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.vocab <span class="op">=</span> {}</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.set_ngram_range(ngram_range)</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> set_ngram_range(<span class="va">self</span>, ngram_range):</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.min_n, <span class="va">self</span>.max_n <span class="op">=</span> ngram_range</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> build_vocab(<span class="va">self</span>, texts, max_features<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.vocab, index <span class="op">=</span> {}, <span class="dv">0</span></span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> n <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.min_n, <span class="va">self</span>.max_n <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>            ngrams <span class="op">=</span> []</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> text <span class="kw">in</span> texts:</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>                ngrams.extend(get_ngrams(text, n))</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>            counts <span class="op">=</span> sort_by_val(collections.Counter(ngrams), max_features)</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> ngram, count <span class="kw">in</span> counts.items():</span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.vocab[ngram] <span class="op">=</span> (index, count)</span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>                index <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> create_feature_matrix(<span class="va">self</span>, texts, norm_rows<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a>        X <span class="op">=</span> np.zeros((<span class="bu">len</span>(texts), <span class="bu">len</span>(<span class="va">self</span>.vocab)))</span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> text_index, text <span class="kw">in</span> <span class="bu">enumerate</span>(texts):</span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a>            ngrams <span class="op">=</span> get_ngrams_in_range(text, <span class="va">self</span>.min_n, <span class="va">self</span>.max_n)</span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> ngram, count <span class="kw">in</span> collections.Counter(ngrams).items():</span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> ngram <span class="kw">in</span> <span class="va">self</span>.vocab:</span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a>                    term_index <span class="op">=</span> <span class="va">self</span>.vocab[ngram][<span class="dv">0</span>]</span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a>                    X[text_index, term_index] <span class="op">=</span> count</span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> norm_rows:</span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a>            X <span class="op">=</span> np.apply_along_axis(<span class="kw">lambda</span> row: row <span class="op">/</span> np.linalg.norm(row), <span class="dv">1</span>, X)</span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> X</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we need to decide which value(s) of n to use as features. Let’s look at the distribution of n-grams in the training documents.</p>
<div id="cell-62" class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>extractor <span class="op">=</span> NgramExtractor(ngram_range<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">15</span>))</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>extractor.build_vocab(texts_train)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>len_counts <span class="op">=</span> collections.Counter([<span class="bu">len</span>(ngram) <span class="cf">for</span> ngram <span class="kw">in</span> extractor.vocab])</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> pplt.subplots(figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">3</span>))</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>x, y <span class="op">=</span> <span class="bu">zip</span>(<span class="op">*</span>len_counts.items())</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>ax.barh(x, y, color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">1.0</span>, width<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>ax.<span class="bu">format</span>(xscale<span class="op">=</span><span class="st">'log'</span>, xformatter<span class="op">=</span><span class="st">'log'</span>, ylabel<span class="op">=</span><span class="st">'n'</span>, xlabel<span class="op">=</span><span class="st">'n-gram count'</span>,</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>          yticks<span class="op">=</span>x, ytickminor<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>ax.grid(axis<span class="op">=</span><span class="st">'x'</span>)</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>plt.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-63" class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>fig.save(<span class="st">'./_output_n_gram_count.png'</span>, dpi<span class="op">=</span><span class="dv">250</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./_output_n_gram_count.png" class="lightbox" data-glightbox="description: .lightbox-desc-12" data-gallery="quarto-lightbox-gallery-12" title="Fig. 12. Distribution of character n-grams in the training text."><img src="./_output_n_gram_count.png" class="img-fluid figure-img" style="width:70.0%" alt="Fig. 12. Distribution of character n-grams in the training text."></a></p>
<figcaption>Fig. 12. Distribution of character n-grams in the training text.</figcaption>
</figure>
</div>
<p>The total number of n-grams with 1 <span class="math inline">\(\le\)</span>n <span class="math inline">\(\le\)</span> 15 is about 31 million; training a classifier on data with this number of dimensions is probably infeasible, and even more so on a larger data set. Previous studies have had success with fixing the value of n to be either 3, 4, or 5, so the authors chose to restrict their attention to these values. Their new idea was to use all n-grams in the range 3 <span class="math inline">\(\le\)</span>n <span class="math inline">\(\le\)</span> 5. This leaves a few hundred thousand features.</p>
<p>The next section will discuss statistical methods to prune the features; for now, though, we’ll implement the simple method of keeping the <span class="math inline">\(k\)</span> most frequent across all the training documents. As long as this doesn’t affect the accuracy too much, we reap the benefits of a reduction in computational time and the ability to fix the feature space dimensionality for the comparison of different feature types. To see why many low-frequency terms may be unimportant, suppose one of the authors wrote a single article about sharks in the training set. The term “shark” would have a small global frequency and be very useful in the training set since no other writers write about sharks, but it’s probably a good idea to discard it since it is unlikely to appear in the testing set. We must be careful, however, because some low-frequency terms could be important. These are probably terms that an author uses rarely but consistently over time. Maybe they like to use “incredible” as an adjective; the global frequency of “incred” would be much less than, say, “that_”, but it’s valuable because its frequency distribution will likely be the same in future writing. A quick test on our data set shows that <span class="math inline">\(k\)</span> = 15,000 is a good number. Let’s try this out on the 15,000 most frequent 3-grams.</p>
<div id="cell-66" class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>ngram_range <span class="op">=</span> (<span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>max_features <span class="op">=</span> <span class="dv">15000</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>norm_rows <span class="op">=</span> <span class="va">True</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>extractor <span class="op">=</span> NgramExtractor(ngram_range)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>extractor.build_vocab(texts_train, max_features)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> extractor.create_feature_matrix(texts_train, norm_rows)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> extractor.create_feature_matrix(texts_test, norm_rows)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here are some of the values in <code>X_train</code>. The columns have been sorted by descending frequency from left to right.</p>
<p>We can now feed this array to the SVM and make predictions on the testing data. I’ll keep the <span class="math inline">\(C\)</span> parameter fixed at <span class="math inline">\(C = 1\)</span> in all cases since this is what is done in the paper (I tried a few different values of <span class="math inline">\(C\)</span> and there wasn’t a large effect on the accuracy). Here is the confusion matrix obtained after training and testing:</p>
<div id="cell-70" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> svm.LinearSVC(C<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>clf.fit(X_train, y_train)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> clf.predict(X_test)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> sklearn.metrics.accuracy_score(y_test, y_pred)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>cmat <span class="op">=</span> sklearn.metrics.confusion_matrix(y_test, y_pred)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> pplt.subplots()</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>ax.imshow(cmat, cmap<span class="op">=</span><span class="st">'mono'</span>, colorbar<span class="op">=</span><span class="va">True</span>, colorbar_kw<span class="op">=</span><span class="bu">dict</span>(width<span class="op">=</span><span class="st">'1em'</span>, label<span class="op">=</span><span class="st">'Count'</span>))</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>ax.<span class="bu">format</span>(title_kw<span class="op">=</span><span class="bu">dict</span>(fontsize<span class="op">=</span><span class="st">'medium'</span>), ylabel<span class="op">=</span><span class="st">'True class'</span>, xlabel<span class="op">=</span><span class="st">'Predicted class'</span>)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>plt.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./_output_confusion_matrix.png" class="lightbox" data-glightbox="description: .lightbox-desc-13" data-gallery="quarto-lightbox-gallery-13" title="Fig. 13. Confusion matrix for linear SVM after training. The total accuracy is 70%."><img src="./_output_confusion_matrix.png" class="img-fluid figure-img" style="width:45.0%" alt="Fig. 13. Confusion matrix for linear SVM after training. The total accuracy is 70%."></a></p>
<figcaption>Fig. 13. Confusion matrix for linear SVM after training. The total accuracy is 70%.</figcaption>
</figure>
</div>
</section>
<section id="feature-selection" class="level3">
<h3 class="anchored" data-anchor-id="feature-selection">3.3. Feature selection</h3>
<p>In the rest of this post, we’ll study how to use statistical methods to further eliminate features from this initial set of 15,000. This process of selecting features which are “best” in a statistical sense is known as <em>feature selection</em>.</p>
<section id="information-gain" class="level4">
<h4 class="anchored" data-anchor-id="information-gain">3.3.1. Information gain</h4>
<p>A classical statistical measure of feature “goodness” is called <em>information gain</em> (IG). The idea is that knowing whether or not a term <em>t</em> is found in a document of a known class <span class="math inline">\(c\)</span> gives information about <span class="math inline">\(c\)</span>, and that some terms will contribute more information than others. The information gain can be written as <span class="citation" data-cites="Yang1997">&nbsp;[<a href="#ref-Yang1997" role="doc-biblioref">8</a>]</span></p>
<p><span id="eq-5"><span class="math display">\[
IG(t) = p(t) \sum_{i=1}^{m}p(c_i | t) \log p(c_i | t) +
p(\bar{t}) \sum_{i=1}^{m}p(c_i | \bar{t}) \log p(c_i | \bar{t}) -
\sum_{i=1}^{m}p(c_i) \log p(c_i).
\tag{5}\]</span></span></p>
<p>The probability of choosing term <span class="math inline">\(t\)</span> out of all terms in the corpus is given by <span class="math inline">\(p(t)\)</span>, and <span class="math inline">\(p(t) + p(\bar{t}) = 1\)</span>. Similarly, <span class="math inline">\(p(c_i)\)</span> is the probability that a randomly chosen document belongs to class <span class="math inline">\(c_i\)</span>, and <span class="math inline">\(p(c_i) + p(\bar{c_i}) = 1\)</span>. The probability that a document belongs to <span class="math inline">\(c_i\)</span> given that it contains <span class="math inline">\(t\)</span> is <span class="math inline">\(p(c_i | t)\)</span>, or <span class="math inline">\(p(c_i | \bar{t})\)</span> if it doesn’t contain <span class="math inline">\(t\)</span>. The strategy is then to keep the terms with the highest information gain scores.</p>
<div id="cell-77" class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> InfoGainSelector:</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.idx <span class="op">=</span> <span class="va">None</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, X, y):</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute probability distributions.</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>        n_docs, n_terms <span class="op">=</span> X.shape</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>        n_classes <span class="op">=</span> <span class="bu">len</span>(np.unique(y))</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>        P_c_and_t <span class="op">=</span> np.zeros((n_classes, n_terms))</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> doc_index, class_index <span class="kw">in</span> <span class="bu">enumerate</span>(y):</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>            P_c_and_t[class_index, :] <span class="op">+=</span> (X[doc_index, :] <span class="op">&gt;</span> <span class="dv">0</span>).astype(<span class="bu">int</span>)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>        P_c_and_t <span class="op">/=</span> np.<span class="bu">sum</span>(P_c_and_t)</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>        P_t <span class="op">=</span> np.<span class="bu">sum</span>(P_c_and_t, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>        P_c <span class="op">=</span> np.<span class="bu">sum</span>(P_c_and_t, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>        P_c_given_t <span class="op">=</span> P_c_and_t <span class="op">/</span> P_t</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>        P_c_given_tbar <span class="op">=</span> <span class="fl">1.0</span> <span class="op">-</span> (<span class="fl">1.0</span> <span class="op">-</span> P_c_and_t) <span class="op">/</span> (<span class="fl">1.0</span> <span class="op">-</span> P_t)</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute information gain for each feature.</span></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> XlogX(X):</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> X <span class="op">*</span> np.log2(X, out<span class="op">=</span>np.zeros_like(X), where<span class="op">=</span>(X <span class="op">&gt;</span> <span class="dv">0</span>))</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> np.zeros(n_terms)</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">+=</span> np.<span class="bu">sum</span>(P_t <span class="op">*</span> XlogX(P_c_given_t), axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">+=</span> np.<span class="bu">sum</span>((<span class="dv">1</span> <span class="op">-</span> P_t) <span class="op">*</span> XlogX(P_c_given_tbar), axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">-=</span> np.<span class="bu">sum</span>(XlogX(P_c))</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.idx <span class="op">=</span> np.argsort(scores)</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> select(<span class="va">self</span>, X, k<span class="op">=-</span><span class="dv">1</span>):  </span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> X[:, <span class="va">self</span>.idx[:k]]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We’ll now compare 4 sets of 15,000 features: 3-grams, 4-grams, 5-grams, and equal parts 3/4/5-grams, each time using IG to select the best <span class="math inline">\(k\)</span> features and plotting the accuracy vs.&nbsp;<span class="math inline">\(k\)</span>. I’ll start from <span class="math inline">\(k\)</span> = 1 to 200.</p>
<div id="cell-79" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>extractor <span class="op">=</span> NgramExtractor()</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>selector <span class="op">=</span> InfoGainSelector()</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> svm.LinearSVC(C<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compare_acc_ig(ngram_ranges<span class="op">=</span><span class="va">None</span>, kmin<span class="op">=</span><span class="dv">0</span>, kmax<span class="op">=</span><span class="dv">1</span>, kstep<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>    n_keep <span class="op">=</span> np.arange(kmin, kmax <span class="op">+</span> kstep, kstep).astype(<span class="bu">int</span>)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    accuracies <span class="op">=</span> np.zeros((<span class="bu">len</span>(ngram_ranges), <span class="bu">len</span>(n_keep)))</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, ngram_range <span class="kw">in</span> <span class="bu">enumerate</span>(ngram_ranges):</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>        extractor.set_ngram_range(ngram_range)</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>        max_features <span class="op">=</span> <span class="dv">5000</span> <span class="cf">if</span> ngram_range <span class="op">==</span> (<span class="dv">3</span>, <span class="dv">5</span>) <span class="cf">else</span> <span class="dv">15000</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>        extractor.build_vocab(texts_train, max_features)</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>        X_train <span class="op">=</span> extractor.create_feature_matrix(texts_train)</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>        X_test <span class="op">=</span> extractor.create_feature_matrix(texts_test)</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>        selector.fit(X_train, y_train)</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j, k <span class="kw">in</span> <span class="bu">enumerate</span>(n_keep):</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>            X_train_red <span class="op">=</span> selector.select(X_train, k)</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>            X_test_red <span class="op">=</span> selector.select(X_test, k)</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>            clf.fit(X_train_red, y_train)</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> clf.predict(X_test_red)</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>            accuracies[i, j] <span class="op">=</span> sklearn.metrics.accuracy_score(y_test, y_pred)</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> accuracies</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_accs(accuracies, kmin<span class="op">=</span><span class="dv">0</span>, kmax<span class="op">=</span><span class="dv">1</span>, kstep<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>    fig, ax <span class="op">=</span> pplt.subplots()</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>    ax.<span class="bu">format</span>(cycle<span class="op">=</span><span class="st">'538'</span>)</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> [<span class="st">"n = 3"</span>, <span class="st">"n = 4"</span>, <span class="st">"n = 5"</span>, <span class="st">"n = (3, 4, 5)"</span>]</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>    ks <span class="op">=</span> np.arange(kmin, kmax <span class="op">+</span> kstep, kstep)</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>):</span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>        m <span class="op">=</span> [<span class="st">"D"</span>, <span class="st">"s"</span>, <span class="st">"^"</span>, <span class="st">"s"</span>][i]</span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a>        mfc <span class="op">=</span> [<span class="va">None</span>, <span class="va">None</span>, <span class="st">"w"</span>, <span class="st">"w"</span>][i]</span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a>        ax.plot(ks, accuracies[i, :], marker<span class="op">=</span>m, ms<span class="op">=</span><span class="dv">4</span>, mew<span class="op">=</span><span class="fl">1.0</span>, mfc<span class="op">=</span>mfc, label<span class="op">=</span>labels[i])</span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a>    ax.<span class="bu">format</span>(xlabel<span class="op">=</span><span class="st">"Number of features selected (k)"</span>, ylabel<span class="op">=</span><span class="st">"Accuracy"</span>)</span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>    pad <span class="op">=</span> <span class="fl">0.05</span> <span class="op">*</span> (kmax <span class="op">+</span> kmin <span class="op">+</span> kstep)</span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a>    ax.<span class="bu">format</span>(xlim<span class="op">=</span>(kmin <span class="op">-</span> pad, kmax <span class="op">+</span> pad))</span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a>    ax.legend(ncols<span class="op">=</span><span class="dv">1</span>, loc<span class="op">=</span><span class="st">'lower right'</span>)</span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> fig, ax</span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a>ngram_ranges <span class="op">=</span> [(<span class="dv">3</span>, <span class="dv">3</span>), (<span class="dv">4</span>, <span class="dv">4</span>), (<span class="dv">5</span>, <span class="dv">5</span>), (<span class="dv">3</span>, <span class="dv">5</span>)]</span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a>kmin, kmax, kstep <span class="op">=</span> (<span class="dv">1</span>, <span class="dv">201</span>, <span class="dv">20</span>)</span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true" tabindex="-1"></a>accuracies <span class="op">=</span> compare_acc_ig(ngram_ranges, kmin, kmax, kstep)</span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plot_accs(accuracies, kmin, kmax, kstep)</span>
<span id="cb15-45"><a href="#cb15-45" aria-hidden="true" tabindex="-1"></a>plt.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./_output_compare_acc_ig1.png" class="lightbox" data-glightbox="description: .lightbox-desc-14" data-gallery="quarto-lightbox-gallery-14" title="Fig. 14. Information Gain (IG) accuracy vs.&nbsp;number of features (k) for 1 \le k \le 200"><img src="./_output_compare_acc_ig1.png" class="img-fluid figure-img" style="width:42.0%" alt="Fig. 14. Information Gain (IG) accuracy vs.&nbsp;number of features (k) for 1 \le k \le 200"></a></p>
<figcaption>Fig. 14. Information Gain (IG) accuracy vs.&nbsp;number of features (<span class="math inline">\(k\)</span>) for <span class="math inline">\(1 \le k \le 200\)</span></figcaption>
</figure>
</div>
<p>The accuracy at <span class="math inline">\(k\)</span> = 1 is 0.04, so using the feature with the highest IG score is actually twice as effective as random guessing! By the end of the plot 3-grams and variable length n-grams have taken a clear leaad, with 5-grams in last place. The performance gap between the different n-grams also appears to be growing with <span class="math inline">\(k\)</span>.</p>
<p>The next region we’ll look at is <span class="math inline">\(200 \le k \le 2000\)</span>.</p>
<div id="cell-84" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>kmin, kmax, kstep <span class="op">=</span> (<span class="dv">200</span>, <span class="dv">2000</span>, <span class="dv">200</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>accuracies <span class="op">=</span> compare_acc_ig(ngram_ranges, kmin, kmax, kstep)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plot_accs(accuracies, kmin, kmax, kstep)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>plt.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./_output_compare_acc_ig2.png" class="lightbox" data-glightbox="description: .lightbox-desc-15" data-gallery="quarto-lightbox-gallery-15" title="Fig. 15. Information Gain (IG) accuracy vs.&nbsp;number of features (k) for 200 \le k \le 2000"><img src="./_output_compare_acc_ig2.png" class="img-fluid figure-img" style="width:42.0%" alt="Fig. 15. Information Gain (IG) accuracy vs.&nbsp;number of features (k) for 200 \le k \le 2000"></a></p>
<figcaption>Fig. 15. Information Gain (IG) accuracy vs.&nbsp;number of features (<span class="math inline">\(k\)</span>) for <span class="math inline">\(200 \le k \le 2000\)</span></figcaption>
</figure>
</div>
<p>Now the gap is decreasing as we approach an upper performance limit at higher <span class="math inline">\(k\)</span>, especially for 3-grams. We’ll now look at the region which is plotted in the paper: <span class="math inline">\(2,000 \le k \le 10,000\)</span>.</p>
<div id="cell-88" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>kmin, kmax, kstep <span class="op">=</span> (<span class="dv">2000</span>, <span class="dv">10000</span>, <span class="dv">1000</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>accuracies <span class="op">=</span> compare_acc_ig(ngram_ranges, kmin, kmax, kstep)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plot_accs(accuracies, kmin, kmax, kstep)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>plt.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./_output_compare_acc_ig3.png" class="lightbox" data-glightbox="description: .lightbox-desc-16" data-gallery="quarto-lightbox-gallery-16" title="Fig. 16. Information Gain (IG) accuracy vs.&nbsp;number of features (k) for 2000 \le k \le 10000"><img src="./_output_compare_acc_ig3.png" class="img-fluid figure-img" style="width:42.0%" alt="Fig. 16. Information Gain (IG) accuracy vs.&nbsp;number of features (k) for 2000 \le k \le 10000"></a></p>
<figcaption>Fig. 16. Information Gain (IG) accuracy vs.&nbsp;number of features (<span class="math inline">\(k\)</span>) for <span class="math inline">\(2000 \le k \le 10000\)</span></figcaption>
</figure>
</div>
<p>I noticed that 5-grams make a big jump from last place to first place. I’m not sure if I have any deep insights into this behavior, but it’s interesting that the best n-gram to choose depends on the number of features selected. Now, I should compare with Fig. 1 from the paper:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="stamatatos_fig1.png" class="lightbox" data-glightbox="description: .lightbox-desc-17" data-gallery="quarto-lightbox-gallery-17" title="Fig. 17. Authorship identification results using information gain for feature selection. (From[@Houvardas2006].)"><img src="stamatatos_fig1.png" class="img-fluid figure-img" style="width:85.0%" alt="Fig. 17. Authorship identification results using information gain for feature selection. (From&nbsp;[1].)"></a></p>
<figcaption>Fig. 17. Authorship identification results using information gain for feature selection. (From<span class="citation" data-cites="Houvardas2006">&nbsp;[<a href="#ref-Houvardas2006" role="doc-biblioref">1</a>]</span>.)</figcaption>
</figure>
</div>
<p>The first difference is the maximum achieved accuracy which is a few percentage points higher. The second difference is that the authors found 3-grams to be worst at low <span class="math inline">\(k\)</span> and best at high <span class="math inline">\(k\)</span>.and the opposite for 5-grams. I’ll leave this as an open problem.</p>
</section>
<section id="localmaxs-algorithm" class="level4">
<h4 class="anchored" data-anchor-id="localmaxs-algorithm">3.3.2. LocalMaxs algorithm</h4>
<p>Let’s look at the top IG scoring n-grams from from the variable-length feature set.</p>
<div id="cell-96" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>extractor.set_ngram_range((<span class="dv">3</span>, <span class="dv">5</span>))</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>extractor.build_vocab(texts_train, max_features<span class="op">=</span><span class="dv">5000</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> extractor.create_feature_matrix(texts_train)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> extractor.create_feature_matrix(texts_test)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>selector.fit(X_train, y_train)</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_term(i):</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> key, (idx, count) <span class="kw">in</span> extractor.vocab.items():</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> idx <span class="op">==</span> i:</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> key</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> rank, i <span class="kw">in</span> <span class="bu">enumerate</span>(selector.idx[:<span class="dv">10</span>], start<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'</span><span class="sc">{:02}</span><span class="st">. </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(rank, get_term(i)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Notice all the variants of <em>the</em> which were included. IG has no way of knowing that these are basically the same. This motivates the definition of something called “glue”. Consider the word bigram <em>Amelia Earhart</em>. These two words are very likely to be found next to each other and could probably be treated as a single multi-word unit; it is as if there is glue holding the two words together. The amount of glue is probably higher than that between, say, <em>window</em> and <em>Earhart</em>. A technique has been developed to quantify this glue and extend its calculation to word n-grams instead of just word bi-grams<span class="citation" data-cites="Silva2009">&nbsp;[<a href="#ref-Silva2009" role="doc-biblioref">9</a>]</span>. The same idea can then be applied to character n-grams.</p>
<p>Let <span class="math inline">\(g(C)\)</span> be the glue of character n-gram <span class="math inline">\(C = c_1 \dots c_n\)</span>. Assuming we had a way to calculate the glue, how could this concept be used for feature selection? One solution is called the LocalMaxs algorithm. First define an antecedent <span class="math inline">\(ant(C)\)</span> as an (n-1)-gram which is contained in <span class="math inline">\(C\)</span>, e.g., “string” <span class="math inline">\(\rightarrow\)</span> “strin” or “tring”. Then define a successor <span class="math inline">\(succ(C)\)</span> as an (n+1)-gram which contains <span class="math inline">\(C\)</span>, e.g., “string” <span class="math inline">\(\rightarrow\)</span> “strings” or “astring”. <em>C</em> is selected as a feature if</p>
<p><span id="eq-6"><span class="math display">\[
g(C) \ge g(ant(C)) \,\, and \,\, g(C) &gt; g(succ(C))
\tag{6}\]</span></span></p>
<p>for all <em>ant</em>(C) and <em>succ</em>(C). Since we’re dealing with 3 <span class="math inline">\(\le\)</span> n <span class="math inline">\(\le\)</span> 5, only the latter condition is checked if n = 3, and only the former condition is checked for n = 5. Eq. (6) says that the glue of a selected feature shouldn’t increase by adding a character to or removing a character from the start or end of the n-gram, i.e., the glue is at a local maximum with respect to similar n-grams. Now that the selection criteria are established, we can move on to calculating the glue. Here there are several options, but the one used in the paper is called <em>symmetrical conditional probability</em> (SCP). If we have a bigram <span class="math inline">\(C = c_1c_2\)</span>, then</p>
<p><span id="eq-7"><span class="math display">\[
SCP(c_1c_2) = p(c_1|c_2) \cdot p(c_2|c_1) = \frac{p(c_1,c_2)^2}{p(c_1)p(c_2)},
\tag{7}\]</span></span></p>
<p>so SCP is a measure of how likely one character is given the other and vice versa. This formula can be applied to an n-gram <span class="math inline">\(C = c_1\dots c_n\)</span> by performing a <em>pseudo bigram transformation</em>, which means splitting the n-gram into two parts at a chosen <em>dispersion point</em>; for example, “help” could be split as “h*elp”, “he*lp”, or “hel*p”, where * is the dispersion point. Splitting <span class="math inline">\(C\)</span> as <span class="math inline">\(c_1 \dots c_{n-1}\)</span>*<span class="math inline">\(c_n\)</span> would give</p>
<p><span id="eq-8"><span class="math display">\[
SCP((c_1 \dots c_{n-1})c_n) = \frac{p(c_1 \dots c_n)^2}{p(c_1 \dots c_{n-1})p(c_n)}.
\tag{8}\]</span></span></p>
<p>Of course, the answer will depend on the dispersion point. We therefore introduce the FairSCP which averages over the possible dispersion points:</p>
<p><span id="eq-9"><span class="math display">\[
FairSCP(c_1 \dots c_n) = \frac{p(c_1 \dots c_n)^2}{\frac{1}{n-1}\sum_{i=1}^{n-1} p(c_1 \dots c_i)p(c_{i+1} \dots c_n)}.
\tag{9}\]</span></span></p>
<p>In summary, LocalMaxs loops through every n-gram in the vocabulary, computes the glue as <span class="math inline">\(g(C) = FairSCP(C)\)</span>, and keeps the n-gram if Eq. (6) is satisfied. It differs from IG selection in that the features are not ranked, so the number of selected features is completely determined by the text. The method is implemented below.</p>
<div id="cell-98" class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> antecedents(ngram):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [ngram[:<span class="op">-</span><span class="dv">1</span>], ngram[<span class="dv">1</span>:]]</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> successors(ngram, characters<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> characters <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>        characters <span class="op">=</span> string.printable</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>    successors <span class="op">=</span> []</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> character <span class="kw">in</span> characters:</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>        successors.append(character <span class="op">+</span> ngram)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>        successors.append(ngram <span class="op">+</span> character)</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> successors</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LocalMaxsExtractor(NgramExtractor):</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, ngram_range<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">5</span>)):</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(ngram_range)</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.counts_list <span class="op">=</span> [] <span class="co"># ith element is dictionary of unique (i+1)-gram counts </span></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sum_counts_list <span class="op">=</span> [] <span class="co"># ith element is the sum of `counts_list[i].values()`</span></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> build_vocab(<span class="va">self</span>, texts, max_features<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Count all n-grams with n &lt;= self.max_n</span></span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.counts_list, <span class="va">self</span>.sum_counts_list <span class="op">=</span> [], []</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>        candidate_ngrams <span class="op">=</span> {}</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> n <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="va">self</span>.max_n <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>            ngrams <span class="op">=</span> []</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> text <span class="kw">in</span> texts:</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>                ngrams.extend(get_ngrams(text, n))</span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>            counts <span class="op">=</span> collections.Counter(ngrams)</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.counts_list.append(counts)</span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.sum_counts_list.append(<span class="bu">sum</span>(counts.values()))</span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>.min_n <span class="op">&lt;=</span> n <span class="op">&lt;=</span> <span class="va">self</span>.max_n:</span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a>                candidate_ngrams.update(sort_by_val(counts, max_features))</span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.available_characters <span class="op">=</span> <span class="va">self</span>.counts_list[<span class="dv">0</span>].keys()    </span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Select candidate n-grams whose glue is at local maximum </span></span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.vocab, index <span class="op">=</span> {}, <span class="dv">0</span></span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> ngram, count <span class="kw">in</span> candidate_ngrams.items():</span>
<span id="cb19-39"><a href="#cb19-39" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>.is_local_max(ngram):</span>
<span id="cb19-40"><a href="#cb19-40" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.vocab[ngram] <span class="op">=</span> (index, count)</span>
<span id="cb19-41"><a href="#cb19-41" aria-hidden="true" tabindex="-1"></a>                index <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb19-42"><a href="#cb19-42" aria-hidden="true" tabindex="-1"></a>                    </span>
<span id="cb19-43"><a href="#cb19-43" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> is_local_max(<span class="va">self</span>, ngram):</span>
<span id="cb19-44"><a href="#cb19-44" aria-hidden="true" tabindex="-1"></a>        glue, n <span class="op">=</span> <span class="va">self</span>.glue(ngram), <span class="bu">len</span>(ngram)</span>
<span id="cb19-45"><a href="#cb19-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> n <span class="op">&lt;</span> <span class="va">self</span>.max_n:</span>
<span id="cb19-46"><a href="#cb19-46" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> succ <span class="kw">in</span> successors(ngram, <span class="va">self</span>.available_characters):</span>
<span id="cb19-47"><a href="#cb19-47" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="va">self</span>.glue(succ) <span class="op">&gt;=</span> glue:</span>
<span id="cb19-48"><a href="#cb19-48" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">return</span> <span class="va">False</span></span>
<span id="cb19-49"><a href="#cb19-49" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> n <span class="op">&gt;</span> <span class="va">self</span>.min_n:</span>
<span id="cb19-50"><a href="#cb19-50" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> ant <span class="kw">in</span> antecedents(ngram):</span>
<span id="cb19-51"><a href="#cb19-51" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="va">self</span>.glue(ant) <span class="op">&gt;</span> glue:</span>
<span id="cb19-52"><a href="#cb19-52" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">return</span> <span class="va">False</span> </span>
<span id="cb19-53"><a href="#cb19-53" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">True</span></span>
<span id="cb19-54"><a href="#cb19-54" aria-hidden="true" tabindex="-1"></a>                    </span>
<span id="cb19-55"><a href="#cb19-55" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> glue(<span class="va">self</span>, ngram):</span>
<span id="cb19-56"><a href="#cb19-56" aria-hidden="true" tabindex="-1"></a>        n <span class="op">=</span> <span class="bu">len</span>(ngram)</span>
<span id="cb19-57"><a href="#cb19-57" aria-hidden="true" tabindex="-1"></a>        P <span class="op">=</span> <span class="va">self</span>.counts_list[n <span class="op">-</span> <span class="dv">1</span>].get(ngram, <span class="dv">0</span>) <span class="op">/</span> <span class="va">self</span>.sum_counts_list[n <span class="op">-</span> <span class="dv">1</span>]</span>
<span id="cb19-58"><a href="#cb19-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> P <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb19-59"><a href="#cb19-59" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="fl">0.0</span></span>
<span id="cb19-60"><a href="#cb19-60" aria-hidden="true" tabindex="-1"></a>        Avp <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb19-61"><a href="#cb19-61" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> disp_point <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, n):</span>
<span id="cb19-62"><a href="#cb19-62" aria-hidden="true" tabindex="-1"></a>            ngram_l, ngram_r <span class="op">=</span> ngram[:disp_point], ngram[disp_point:]</span>
<span id="cb19-63"><a href="#cb19-63" aria-hidden="true" tabindex="-1"></a>            n_l, n_r <span class="op">=</span> disp_point, n <span class="op">-</span> disp_point</span>
<span id="cb19-64"><a href="#cb19-64" aria-hidden="true" tabindex="-1"></a>            P_l <span class="op">=</span> <span class="va">self</span>.counts_list[n_l <span class="op">-</span> <span class="dv">1</span>].get(ngram_l, <span class="dv">0</span>) <span class="op">/</span> <span class="va">self</span>.sum_counts_list[n_l <span class="op">-</span> <span class="dv">1</span>]</span>
<span id="cb19-65"><a href="#cb19-65" aria-hidden="true" tabindex="-1"></a>            P_r <span class="op">=</span> <span class="va">self</span>.counts_list[n_r <span class="op">-</span> <span class="dv">1</span>].get(ngram_r, <span class="dv">0</span>) <span class="op">/</span> <span class="va">self</span>.sum_counts_list[n_r <span class="op">-</span> <span class="dv">1</span>]</span>
<span id="cb19-66"><a href="#cb19-66" aria-hidden="true" tabindex="-1"></a>            Avp <span class="op">+=</span> P_l <span class="op">*</span> P_r</span>
<span id="cb19-67"><a href="#cb19-67" aria-hidden="true" tabindex="-1"></a>        Avp <span class="op">=</span> Avp <span class="op">/</span> (n <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb19-68"><a href="#cb19-68" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> P<span class="op">**</span><span class="dv">2</span> <span class="op">/</span> Avp</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The first thing we should do is check the the glue of the derivative n-grams <em>the</em>, *_the*, etc.</p>
<div id="cell-100" class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>extractor <span class="op">=</span> LocalMaxsExtractor(ngram_range<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">5</span>))</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>extractor.build_vocab(texts_train)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ngram <span class="kw">in</span> [<span class="st">'the'</span>, <span class="st">'_the'</span>, <span class="st">'the_'</span>, <span class="st">'_the_'</span>]:</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    glue <span class="op">=</span> extractor.glue(ngram)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    selected <span class="op">=</span> extractor.is_local_max(ngram)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    freq <span class="op">=</span> extractor.counts_list[<span class="bu">len</span>(ngram) <span class="op">-</span> <span class="dv">1</span>][ngram]</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'</span><span class="sc">{:&lt;5}</span><span class="st">: glue = </span><span class="sc">{:.4f}</span><span class="st">, selected = </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(ngram, glue, selected))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>It seems to be working correctly. Now we’d like to compare the performance to IG. There’s no way to directly compare since LocalMaxs doesn’t rank features; however, it’s possible to vary the size of the initial set of features from which LocalMaxs makes its selections. Below, this initial size is varied from 3,000 to 24,000 using equal parts 3/4/5 grams as features.</p>
<div id="cell-102" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>lm_extractor <span class="op">=</span> LocalMaxsExtractor(ngram_range<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">5</span>))</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> svm.LinearSVC(C<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>max_features_list <span class="op">=</span> np.arange(<span class="dv">2000</span>, <span class="dv">8000</span>, <span class="dv">1000</span>).astype(<span class="bu">int</span>)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>lm_accuracies, lm_vocabs <span class="op">=</span> [], []</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> max_features <span class="kw">in</span> max_features_list:</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>    lm_extractor.build_vocab(texts_train, max_features)</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>    X_train_red <span class="op">=</span> lm_extractor.create_feature_matrix(texts_train)</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>    X_test_red <span class="op">=</span> lm_extractor.create_feature_matrix(texts_test)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    clf.fit(X_train_red, y_train)</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> clf.predict(X_test_red)</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>    lm_accuracies.append(sklearn.metrics.accuracy_score(y_test, y_pred))</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>    lm_vocabs.append(lm_extractor.vocab)</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>n_keep <span class="op">=</span> [<span class="bu">len</span>(vocab) <span class="cf">for</span> vocab <span class="kw">in</span> lm_vocabs]</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>ig_extractor <span class="op">=</span> NgramExtractor(ngram_range<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">5</span>))</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>ig_extractor.build_vocab(texts_train, max_features<span class="op">=</span><span class="dv">5000</span>)</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> ig_extractor.create_feature_matrix(texts_train)</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> ig_extractor.create_feature_matrix(texts_test)</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>ig_selector <span class="op">=</span> InfoGainSelector()</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>ig_selector.fit(X_train, y_train)</span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>ig_accuracies, ig_vocabs <span class="op">=</span> [], []</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> lm_vocab <span class="kw">in</span> lm_vocabs:</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>    k <span class="op">=</span> <span class="bu">len</span>(lm_vocab)</span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>    X_train_red <span class="op">=</span> ig_selector.select(X_train, k)</span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a>    X_test_red <span class="op">=</span> ig_selector.select(X_test, k)</span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a>    clf.fit(X_train_red, y_train)</span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> clf.predict(X_test_red)</span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a>    ig_accuracies.append(sklearn.metrics.accuracy_score(y_test, y_pred))</span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a>    ig_vocabs.append(extractor.vocab)</span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> pplt.subplots(figsize<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">3</span>))</span>
<span id="cb21-32"><a href="#cb21-32" aria-hidden="true" tabindex="-1"></a>kws <span class="op">=</span> <span class="bu">dict</span>(marker<span class="op">=</span><span class="st">'.'</span>, ms<span class="op">=</span><span class="dv">10</span>, lw<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb21-33"><a href="#cb21-33" aria-hidden="true" tabindex="-1"></a>ax.plot(n_keep, lm_accuracies, label<span class="op">=</span><span class="st">'n = (3, 4, 5) — LocalMaxs'</span>, <span class="op">**</span>kws)</span>
<span id="cb21-34"><a href="#cb21-34" aria-hidden="true" tabindex="-1"></a>ax.plot(n_keep, ig_accuracies, label<span class="op">=</span><span class="st">'n = (3, 4, 5) — IG'</span>, <span class="op">**</span>kws)</span>
<span id="cb21-35"><a href="#cb21-35" aria-hidden="true" tabindex="-1"></a>ax.<span class="bu">format</span>(xlabel<span class="op">=</span><span class="st">'Number of features selected'</span>, ylabel<span class="op">=</span><span class="st">'Accuracy'</span>)</span>
<span id="cb21-36"><a href="#cb21-36" aria-hidden="true" tabindex="-1"></a>ax.legend(ncols<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb21-37"><a href="#cb21-37" aria-hidden="true" tabindex="-1"></a>ax.<span class="bu">format</span>(xmin<span class="op">=</span><span class="dv">900</span>, xmax<span class="op">=</span><span class="dv">2800</span>)</span>
<span id="cb21-38"><a href="#cb21-38" aria-hidden="true" tabindex="-1"></a>plt.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./_output_localmaxs_vs_ig.png" class="lightbox" data-glightbox="description: .lightbox-desc-18" data-gallery="quarto-lightbox-gallery-18" title="Fig. 18. IG vs.&nbsp;LocalMaxs feature selection for 3/4/5-grams."><img src="./_output_localmaxs_vs_ig.png" class="img-fluid figure-img" style="width:55.0%" alt="Fig. 18. IG vs.&nbsp;LocalMaxs feature selection for 3/4/5-grams."></a></p>
<figcaption>Fig. 18. IG vs.&nbsp;LocalMaxs feature selection for 3/4/5-grams.</figcaption>
</figure>
</div>
<p>As you can see, LocalMaxs achieves a higher accuracy with the same number of features. The neat thing is that the vocabularies are totally different; for example, at the last data point, only about 15% of the n-grams are found in both sets! Let’s count the number of related n-grams in the two sets, where x is related to y if x is an antecedent or successor of y.</p>
<div id="cell-106" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> count_related(ngrams):</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> n1 <span class="kw">in</span> ngrams:</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> n2 <span class="kw">in</span> ngrams:</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> n1 <span class="op">!=</span> n2 <span class="kw">and</span> n1 <span class="kw">in</span> n2:</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>                count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> count</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>lm_ngrams <span class="op">=</span> <span class="bu">list</span>(lm_vocabs[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>vocab_size <span class="op">=</span> <span class="bu">len</span>(lm_ngrams)</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>ig_vocab <span class="op">=</span> <span class="bu">list</span>(ig_extractor.vocab)</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>ig_ngrams <span class="op">=</span> [ig_vocab[i] <span class="cf">for</span> i <span class="kw">in</span> ig_selector.idx[:vocab_size]]</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>shared <span class="op">=</span> <span class="bu">len</span>([ig_ngram <span class="cf">for</span> ig_ngram <span class="kw">in</span> ig_ngrams <span class="cf">if</span> ig_ngram <span class="kw">in</span> lm_ngrams])</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Vocab size: </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(vocab_size))</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'n-grams selected by both IG and LM: </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(shared))</span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'IG related n-grams: </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(count_related(ig_ngrams)))</span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'LM related n-grams: </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(count_related(lm_ngrams)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>As mentioned earlier, IG selects many related terms such as <em>the</em> and <em>the_</em>. The LocalMaxs vocabulary is much “richer”, as the authors put it. Here is the corresponding figure from the paper (ignore the white squares):</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="stamatatos_fig2.png" class="lightbox" data-glightbox="description: .lightbox-desc-19" data-gallery="quarto-lightbox-gallery-19" title="Results of the proposed method using only variable-length n-grams and variable-length n-grams plus words longer than 5 characters. (Source:[@Houvardas2006].)"><img src="stamatatos_fig2.png" class="img-fluid figure-img" style="width:80.0%" alt="Results of the proposed method using only variable-length n-grams and variable-length n-grams plus words longer than 5 characters. (Source:&nbsp;[1].)"></a></p>
<figcaption>Results of the proposed method using only variable-length n-grams and variable-length n-grams plus words longer than 5 characters. (Source:<span class="citation" data-cites="Houvardas2006">&nbsp;[<a href="#ref-Houvardas2006" role="doc-biblioref">1</a>]</span>.)</figcaption>
</figure>
</div>
<p>For some reason, their implementation extracted way more features than mine did. I don’t have access to the author’s code, and I couldn’t find any implementation of LocalMaxs online, so it’s hard for me to say what’s happening. At least my implementation exhibits some expected behavior (less related terms, better performance at lower feature numbers).</p>
</section>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">4. Conclusion</h2>
<p>In the future, I may apply these methods to my own data set; I’m particularly interested in what would happen with <a href="https://en.wikipedia.org/wiki/Chinese_characters">Chinese characters</a>. A different problem I’d like to examine is that of <em>artist</em> identification; the problem would be to match a collection of paintings with their painters. The <a href="https://www.wga.hu/index.html">Web Gallery of Art</a> is a database I found after a quick search, and I’m sure there are others. This would give me the chance to learn about image classification.</p>


<div class="hidden" aria-hidden="true">
<span class="glightbox-desc lightbox-desc-1">Fig. 1. The authorship identification task.</span>
<span class="glightbox-desc lightbox-desc-2">Fig. 2. Alexander Hamilton (left) and James Madison (right). (Source: Wikipedia.)</span>
<span class="glightbox-desc lightbox-desc-3">Fig. 3. Distribution of word lengths in “Oliver Twist”. Each curve is for a different sample of 1000 words. (Source:<span class="citation" data-cites="Mendenhall1887">&nbsp;[<a href="#ref-Mendenhall1887" role="doc-biblioref">4</a>]</span>.</span>
<span class="glightbox-desc lightbox-desc-4">Fig. 4. National GDPs appear to be moving toward the prediction by Zipf’s Law (red line). (Source:<span class="citation" data-cites="Cristelli2012">&nbsp;[<a href="#ref-Cristelli2012" role="doc-biblioref">6</a>]</span>.)</span>
<span class="glightbox-desc lightbox-desc-5">Fig. 5. Maximum margin separating plane. (Source: Wikipedia.)</span>
<span class="glightbox-desc lightbox-desc-6">Fig. 6. An SVM decision boundary for a two-class data set. Each point is colored by its class.</span>
<span class="glightbox-desc lightbox-desc-7">Fig. 7. Two-dimensional data set which no linear model can classify.</span>
<span class="glightbox-desc lightbox-desc-8">Fig. 9. Result of SVM with RBF kernel</span>
<span class="glightbox-desc lightbox-desc-9">Fig. 10. Linear SVM trained on three-class data.</span>
<span class="glightbox-desc lightbox-desc-10">Fig. 11. Organization of RCV1 data set.</span>
<span class="glightbox-desc lightbox-desc-11">Fig. 11. Distribution of document lengths in training set.</span>
<span class="glightbox-desc lightbox-desc-12">Fig. 12. Distribution of character n-grams in the training text.</span>
<span class="glightbox-desc lightbox-desc-13">Fig. 13. Confusion matrix for linear SVM after training. The total accuracy is 70%.</span>
<span class="glightbox-desc lightbox-desc-14">Fig. 14. Information Gain (IG) accuracy vs.&nbsp;number of features (<span class="math inline">\(k\)</span>) for <span class="math inline">\(1 \le k \le 200\)</span></span>
<span class="glightbox-desc lightbox-desc-15">Fig. 15. Information Gain (IG) accuracy vs.&nbsp;number of features (<span class="math inline">\(k\)</span>) for <span class="math inline">\(200 \le k \le 2000\)</span></span>
<span class="glightbox-desc lightbox-desc-16">Fig. 16. Information Gain (IG) accuracy vs.&nbsp;number of features (<span class="math inline">\(k\)</span>) for <span class="math inline">\(2000 \le k \le 10000\)</span></span>
<span class="glightbox-desc lightbox-desc-17">Fig. 17. Authorship identification results using information gain for feature selection. (From<span class="citation" data-cites="Houvardas2006">&nbsp;[<a href="#ref-Houvardas2006" role="doc-biblioref">1</a>]</span>.)</span>
<span class="glightbox-desc lightbox-desc-18">Fig. 18. IG vs.&nbsp;LocalMaxs feature selection for 3/4/5-grams.</span>
<span class="glightbox-desc lightbox-desc-19">Results of the proposed method using only variable-length n-grams and variable-length n-grams plus words longer than 5 characters. (Source:<span class="citation" data-cites="Houvardas2006">&nbsp;[<a href="#ref-Houvardas2006" role="doc-biblioref">1</a>]</span>.)</span>
</div>

</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body" data-entry-spacing="0" role="list">
<div id="ref-Houvardas2006" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">J. Houvardas and E. Stamatatos, <em>N-Gram Feature Selection for Authorship Identification</em>, in <em>Artificial Intelligence: Methodology, Systems, and Applications</em>, edited by J. Euzenat and J. Domingue (Springer Berlin Heidelberg, Berlin, Heidelberg, 2006), pp. 77–86.</div>
</div>
<div id="ref-Adair1944" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">A. Douglass, <em>The Authorship of the Disputed Federalist Papers</em>, The William and Mary Quarterly <strong>1</strong>, 97 (1944).</div>
</div>
<div id="ref-Holmes1998" class="csl-entry" role="listitem">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">D. Holmes, <em>The Evolution of Stylometry in Humanities Scholarship</em>, Literary and Linguistic Computing <strong>13</strong>, 111 (1998).</div>
</div>
<div id="ref-Mendenhall1887" class="csl-entry" role="listitem">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">T. C. Mendenhall, <em><a href="https://doi.org/10.1126/science.ns-9.214S.237">THE CHARACTERISTIC CURVES OF COMPOSITION</a></em>, Science <strong>ns-9</strong>, 237 (1887).</div>
</div>
<div id="ref-Piantadosi2014" class="csl-entry" role="listitem">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">S. T. Piantadosi, <em><a href="https://doi.org/10.3758/s13423-014-0585-6">Zipf’s Word Frequency Law in Natural Language: A Critical Review and Future Directions</a></em>, Psychon Bull Rev <strong>21</strong>, 1112 (2014).</div>
</div>
<div id="ref-Cristelli2012" class="csl-entry" role="listitem">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">M. Cristelli, M. Batty, and L. Pietronero, <em><a href="https://doi.org/10.1038/srep00812">There Is More Than a Power Law in Zipf</a></em>, Sci Rep <strong>2</strong>, (2012).</div>
</div>
<div id="ref-Bishop2006" class="csl-entry" role="listitem">
<div class="csl-left-margin">[7] </div><div class="csl-right-inline">C. M. Bishop, <em>Pattern Recognition and Machine Learning</em> (Springer, 2006).</div>
</div>
<div id="ref-Yang1997" class="csl-entry" role="listitem">
<div class="csl-left-margin">[8] </div><div class="csl-right-inline">Y. Yang and J. O. Pedersen, <em>A Comparative Study on Feature Selection in Text Categorization</em>, in <em>ICML</em> (1997).</div>
</div>
<div id="ref-Silva2009" class="csl-entry" role="listitem">
<div class="csl-left-margin">[9] </div><div class="csl-right-inline">J. Silva, <em>A Local Maxima Method and a Fair Dispersion Normalization for Extracting Multi-Word Units from Corpora</em>, in (2009).</div>
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = true;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="austin-hoover/blog" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<script>var lightboxQuarto = GLightbox({"loop":false,"descPosition":"bottom","openEffect":"zoom","selector":".lightbox","closeEffect":"zoom"});
window.onload = () => {
  lightboxQuarto.on('slide_before_load', (data) => {
    const { slideIndex, slideNode, slideConfig, player, trigger } = data;
    const href = trigger.getAttribute('href');
    if (href !== null) {
      const imgEl = window.document.querySelector(`a[href="${href}"] img`);
      if (imgEl !== null) {
        const srcAttr = imgEl.getAttribute("src");
        if (srcAttr && srcAttr.startsWith("data:")) {
          slideConfig.href = srcAttr;
        }
      }
    } 
  });

  lightboxQuarto.on('slide_after_load', (data) => {
    const { slideIndex, slideNode, slideConfig, player, trigger } = data;
    if (window.Quarto?.typesetMath) {
      window.Quarto.typesetMath(slideNode);
    }
  });

};
          </script>




</body></html>