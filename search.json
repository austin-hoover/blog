[
  {
    "objectID": "notes/index.html",
    "href": "notes/index.html",
    "title": "Notes",
    "section": "",
    "text": "Title\n\n\n\n\n\n\nBooks\n\n\n\n\nPapers\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/papers/index.html",
    "href": "notes/papers/index.html",
    "title": "Papers",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nA Bayesian approach to the simulation argument\n\n\nDavid Kipping\n\n\n\n\nA Counterexample to Plantinga’s Free Will Defense\n\n\nAlexander Pruss\n\n\n\n\nA Defense of Abortion\n\n\nJudith Thomson\n\n\n\n\nA Priori (Atheism)\n\n\nFilipe Leon\n\n\n\n\nA challenge for Frankfurt-style compatibilists\n\n\nPhilip Swensen\n\n\n\n\nAbility, Foreknowledge, and Explanatory Dependence\n\n\nPhilip Swensen\n\n\n\n\nBenardete Paradoxes, Causal Finitism, and the Unsatisfiable Pair Diagnosis\n\n\nJoseph Schmid, Alex Malpass\n\n\n\n\nBoltzmannian Immortality\n\n\nChristian Loew\n\n\n\n\nBook Review: ‘A New Kind of Science’\n\n\nScott Aaronson\n\n\n\n\nCausation as Folk Science\n\n\nJohn Norton\n\n\n\n\nContact inequality: first contact will likely be with an older civilization\n\n\nDavid Kipping\n\n\n\n\nContemporary Philosophical Perspectives on the Cosmological Constant\n\n\nA. Koberinski, B. Falck‡, C. Smeenk\n\n\n\n\nCosmological Arguments\n\n\nGraham Oppy\n\n\n\n\nCraig on the actual infinite\n\n\nWes Morriston\n\n\n\n\nDependence and the Freedom to Do Otherwise\n\n\nTaylor Cyr\n\n\n\n\nDivine Creative Freedom\n\n\nAlexander Pruss\n\n\n\n\nDo We Have Any Viable Solution to the Measurement Problem?\n\n\nEmily Adlam\n\n\n\n\nFacing Up to the Problem of Consciousness\n\n\nDavid Chalmers\n\n\n\n\nFifteen Arguments Against Hypothetical Frequentism\n\n\nAlan Hájek\n\n\n\n\nHeaven and the Problem of Eternal Separation\n\n\nEric Yang\n\n\n\n\nHow not to build an infinite lottery machine\n\n\nJohn Norton\n\n\n\n\nI Was Once a Fetus: That Is Why Abortion Is Wrong\n\n\nAlexander Pruss\n\n\n\n\nIf Materialism Is True, the United States Is Probably Conscious\n\n\nEric Schwitzgebel\n\n\n\n\nIf naturalism is true, then scientific explanation is impossible\n\n\nTomas Bogardus\n\n\n\n\nIn What Sense Is the Early Universe Fine-Tuned?\n\n\nSean M. Carroll\n\n\n\n\nIntrospection in Group Minds, Disunities of Consciousness, and Indiscrete Persons\n\n\nEric Schwitzgebel\n\n\n\n\nLeibnizian Cosmological Arguments\n\n\nAlexander R. Pruss\n\n\n\n\nLosing energy in classical, relativistic and quantum mechanics\n\n\nDavid Atkinson\n\n\n\n\nNaturalistic Dualism and the Problem of the Physical Correlate\n\n\nJonathan Schaffer\n\n\n\n\nNaturalness and Emergence\n\n\nDavid Wallace\n\n\n\n\nNew Omnivorism: a Novel Approach to Food and Animal Ethics\n\n\nJosh Milburn, Christopher Bobier\n\n\n\n\nOn What Grounds What\n\n\nJonathan Schaffer\n\n\n\n\nOn the Boundary of the Cosmos\n\n\nDaniel Linford\n\n\n\n\nOnly All Naturalists Should Worry About Only One Evolutionary Debunking Argument\n\n\nThomas Bogardus\n\n\n\n\nOppy on Thomistic cosmological arguments\n\n\nEdward Feser\n\n\n\n\nPsychophysical Harmony: A New Argument for Theism\n\n\nBrian Cutter\n\n\n\n\nReconsidering the Inverse Gambler’s Fallacy Charge Against the Fine-Tuning Argument for the Multiverse\n\n\nSimon Friederich\n\n\n\n\nRepetition and Value in an Infinite Universe\n\n\nEric Schwitzgebel\n\n\n\n\nReview of ‘Arguing about Gods’\n\n\nAlexander Pruss\n\n\n\n\nReview of ‘Necessary Existence’\n\n\nGraham Oppy\n\n\n\n\nRevisiting the argument from fetal potential\n\n\nBertha Manninen\n\n\n\n\nScepticism about the argument from divine hiddenness\n\n\nJustin McBayer, Philip Swenson\n\n\n\n\nShould a Materialist Believe in Qualia?\n\n\nDavid Lewis\n\n\n\n\nShould atheists wish there were no gratuitous evils?\n\n\nGuy Kahane\n\n\n\n\nSkeptical Theism\n\n\nJustin McBrayer\n\n\n\n\nSkepticism and the principle of sufficient reason\n\n\nRobert Koons, Alexander Pruss\n\n\n\n\nSpooky Action at a Temporal Distance\n\n\nEmily Adlam\n\n\n\n\nThe AI ensoulment hypothesis\n\n\nBrian Cutter\n\n\n\n\nThe Emergent Multiverse: Quantum Theory according to the Everett Interpretation\n\n\nPeter Lewis\n\n\n\n\nThe False Promise of ChatGPT\n\n\nNoam Chomsky\n\n\n\n\nThe Fine-Tuning Argument and the Requirement of Total Evidence\n\n\nPeter Fisher Epstein\n\n\n\n\nThe Fine-Tuning of the Universe for Intelligent Life\n\n\nLuke A. Barnes\n\n\n\n\nThe Form of the Benardete Dichotomy\n\n\nNicholas Shackel\n\n\n\n\nThe Fundamentality of Fields\n\n\nCharles T. Sebens\n\n\n\n\nThe Grim Reaper Kalam Argument: From Temporal and Causal Finitism to God\n\n\nRobert C. Koons\n\n\n\n\nThe Harder Problem of Consciousness\n\n\nNed Block\n\n\n\n\nThe Infinity from Nothing paradox and the Immovable Object meets the Irresistible Force\n\n\nNicholas Shackel\n\n\n\n\nThe Mind-Body Problem and the Color-Body Problem\n\n\nBrian Cutter\n\n\n\n\nThe Multiverse Theodicy Meets Population Ethics\n\n\nHan Li, Bradford Saad\n\n\n\n\nThe Nature of the Past Hypothesis\n\n\nDavid Wallace\n\n\n\n\nThe Nomological Argument for the Existence of God\n\n\nTyler Hildebrand, Thomas Metcalf\n\n\n\n\nThe Past Hypothesis and the Nature of Physical Laws\n\n\nEddy Chen\n\n\n\n\nThe Past Hypothesis: Not even false\n\n\nJohn Earman\n\n\n\n\nThe Problem of Creation Ex Nihilo: A New Argument Against Classical Theism\n\n\nFelipe Leon\n\n\n\n\nThe Simplicity of Physical Laws\n\n\nEddy Chen\n\n\n\n\nThe Tristram Shandy Paradox\n\n\nGraham Oppy\n\n\n\n\nThe Unreasonable Effectiveness of Mathematics in the Natural Sciences\n\n\nEugene Wigner\n\n\n\n\nThe problem of nomological harmony\n\n\nBrian Cutter and Bradford Saad\n\n\n\n\nThe reductionist blindspot\n\n\nRuss Abbott\n\n\n\n\nUniversal Library\n\n\nW. Quine\n\n\n\n\nWas evolution worth it?\n\n\nGuy Kahane\n\n\n\n\nWhat Bell did\n\n\nTim Maudlin\n\n\n\n\nWhy is There Anything?\n\n\nJoshua Rasmussen, Christopher Gregory Weaver\n\n\n\n\nWorlds in the Everett interpretation\n\n\nDavid Wallace\n\n\n\n\nYablo’s Paradox and Beginningless Time\n\n\nLaureano Luna\n\n\n\n\nZeno’s Paradoxes: A Timely Solution\n\n\nPeter Lynds\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Austin's Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nApplying 6D MENT to data\n\n\n2025‑03‑20\n\n\n\n\nPhase space spirals\n\n\n2024‑12‑27\n\n\n\n\nInitial halo-level particle-in-cell simulation benchmarks\n\n\n2024‑12‑26\n\n\n\n\nBreak apart images in Keynote\n\n\n2024‑11‑13\n\n\n\n\nNotes on The Sense of Style by Steven Pinker\n\n\n2024‑11‑10\n\n\n\n\nFour-dimensional phase space tomography from one-dimensional measurements of a hadron beam\n\n\n2024‑11‑05\n\n\n\n\nN-dimensional MENT via particle sampling\n\n\n2024‑10‑30\n\n\n\n\nThe impact of phase space correlations on the beam dynamics in linear accelerators\n\n\n2024‑10‑13\n\n\n\n\nMaximum Entropy and Bayesian Inference\n\n\n2024‑09‑29\n\n\n\n\nInaugural US Muon Collider Collaboration meeting\n\n\n2024‑08‑12\n\n\n\n\nReconstructing the 4D phase space density of a high-power proton beam from 1D measurements in the SNS ring\n\n\n2024‑06‑10\n\n\n\n\nMaximum-entropy phase space tomography using normalizing flows (part 2)\n\n\n2024‑06‑09\n\n\n\n\nMaximum-entropy phase space tomography using normalizing flows (part 1)\n\n\n2024‑06‑06\n\n\n\n\nWhy maximize entropy?\n\n\n2024‑05‑16\n\n\n\n\nPrinceton Muon Collider Workshop\n\n\n2024‑04‑01\n\n\n\n\nHigh Brightness Workshop 2023\n\n\n2024‑03‑23\n\n\n\n\nFine-tuning arguments\n\n\n2023‑07‑10\n\n\n\n\nBiological design arguments\n\n\n2023‑07‑09\n\n\n\n\nHigh-dimensional phase space measurements\n\n\n2023‑05‑28\n\n\n\n\nLeibnizian cosmological arguments\n\n\n2022‑03‑22\n\n\n\n\nSome figures to illustrate beam injection and accumulation\n\n\n2022‑02‑11\n\n\n\n\nTomographic reconstruction in four dimensions\n\n\n2021‑10‑16\n\n\n\n\nKalam cosmological arguments\n\n\n2021‑09‑15\n\n\n\n\nOntological arguments\n\n\n2021‑07‑05\n\n\n\n\nSpace charge resonances and instabilities\n\n\n2021‑07‑01\n\n\n\n\nArguing about gods\n\n\n2021‑06‑12\n\n\n\n\nPainting a particle beam\n\n\n2021‑05‑27\n\n\n\n\nComputing matched envelopes\n\n\n2021‑05‑13\n\n\n\n\nAuthorship identification\n\n\n2021‑04‑29\n\n\n\n\nNonlinear resonances\n\n\n2021‑03‑28\n\n\n\n\nParticle-in-cell simulation\n\n\n2021‑02‑22\n\n\n\n\nCoupled parametric oscillators\n\n\n2021‑01‑25\n\n\n\n\nParametric oscillators\n\n\n2021‑01‑21\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-12-27_phase-space-spirals/index.html",
    "href": "posts/2024-12-27_phase-space-spirals/index.html",
    "title": "Phase space spirals",
    "section": "",
    "text": "In the previous post I showed the first high-dynamic-range PIC simulation benchmark of the beam dynamics in the SNS Beam Test Facility (BTF). The most striking observation from these studies is that the two-dimensional phase space projections develop low-density spiral arms. Spiral patterns are generic features of nonlinear dynamics: in a linear system, all particles rotate by the same angle in phase space, but in nonlinear systems, the rotation angle depends on the particle amplitude.\nIt’s interesting to trace the development of these spirals within the accelerator. The following figures show the two-dimensional distributions on the \\(x\\)-\\(p_x\\), \\(y\\)-\\(p_y\\), and \\(x\\)-\\(y\\) planes. The coordinates are normalized such that the covariance matrix \\(\\mathbf{\\Sigma} = \\langle \\mathbf{x} \\mathbf{x}^T \\rangle = \\mathbf{I}\\), where \\(\\mathbf{I}\\) is the identity matrix and \\(\\mathbf{x} = (x, p_x, y, p_y)\\). In these coordinates, linear forces leave the distribution invariant; all changes are due to nonlinear forces. Note that the density is in log-scale.\n\nThe initially slow rotations occur before the FODO line, while the fast rotations occur in the FODO line, where the beam size oscillates rapidly. The following figure shows the root-mean-square (rms) beam size, i.e., the standard deviation of the transverse positions \\(x\\) and \\(y\\), as a function of position. The figure also shows the maximum \\(x\\) and \\(y\\) coordinates among all particles in the bunch. Notice that the rms beam size is periodic, i.e., matched to the lattice, while the maximum beam size is not matched."
  },
  {
    "objectID": "posts/2021-09-15_kalam-cosmological-arguments/index.html",
    "href": "posts/2021-09-15_kalam-cosmological-arguments/index.html",
    "title": "Kalam cosmological arguments",
    "section": "",
    "text": "These are my notes on chapter 3 of Oppy’s Arguing About Gods. This chapter is about cosmological arguments, which conclude that there is a cause or explanation of the existence of the universe. The first step in a cosmological argument is to assert a Causal Principle (CP) — that all things of a certain kind require a cause — or a Principle of Sufficient Reason (PSR) — that all things of a certain kind require an explanation. The next step is to show that the universe fits within the scope of the chosen CP or PSR. A bonus step is to identify the cause or explanation of the universe as God.\nThese arguments raise difficult questions about time, causation, infinity, physics, etc., and it’s not realistic to have a firm grasp on all these topics without further study. My goal here is to create a sort of roadmap, identifying the main points of contention. Pruss notes four main hurdles for cosmological arguments:\n\nThe Glendower Problem: How wide should the scope of the CP or PSR be?\nThe Regress Problem: Which types of infinite regresses are possible?\nThe Taxicab Problem: What caused the first cause?\nThe Gap Problem: How does one identify the first cause with God?\n\nThe arguments can be classified according to how they address these problems. The Kalam argument uses a CP to rule out an infinite past; the Thomistic argument allows for an infinite past but uses a CP to rule out infinite “vertical” causal chains; the Leibnizian argument allows for infinite causal chains but uses a PSR to demand an external explanation for such chains. I decided to dedicate one post to each of these arguments, with a final post on the Gap Problem.\nWe’ll start with the Kalam, which is simply expressed:\n\nEverything that beings to exist has a cause.\nThe universe began to exist.\nTherefore, the universe has a cause."
  },
  {
    "objectID": "posts/2021-09-15_kalam-cosmological-arguments/index.html#introduction",
    "href": "posts/2021-09-15_kalam-cosmological-arguments/index.html#introduction",
    "title": "Kalam cosmological arguments",
    "section": "",
    "text": "These are my notes on chapter 3 of Oppy’s Arguing About Gods. This chapter is about cosmological arguments, which conclude that there is a cause or explanation of the existence of the universe. The first step in a cosmological argument is to assert a Causal Principle (CP) — that all things of a certain kind require a cause — or a Principle of Sufficient Reason (PSR) — that all things of a certain kind require an explanation. The next step is to show that the universe fits within the scope of the chosen CP or PSR. A bonus step is to identify the cause or explanation of the universe as God.\nThese arguments raise difficult questions about time, causation, infinity, physics, etc., and it’s not realistic to have a firm grasp on all these topics without further study. My goal here is to create a sort of roadmap, identifying the main points of contention. Pruss notes four main hurdles for cosmological arguments:\n\nThe Glendower Problem: How wide should the scope of the CP or PSR be?\nThe Regress Problem: Which types of infinite regresses are possible?\nThe Taxicab Problem: What caused the first cause?\nThe Gap Problem: How does one identify the first cause with God?\n\nThe arguments can be classified according to how they address these problems. The Kalam argument uses a CP to rule out an infinite past; the Thomistic argument allows for an infinite past but uses a CP to rule out infinite “vertical” causal chains; the Leibnizian argument allows for infinite causal chains but uses a PSR to demand an external explanation for such chains. I decided to dedicate one post to each of these arguments, with a final post on the Gap Problem.\nWe’ll start with the Kalam, which is simply expressed:\n\nEverything that beings to exist has a cause.\nThe universe began to exist.\nTherefore, the universe has a cause."
  },
  {
    "objectID": "posts/2021-09-15_kalam-cosmological-arguments/index.html#the-universe-began-to-exist",
    "href": "posts/2021-09-15_kalam-cosmological-arguments/index.html#the-universe-began-to-exist",
    "title": "Kalam cosmological arguments",
    "section": "2. The universe began to exist",
    "text": "2. The universe began to exist\nLet’s say we’re undecided about whether the past had a beginning. We might ask modern physical theories for input on this question. We might also think “from the armchair” about whether a beginningless past is even possible. We’ll start with the former approach.\n\n2.1. Cosmology\nConsider the Standard Model (SM) of cosmology: with the assumption of an isotropic homogeneous mass distribution, general relativity produces an expanding universe solution. The solution diverges as \\(t \\rightarrow 0\\), where \\(t = 0\\) is some finite time in the past. This implies that the universe expanded from a very dense state at a finite time in the past. The SM is empirically supported for times sufficiently far from \\(t = 0\\). (The spectrum of leftover radiation from the early universe, the abundances of the light elements, and the measured Hubble Constant all agree with SM predictions.)\nThings are less clear as \\(t \\rightarrow 0\\). The singularity predicted by general relativity is taken by many to be unphysical. Quantum effects are expected to be important at these scales, and there is currently no complete theory of quantum gravity and no known way to experimentally test such a theory. Thus, the SM is undecided on whether the universe began to exist.\nStill, we can speculate about which model of the early universe is most probable. Some of them support an infinite past and others do not. Since I’m not a cosmologist, I don’t understand these models in depth. It does seem that classical physics gives some indication that the past is finite (see BGV theorem), but future physics may turn the tide. Thus, we should be cautious when using physics to support a strong claim about whether the universe began to exist.\n\n\n2.2. Finitism\n\n2.2.1. Hilbert’s Hotel\nWe now move to philosophical arguments against an infinite past. The first view to discuss is finitism, the idea that infinity never shows up in the real world. The benefit of finitism is that it rules out some oddities like Hilbert’s Hotel — a hotel with infinitely many rooms. Even if all the rooms are occupied, the hotel can always accept an additional guest. If all guests in odd-numbered rooms leave, of which there are an infinite number, then an infinite number of guests remain. If all guests in room numbers &gt; 7 leave, of which there are an infinite number, then exactly 7 guests remain.\nThe question is whether this story precludes the existence of Hilbert’s Hotel. It’s not clear that it does; it might just describe the strange rules the hotel would obey if it existed. The key issue is that a subset of an infinite set is another infinite set. Since the operations done on Hilbert’s Hotel can be done on the natural numbers as well, while Hilbert’s Hotel might be strange, it’s not clear why the reasons for ruling out its possibility wouldn’t also apply to infinite mathematical sets.\n\n\n2.2.2. An endless future?\nShowing that a completed infinite is impossible to instantiate is only helpful to the Kalam if the series of events in a beginningless past is a completed infinite. The answer to this question depends on the relationship between the past, present, and future. In my understanding, there are three main views: all times exist (four-dimensionalism), present times exist (presentism), or past and present times exist (growing block). The set of future events in an endless future would form an actual infinite on four-dimensionalism, which would then be ruled out by finitism, which is bad. The Kalam gains nothing on presentism since there is only ever one time that exists. I’m not sure which theory of time is correct, but I would initially lean toward presentism or four-dimensionalism.\nThere is a causal asymmetry between an infinite future and an infinite past. An infinite past allows an infinite number of past events to affect the state of the world at a given time, while there is no such problem in an infinite future. (This will be discussed in the next section on causal finitism.)\n\n\n2.2.3. Counting to infinity\nSuppose finitism is false. The following argument could then be run: (i) the collection of temporal events is formed by successive addition; (ii) a collection formed by successive addition cannot be an actual infinite; (iii) the temporal series of events cannot be an actual infinite.\nThe idea is that getting to “now” in a beginningless universe is like traversing an infinite set, which is an impossible task like counting all the negative integers: …, -3, -2, -1, 0. And this counting example raises a question: suppose I counted all the negative integers; why did I finish when I did? I should have finished an infinite time ago.\nThese counting tasks assume there was a time at which I began counting, i.e., counting all the positive integers starting from zero. But there was no time at which I began counting. I counted 0 today, -1 yesterday, etc. So, it seems to be a coherent story. Yet even though it’s a coherent story, it feels uncomfortable to be left with this infinite regress of explanations. This kind of consideration is central to the Leibnizian argument.\n\n\n\n2.3. Causal finitism\nCausal finitism is the idea that every event has a finite causal history.{% fn 1 %} Causal finitism’s advantage over finitism is that it doesn’t touch abstract mathematical objects since they can’t cause anything. The case for causal finitism given by Pruss in Infinity, Causation, and Paradox is that causal finitism provides a unified way to kill a wide range of paradoxes; this section looks at a few of these.\n\n2.3.1. Grim Reapers\nThompson’s lamp is off at t = 0. I turn it on at t = ½, off at t = ¾, on at t = 7/8, and so on until t = 1. Is the lamp on or off at t = 1? It seems there is no way to answer this question. But although this situation is strange, it’s hard to get a real paradox without appealing to a PSR.\nThe Grim Reaper paradox is more troublesome. I’m alive at \\(t = 0\\) along with an infinite number of sleeping Grim Reapers (GR). Each GR has an alarm set to a time \\(0 \\le t \\le 1\\); when a GR’s alarm goes off, it wakes up and kills me if I’m alive, otherwise it goes back to sleep.\nLet’s label the alarm time for GR \\(n\\) as \\(t_n\\), where \\(n\\) can be any natural number. Suppose \\(t_n = 1 / 2^n\\). I’ll be dead at all \\(t &gt; 0\\): I couldn’t be alive at \\(t = 1\\) because GR 1 would have already killed me, I couldn’t be alive at \\(t = 1/2\\) because GR 2 would have already killed me, and so on. But none of the GRs killed me: for each GR that could have killed me, there was always a GR that came before. Since my well-being at \\(t = 1\\) is caused by infinitely many GRs, causal finitism kills the paradox.\nOne way to resolve the paradox is to say that the sum of the GRs killed me; however, the sum of all GRs doing nothing is nothing. We could also say that my death was uncaused; this will be discussed with the first premise of the Kalam. Or we could say that time is discrete. (If time is discrete, the paradox can remain alive if the GRs are spread out at equal intervals into the eternal past: t = -1, t = -2, etc. Again, no GR killed me, but I must be dead at t = 0. The paradox is a bit different since there is no time at which I was alive. Beginningless sequences such as these will be discussed with the Leibnizian argument.)\nThe best way to resolve the paradox without causal finitism is the Unsatisfiable Pair Diagnosis (UPD). The UPD says that the situation is impossible because it leads to a paradox. There are two conditions: (A) there is a beginningless sequence, and (B) E occurs at n iff E has not occurred before n. We’re then claiming that A and B can’t both be true at the same time. One example is that “Austin is taller than Paris” and “Paris is taller than Austin” could be true individually but not together.\nThe crucial question here is whether it’s possible to reach the GR scenario from nearby unparadoxical scenarios; this is known as rearrangement. For example, there is no problem if \\(t_n = 1 – 1 / 2^n\\) since I would die at t = 0.5 and remain dead, and if the alarms could be set to these values, why not the original scenario? Or the original scenario could be modified by adding a GR at \\(t_0 \\le 0\\): I would be killed by GR 0 at \\(t_0\\). In this case, we only need to remove GR 0 to get back to the paradox. The proponent of the UPD is going to have to call into question the possibility of rearrangement in these cases.\n\n\n2.3.2. Newtonian universes\nNewtonian physics is false, but it seems there would be nothing inconsistent about a world that obeys Newtonian physics. For example, space could be infinite in extent and an infinite number of particles could collectively cause the motion of another particle, violating causal finitism. An example is a collection of particles spread evenly over an infinite plane. But variation of the initial conditions leads to bizarre results. Pruss uses the example of particles spread evenly over “half” of an infinite plane: the force on a particle on the edge of the distribution will be infinite, so it will be nowhere as soon as any time passes. There are other fun examples. Causal finitism kills these paradoxes.\n\n\n2.3.3. Infinite lotteries\nAn infinite (fair) lottery is a lottery with an infinite number of tickets, each of which has zero or infinitesimal chance of winning. The claim is that infinite lotteries are absurd, but possible on causal infinitism.\nLet’s start with the absurdity of an infinite lottery. We can label each ticket with a natural number. Let’s say I draw ticket N but don’t look at the number. Then, for each natural number \\(n\\), I guess whether \\(N &gt; n\\). I get a dollar if I’m right, but I lose a dollar if I’m wrong. I should always guess that \\(N &gt; n\\), but I’ll lose an infinite number of times with this strategy.\nOr suppose there are \\(10^{10^10}\\)coins flipped and I’m asked to guess whether any coins came up heads. Before I guess, I’m also given a random number \\(n\\). If any of the coins came up tails, \\(n\\) was generated from an infinite fair lottery; if none of the coins came up tails, \\(n\\) was generated from a lottery in which the probability of drawing \\(n\\) is \\(p_n = 1 / 2^n\\). Since \\(1 / 2^n\\) is always larger than an infinitesimal, I should always guess that \\(n\\) didn’t come from the infinite lottery, and hence that no coins came up heads.\nIt’s also possible to raise the winning probability of every ticket by replacing the infinite fair lottery with the \\(p_n = 1 / 2^n\\) lottery. There are other examples.\nThe next claim to investigate is whether causal finitism can rule out infinite lotteries. There is a technical section in the book related to this question, but here I’ll mention the simplest case: random walker Bob. For every day in a beginningless past, Bob takes one step in a randomly chosen direction — left or right — and writes down his position on a piece of paper. On a random day, Bob writes also writes “winner” on the piece of paper. Thus, an infinite fair lottery has been generated when Bob arrives at today. (The lottery is fair because the probability of any given position being the winner is infinitesimal.) Causal finitism rules out this story because the position of Bob at any point in the story is caused by an infinite number of previous positions.\nThe most promising way to kill the paradoxes without causal finitism is to note that human reasoning shouldn’t be expected to work with infinities; that’s fine, but causal finitism might be more attractive because it can also rule out paradoxes that don’t involve human reasoning. This whole discussion is pretty mathy and would take some time for me to understand it well.\n\n\n2.3.4. Decisions\nEvery minute in a beginningless past, a die is rolled. On each roll, I’m asked to guess whether the die landed on four. The penalty for a wrong answer is an electric shock. Obviously, I should always guess “no”. Now suppose I have access to the infinite number of previous rolls, that I know the game will end at some point, and that only finitely many non-fours have been rolled in the past. I now know with certainty that there will only be finitely many non-fours rolled for the rest of the game, so I can guess “yes” from now on and ensure a finite number of total shocks as opposed to the infinite number of total shocks I would normally receive. The paradox is that each roll is independent, so knowledge of previous rolls shouldn’t improve your future guesses. Causal finitism wouldn’t allow decisions to be made based on an infinite number of previous rolls.\n\n\n2.3.5. The Axiom of Choice\n[There is a chapter in the book devoted to paradoxes involving the Axiom of Choice. I’ve avoided this chapter until now because it looks like it would take some time to digest, but maybe I will read it at some point and replace this bracketed text with a summary.]\n\n\n2.3.6. Summary\nThere are several issues to resolve in relation to causal finitism. First, if it were true, it might raise the probability that spacetime is discrete. Second, its usefulness for killing paradoxes will depend on the nature of causation.\nCausal finitism leads to the existence of at least one uncaused cause — just trace each causal chain back to its origin. (It does allow for an infinite past in which different regions of an infinitely large universe are causally isolated; however, each of these isolated regions would need a first cause.) The details of the connection between causal finitism and the second premise of the Kalam are explored by Koons in his article linked at the bottom of the post. What remains is the Gap Problem: what is the nature of the first cause(s)?\nOkay, to my knowledge, those are some of the main arguments for the second premise of the Kalam."
  },
  {
    "objectID": "posts/2021-09-15_kalam-cosmological-arguments/index.html#everything-that-begins-to-exist-has-a-cause",
    "href": "posts/2021-09-15_kalam-cosmological-arguments/index.html#everything-that-begins-to-exist-has-a-cause",
    "title": "Kalam cosmological arguments",
    "section": "3. Everything that begins to exist has a cause",
    "text": "3. Everything that begins to exist has a cause\nThe first premise of the Kalam says that everything that begins to exist has a cause. I’d like to accept this Causal Principle (CP). It’s intuitive and seems to be supported by empirical evidence. Oppy pushes back in two ways. First, the intuitive support for the premise can be questioned. There is much debate about causation; some models take causation to be fundamental to physics, others see causation as a useful fiction, and others do away with it altogether. For example, while causation looks to be fundamental when billiard balls collide, it’s not so clear in the case of interacting quantum fields. (My intuition tells me that causation is fundamental to physics.)\nSecond, the empirical support for the CP can be questioned. The pushback is that we only observe things that began to exist at \\(t &gt;0\\), while the universe began to exist at \\(t = 0\\) (if there was a first moment of time), and that we can’t generalize observations from non-initial times to the initial time. I understand this worry, but I also think that the empirical support for the CP is strong enough that I could just treat the CP as a default rule unless I have strong reasons to think that it’s false in a certain case. In this case, I don’t see an obvious reason why the collection of things (the universe) violates the CP while each individual thing does not.\nNow, one might offer some reason why the collection of things (the universe) violates the CP while each individual thing does not. For example, if time began, then the universe didn’t “pop into existence”; instead, it always exists in the sense that it exists at every time. I think what people are getting at here is the idea that there could be nothing external to or prior to the universe, and since the cause of the universe would be external to it and causally prior to it, it is meaningless to ask what caused the universe. This feels a bit tricky to me, though. The beginning of the universe is an event in time, and of every event, we can ask whether it has a cause. So, if the universe began to exist, then the beginning of the universe was either an uncaused event or it was a caused event."
  },
  {
    "objectID": "posts/2021-09-15_kalam-cosmological-arguments/index.html#conclusion",
    "href": "posts/2021-09-15_kalam-cosmological-arguments/index.html#conclusion",
    "title": "Kalam cosmological arguments",
    "section": "4. Conclusion",
    "text": "4. Conclusion\nThere are difficult subjects at play in the Kalam cosmological argument like causation, infinity, and cosmology. I’m tempted to refuse to evaluate the argument I have a better handle on these subjects, but I should probably evaluate the premises to the best of my ability right now.\nThe universe began to exist.\n\nModern cosmology is undecided on this premise.\nThe philosophical arguments given by Craig aren’t super convincing.\nSome version of causal finitism is probably true.\nIt’s strange to imagine that space and time began to exist, but it is less strange than an infinite past.\nEvaluation: Probably true.\n\nWhatever begins to exist has a cause.\n\nIt’s intuitive.\nI have no strong reasons to abandon this when it comes to the universe, but does anything change when something begins to exist at the beginning of time as opposed to some later time?\nThe principle “from nothing, nothing comes” is valid.\nIt might be more helpful to talk about the PSR and CP together.\nEvaluation: Probably true, but need to think more about edge cases (quantum mechanics, free will)."
  },
  {
    "objectID": "posts/2021-09-15_kalam-cosmological-arguments/index.html#some-references",
    "href": "posts/2021-09-15_kalam-cosmological-arguments/index.html#some-references",
    "title": "Kalam cosmological arguments",
    "section": "5. Some references",
    "text": "5. Some references\n\nCausality\n\nAristotle On Causality (SEP)\nCausation as Folk Science (Norton)\nCaustion in Physics (SEP)\n\nCosmology\n\nThe case for the relativistic hot Big Bang cosmology (Peebles et. al.)\nPhilosophy of Cosmology (SEP)\nPhilosophy and Theology (SEP)\n\nGeneral\n\nCosmological Argument (SEP\nCausation and Sufficient Reason (Leon)\nInfinity (SEP)\n\nCausal finitism\n\nInfinity, Causation, and Paradox (Pruss)\nThe Form of the Benardete Dichotomy (Shackel)\nThe Grim Reaper Kalam Argument (Koons)\nSupertasks (SEP)\nYablo’s paradox and beginningless time (Luna)\n\nCraig’s Kalam\n\nCraig on the Actual Infinite (Morriston)\nCraig’s Contradictory Kalam: Trouble at the Moment of Creation (Wielenberg)\nNo Trouble: A Reply to Wielenberg (Craig)"
  },
  {
    "objectID": "posts/2021-06-12_arguing-about-gods/index.html",
    "href": "posts/2021-06-12_arguing-about-gods/index.html",
    "title": "Arguing about gods",
    "section": "",
    "text": "I’m reading the book Arguing About Gods by Graham Oppy, one of the most prominent philosophers of religion  [1]. Oppy’s main thesis is that there are no successful arguments for the existence of orthodoxly conceived monotheistic gods, where “orthodoxly conceived monotheistic god” refers to an omnipotent, omniscient, omnibenevolent creator of the world — I’ll take “God” to mean the same thing — and “successful arguments” will be defined later. The book is organized as follows:\n\nPreliminary Considerations\nOntological Arguments\nCosmological Arguments\nTeleological Arguments\nPascal’s Wager\nArguments from Evil\nOther Arguments\nConcluding Remarks\n\nI’m planning to write a post for each of these chapters. In each post, I’ll try to summarize Oppy’s ideas, discuss known challenges to these ideas if I’m aware of them, and record my thoughts and/or questions as they arise.1 This post will focus on the first chapter.\nBefore I begin, I should mention what I hope to achieve during this study. My general goal is to think more carefully about each of the following questions (G stands for the question of the existence of God):\n\nHow, and on what basis, do I answer G?\nHow confident am I in my answer to G?\nCould my answer to G change in the future?\nWhat is the relationship between my answer to G and my other beliefs?\nDo I — or should I — treat G in the same way as I treat other questions? Why or why not?\nHow should I understand disagreement about G between equally intelligent people?\nWhat role should internal evidence — intuitions, feelings, things I just take to be true — play when forming an answer to G?\n\nI’ll briefly give my current thinking on “How confident am I in my answer to G?”. A helpful concept discussed in the book and many other places is that of credence: my credence x in a proposition S is a value ranging from 0 to 1 which represents how confident I am that S. This contrasts with the three traditional responses: belief, disbelief, or withheld judgment. But perhaps it’s too difficult to choose a specific value for x; maybe it’s better to choose a credence interval r = (p, q) such that x is somewhere within this interval; perhaps r = (0.9, 1) if I’m very confident, r = (0, 0.1) if I’m not very confident, etc.2 It’s also still possible to choose a zero-width interval like r = (0.33, 0.33). If S = “God exists”, then the atheist’s credence interval may be bounded by (0, 0.5), the theist’s by (0.5, 1), and the agnostic’s by (0.5, 0.5).\nMy inclination is to make r quite narrow and centered on 0.5; I don’t think this is a good or bad thing — it’s just how my brain works. I’d be tempted to call this agnosticism. My actual credence interval, however, might be something like (0.45, 0.65). I’m attracted by several arguments for theism, but also by a couple of arguments for atheism. Overall, I’m more convinced by arguments for theism than for atheism, hence the higher upper bound. There are also other issues at play, but that’s the short explanation for my choice.\nIt’s hard to know if the numbers I chose are accurate.3 I mean “accurate” in the following sense: maybe I would choose an interval with a different width or mean if a gun was held to my head or if I thought about the question in more depth. I’ll hopefully have an improved credence interval by the end of this study. This may sound strange: why not change my interval if I don’t think it’s accurate? Part of the reason is that I’ve only recently begun to consider credence and belief as distinct, so I’d like to think more about how I’m choosing my interval. For example, what is my credence interval for the proposition “I won’t die at an early age”, and how does this relate to my belief that I won’t die at an early age? I’m also expecting my interval to shift as I think more about G.\nI’ll now move on to summarizing and commenting on the four sections of this first chapter."
  },
  {
    "objectID": "posts/2021-06-12_arguing-about-gods/index.html#introduction",
    "href": "posts/2021-06-12_arguing-about-gods/index.html#introduction",
    "title": "Arguing about gods",
    "section": "",
    "text": "I’m reading the book Arguing About Gods by Graham Oppy, one of the most prominent philosophers of religion  [1]. Oppy’s main thesis is that there are no successful arguments for the existence of orthodoxly conceived monotheistic gods, where “orthodoxly conceived monotheistic god” refers to an omnipotent, omniscient, omnibenevolent creator of the world — I’ll take “God” to mean the same thing — and “successful arguments” will be defined later. The book is organized as follows:\n\nPreliminary Considerations\nOntological Arguments\nCosmological Arguments\nTeleological Arguments\nPascal’s Wager\nArguments from Evil\nOther Arguments\nConcluding Remarks\n\nI’m planning to write a post for each of these chapters. In each post, I’ll try to summarize Oppy’s ideas, discuss known challenges to these ideas if I’m aware of them, and record my thoughts and/or questions as they arise.1 This post will focus on the first chapter.\nBefore I begin, I should mention what I hope to achieve during this study. My general goal is to think more carefully about each of the following questions (G stands for the question of the existence of God):\n\nHow, and on what basis, do I answer G?\nHow confident am I in my answer to G?\nCould my answer to G change in the future?\nWhat is the relationship between my answer to G and my other beliefs?\nDo I — or should I — treat G in the same way as I treat other questions? Why or why not?\nHow should I understand disagreement about G between equally intelligent people?\nWhat role should internal evidence — intuitions, feelings, things I just take to be true — play when forming an answer to G?\n\nI’ll briefly give my current thinking on “How confident am I in my answer to G?”. A helpful concept discussed in the book and many other places is that of credence: my credence x in a proposition S is a value ranging from 0 to 1 which represents how confident I am that S. This contrasts with the three traditional responses: belief, disbelief, or withheld judgment. But perhaps it’s too difficult to choose a specific value for x; maybe it’s better to choose a credence interval r = (p, q) such that x is somewhere within this interval; perhaps r = (0.9, 1) if I’m very confident, r = (0, 0.1) if I’m not very confident, etc.2 It’s also still possible to choose a zero-width interval like r = (0.33, 0.33). If S = “God exists”, then the atheist’s credence interval may be bounded by (0, 0.5), the theist’s by (0.5, 1), and the agnostic’s by (0.5, 0.5).\nMy inclination is to make r quite narrow and centered on 0.5; I don’t think this is a good or bad thing — it’s just how my brain works. I’d be tempted to call this agnosticism. My actual credence interval, however, might be something like (0.45, 0.65). I’m attracted by several arguments for theism, but also by a couple of arguments for atheism. Overall, I’m more convinced by arguments for theism than for atheism, hence the higher upper bound. There are also other issues at play, but that’s the short explanation for my choice.\nIt’s hard to know if the numbers I chose are accurate.3 I mean “accurate” in the following sense: maybe I would choose an interval with a different width or mean if a gun was held to my head or if I thought about the question in more depth. I’ll hopefully have an improved credence interval by the end of this study. This may sound strange: why not change my interval if I don’t think it’s accurate? Part of the reason is that I’ve only recently begun to consider credence and belief as distinct, so I’d like to think more about how I’m choosing my interval. For example, what is my credence interval for the proposition “I won’t die at an early age”, and how does this relate to my belief that I won’t die at an early age? I’m also expecting my interval to shift as I think more about G.\nI’ll now move on to summarizing and commenting on the four sections of this first chapter."
  },
  {
    "objectID": "posts/2021-06-12_arguing-about-gods/index.html#arguments-about-the-existence-of-monotheistic-gods",
    "href": "posts/2021-06-12_arguing-about-gods/index.html#arguments-about-the-existence-of-monotheistic-gods",
    "title": "Arguing about gods",
    "section": "Arguments about the existence of monotheistic gods",
    "text": "Arguments about the existence of monotheistic gods\nThis section explains how the various arguments for and against the existence of God have been organized in the book. First, ontological arguments appeal to reason alone. Second, cosmological arguments appeal to necessary features of the world. Third, teleological arguments appeal to contingent features of the world. Fourth, moral arguments appeal to the moral structure of the world. Fifth, arguments such as Pascal’s wager simply claim that belief in God is rational. Sixth, there are also “minor” arguments such as the argument from consciousness or the argument from beauty. Finally, there are arguments from personal religious experience, corporate religious experience, etc.\nAn interesting point is made about cumulative arguments. If there is a set of n arguments, all with the same conclusion, is the entire set stronger than any of the individual members? Not necessarily. For example, all the arguments could be strong except for one, which is circular; in this case, a mega-argument formed from all the arguments in the set would also be circular and hence weaker than any of the strong arguments in isolation. The mega-argument could be stronger than any individual argument if the arguments are viewed probabilistically, i.e., each argument uses some evidence to raise or lower the probability of its conclusion. Oppy doesn’t like this idea; he notes that each argument only considers a carefully selected portion of the full body of evidence and that the correct thing to do is to consider all the evidence at once. Discussion is left for later chapters.\nAlexander Pruss raises the following problems with this treatment of cumulative arguments in his review. Suppose each argument has one premise that could be rejected by a reasonable person who holds a particular view —a moral realist will reject this premise, a materialist will reject that premise, etc. But unless each premise could be rejected on the same grounds, it doesn’t follow that a single person could hold all these views at the same time without contradictions. Pruss is also critical of Oppy’s suggestion that all the evidence must be considered at the same time if arguments are viewed probabilistically; instead, only the evidence relevant to a given statement is required to evaluate that statement. I’m keen to align with Pruss here."
  },
  {
    "objectID": "posts/2021-06-12_arguing-about-gods/index.html#arguments",
    "href": "posts/2021-06-12_arguing-about-gods/index.html#arguments",
    "title": "Arguing about gods",
    "section": "Arguments",
    "text": "Arguments\nThis section discusses what makes arguments successful. Oppy says this requires “(1) an account of rationality and rational belief revision, (2) an account of arguments, (3) an account of rational argumentation amongst rational agents, and (4) an account of the difficulties that arise as a result of the fact that we are not perfectly rational agents”.\nIt’s claimed that it’s possible for two perfectly rational people to disagree even if they have access to the same evidence. This seems wrong at first. The reasoning is: 1) the order in which the evidence is processed might be important, 2) there is a limit to how much information can be stored in a person’s mind at one time, and further 3) it could be argued that we generally make decisions in a Bayesian sense, evaluating the probability of the evidence given a set of prior probabilities or prior beliefs. In this way, every new belief is formed against the background of other beliefs. If this is true of perfectly rational people, then, of course, we can expect disagreement between less-than-perfectly rational people.\nIt’s also claimed that the point of arguments is to cause reasonable belief revision.4 The following definition of a successful argument is given:\n\nThe most successful argument would be one that succeeds — or perhaps would or ought to succeed — in persuading any reasonable person to accept its conclusion; good, but less successful arguments would succeed — or perhaps would or ought to succeed — in persuading a non-zero percentage of reasonable people to accept their conclusions.\n\nThus, successful arguments are extremely difficult to form. Imagine I’m trying to change Alice’s mind. I should form an argument whose conclusion follows from premises Alice already believes; I’ll otherwise need to convince Alice of the premises using separate arguments. There’s nothing inherently wrong with this, but we may eventually reach beliefs for which there are no appeals to other beliefs for justification, and there is no hope of changing Alice’s mind if we disagree about those, at least not using arguments. If this view is correct, then arguments might not be the best way to persuade other reasonable people.\nThat it’s difficult to change the mind of a reasonable person is further supported by observing myself. I remember having long (friendly, non-serious) debates with my friends in college about various topics. I recall that, in some of these discussions, I didn’t even have a strong opinion on the topic beforehand. Yet, it was quite difficult to change the belief I formed at the beginning of the debate. I would think: “I formed this belief five minutes ago based on my intuition, and my intuition is infallible!” In hindsight, I was wrong some of the time and my arguments were likely weak. The fact that I had a strong resistance to changing my beliefs, even though I had very little to lose by giving them up, demonstrates that it would be difficult to change my mind about my core beliefs using arguments alone.\nThere are some questions about this view of successful arguments. Notice the phrase “would or ought to succeed”. Suppose there’s a premise h that any reasonable person would or ought to accept. I take “would” to mean that Austin would accept h if he had a certain set of priors and if his reasoning faculties were functioning correctly, and I take “ought” to mean that Austin ought to accept h since he has a certain set of priors and since his reasoning faculties are functioning correctly. One question is whether there are constraints on a reasonable person’s priors. On this, Oppy writes:\n\nI am not committing myself to the claim that there are no substantive constraints on ‘priors’ — it may be that there are quite severe constraints on reasonable sets of ‘priors’; however, I am claiming that I can see no reason at all for thinking that there is a unique set of ‘priors’ that any reasonable person must have on pain of conviction of irrationality.\n\nHow can we know that Austin’s priors obey these constraints? Pruss has some things to say about a related issue in the beginning of his review.\nI also want to mention a potential problem for theists who accept that there can be disagreement about God between reasonable truth-seeking people. This is the problem of divine hiddenness: it seems plausible that God would guide all reasonable truth-seekers to true beliefs about God if God existed, but there are reasonable truth-seekers who don’t believe in God. Perhaps this will come up in future chapters."
  },
  {
    "objectID": "posts/2021-06-12_arguing-about-gods/index.html#some-considerations-about-agnosticism",
    "href": "posts/2021-06-12_arguing-about-gods/index.html#some-considerations-about-agnosticism",
    "title": "Arguing about gods",
    "section": "Some considerations about agnosticism",
    "text": "Some considerations about agnosticism\nIn this section, Oppy distinguishes between weak agnosticism and strong agnosticism. He then argues that strong agnosticism fails, but that weak agnosticism is acceptable. The strong agnostic’s view can be summarized as:\n\nIn circumstances in which the available evidence no more — and no less — supports \\(p\\) than it supports logically incompatible hypotheses \\(p_1\\), \\(\\dots\\), \\(p_n\\), \\(\\dots\\), one ought to suspend judgment between \\(p\\), \\(p_1\\), \\(\\dots\\), \\(p_n\\), \\(\\dots\\) as well as the hypothesis that \\(p\\) is false.\nThere are infinitely many God hypotheses, none of which are more likely than the others given the available evidence (e.g., a god who is not quite omniscient, a god who is not quite omnipotent, two gods rather than one, etc.).\nIt is neither rational to believe that God exists, nor to believe that God does not exist.\n\nLet’s start with (1). It wasn’t obvious to me why we shouldn’t think that p is false in this case. There is, however, a paradox that might illustrate the reason. Consider a lottery with infinitely many tickets. I shouldn’t believe that my ticket is the winner since the probability is infinitesimal.5 But I should think this about each ticket, which amounts to believing that no ticket is the winner — a false belief. So, I should neither believe nor disbelieve that any particular ticket is the winner (that God exists).\nSo, (1) seems to make sense; however, there is a major problem: believing (1) could make it very difficult to avoid skepticism about basic things like the reality of my perceived world. Consider my belief that I am not a brain in a vat (BIV). On what basis do I believe this? My belief can’t be based on external evidence because the BIV hypothesis explains this evidence just as well as the alternative. Rather, it must be due to some internal evidence; in other words, the belief is basic.\nIf I accept that some beliefs are basic and accept the principle that it’s best to keep a belief unless there is some good reason to give it up (Oppy calls this “methodological conservatism”), then I have good reason to believe I’m not a BIV. The strong agnostic doesn’t agree with this and seems forced to suspend judgment on whether they are a BIV. The same would go for the belief that I didn’t pop into existence two seconds ago, and so on.\nThe strong agnostic needs to make some distinction between the God hypotheses and the cases involving extreme skepticism; however, there’s no clear way to do this. Oppy notes that the best approach is probably to say that my skepticism about the external world requires me to believe I’m special — somehow, I’m the only real thing in my perceived world — and that this gives me a reason to reject my skepticism. Call this the “I’m special” consideration ISC. However, Oppy proceeds, if the strong atheist concedes that the ISC is required to distinguish between these cases, then they have conceded that external evidence is not the only thing that can support reasonable beliefs, for the reason why the ISC should count against the truth of a belief is not clear from external evidence even though it may be clear to the strong agnostic. So, the problem is not that the strong agnostic is unreasonable for liking the ISC, it’s that they might be forced to choose between extreme skepticism and admitting that theists and atheists can be reasonable. In other words, the strong agnostic cannot believe (3) and the following statement at the same time:\n\n\nIt is possible to characterize a suitable notion of evidential support that does not rely upon a relativization to background assumptions.\n\n\nThis last view — suspending belief or non-belief in God while recognizing that this is not the only reasonable way to proceed — is weak agnosticism."
  },
  {
    "objectID": "posts/2021-06-12_arguing-about-gods/index.html#alternative-deities",
    "href": "posts/2021-06-12_arguing-about-gods/index.html#alternative-deities",
    "title": "Arguing about gods",
    "section": "Alternative deities",
    "text": "Alternative deities\nThe final section of this chapter considers the arguments for alternative deities like God*. God* is exactly like God but is completely evil. The thought is that the arguments for God work just as well for God*. For example, one way to respond to the problem of evil is to adopt skeptical theism, and it seems that this view could also be used as a defense of God*.6 Any arguments for which this applies would then seem to be ineffective as attacks against non-theists. A further step would be to claim that there is no way to choose God over God* and that theists should therefore give up their belief in God.\nIf one thinks that moral facts exist and can be known, which I think I do, then the following critique will be convincing. An omniscient being would know all moral facts, but it’s impossible to believe that an action is wrong without being inclined to refrain from performing that action, i.e., I couldn’t know that murder is wrong but have no problem carrying out murder myself. Therefore, God* is logically impossible.\nIf God* is off the table, then God’ could be introduced. God’ is as close to God* as possible: completely evil, but not quite omniscient. Again, the arguments for God seem to work for God’. If the idea of God’ is shown to be incoherent, maybe we could think of God’’, God’’’, God#, etc. Oppy thinks these alternative deities could be useful to the non-theist as a defense, but not as an attack. This is because the theist likely has a set of prior beliefs that make an orthodoxly conceived monotheistic god seem the most likely to them even if they accept that some arguments don’t nail down God’s moral character. In general, it’s supposed that we shouldn’t be forced to give up a belief just because we recognize the existence of alternatives that seem just as good to an outside observer."
  },
  {
    "objectID": "posts/2021-06-12_arguing-about-gods/index.html#conclusion",
    "href": "posts/2021-06-12_arguing-about-gods/index.html#conclusion",
    "title": "Arguing about gods",
    "section": "Conclusion",
    "text": "Conclusion\nThis chapter was primarily about epistemology. I haven’t thought much about this topic — almost not at all before recently reading Alvin Plantinga’s Knowledge and Christian Belief — and I think that articulating my epistemological views is necessary to answer the questions I listed at the beginning of this post. A good first step might be to read the relevant entry in the Stanford Encyclopedia of Philosophy.\nI’m looking forward to the remaining chapters in the book in which specific arguments for the existence of God are examined."
  },
  {
    "objectID": "posts/2021-06-12_arguing-about-gods/index.html#footnotes",
    "href": "posts/2021-06-12_arguing-about-gods/index.html#footnotes",
    "title": "Arguing about gods",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI won’t use direct quotes very often; hopefully it’s clear when I’m paraphrasing the book vs. discussing the book.↩︎\nI probably shouldn’t choose [0, q) or (p, 1]; in this case, I would possibly think there is zero chance that S is true (or false).↩︎\nSome argue that credences don’t exist in reality; see [2].↩︎\nOppy switches from “rational” to “reasonable”, and I’m sometimes confused by the difference.↩︎\nAn infinitesimal is smaller than every number but greater than zero.↩︎\nSkeptical theism is the view that we’re not in a position to know God’s thoughts, motives, etc. in any specific situation.↩︎"
  },
  {
    "objectID": "posts/2024-03-23_hb/index.html",
    "href": "posts/2024-03-23_hb/index.html",
    "title": "High Brightness Workshop 2023",
    "section": "",
    "text": "I spoke at the Workshop on High-Brightness and High-Intensity Hadron Beams (HB) at CERN a few months ago. The workshop drew around 200 attendees from the United States, United Kingdom, South Korea, China, Japan, Germany, Italy, France, and Switzerland. All slides are available online. Here are some notes and highlights:"
  },
  {
    "objectID": "posts/2024-03-23_hb/index.html#operations-and-commissioning",
    "href": "posts/2024-03-23_hb/index.html#operations-and-commissioning",
    "title": "High Brightness Workshop 2023",
    "section": "Operations and commissioning",
    "text": "Operations and commissioning\n\nThe SNS has reached 1.7 MW beam power and should reach 2.8 MW this summer.\nFRIB has commissioned their accelerator and started generating data. They have a fascinating research program studying rare isotopes near the nuclear drip line.\nThe European Spallation Source (ESS) has started commissioning its linac. Mamad Eshraqi talked about the practical challenges of building a brand new complex in Lund, Sweden, including environmental and radiational licenses and the impact on the community. I hadn’t considered those issues before. It’s interesting to see this process unfolding since I wasn’t around for the SNS commissioning 20 years ago.\nChina is rapidly developing several large-scale accelerator facilities. In addition to the recently commissioned China Spallation Source (CNS), they will soon introduce the Heavy Ion Accelerator Facility (HIAF), which will study nuclear physics phenomena like FRIB. HIAF will deal with all sorts of collective effects, and their GPU-accelerated end-to-end simulations are a powerful tool for modeling them. The China Initiative Accelerator Driven System (CiADS) is under construction on the same campus. CiADS will compete with MYRRHA to demonstrate the first accelerator-driven system, using a high-power linear accelerator to drive a subcritical nuclear reactor. I don’t know if the US will ever engage in similar efforts.\nAfter operating for many years, the JPARC, Fermilab, ISIS, and LHC rings are being upgraded to higher energy/intensity. Most of these talks followed a similar pattern, reporting that small changes to the injection scheme, optics, etc., reduced losses."
  },
  {
    "objectID": "posts/2024-03-23_hb/index.html#beam-dynamics-in-rings",
    "href": "posts/2024-03-23_hb/index.html#beam-dynamics-in-rings",
    "title": "High Brightness Workshop 2023",
    "section": "Beam dynamics in rings",
    "text": "Beam dynamics in rings\n\nAdrian Oeftiger (GSI) gave a fantastic talk on the space charge limit in the FAIR synchrotron. He compared three space charge models: a fixed frozen model that assumes the same Gaussian distribution on each turn, an adaptive frozen model that scales a Gaussian to the second-order moments on each turn, and a self-consistent particle-in-cell model. Despite its lack of self-consistency, the fixed frozen model predicts the loss-free regions of tune space over hundreds of thousands of turns. The beams’ nonuniform charge distribution washes out low-order coherent instabilities through Landau damping, rendering the particle-in-cell approach unnecessary. Adrian’s simulations also showed that pulsed electron lenses—small electron beam currents that partially cancel the beam’s space charge force—could increase the beam intensity by almost 100%. What is the space charge limit in a machine like the SNS, which operates with many fewer turns and a noon-gaussian charge density.\nCristhian Gonzalez-Ortiz (MSU) has done excellent work on resonance diagram measurements and correction schemes using sextupoles in the Fermilab booster ring.\nNick Evans (ORNL) presented our work on self-consistent injection painting (eigenpainting) to generate bright, spinning hadron beams.\nTimeofey Gorlov (ORNL) discussed the laser-assisted charge exchange (LACE) research at ORNL. We currently strip electrons from injected H- particles using thin carbon foils. LACE aims to replace the foils with lasers, enabling much higher beam power. After years of reducing the laser power requirements, LACE is ready for operational tests.\nThe beam dyanmics are very complex in RHIC and the planned EIC.\nI need to study Chao’s book on collective effects."
  },
  {
    "objectID": "posts/2024-03-23_hb/index.html#beam-dynamics-in-linacs",
    "href": "posts/2024-03-23_hb/index.html#beam-dynamics-in-linacs",
    "title": "High Brightness Workshop 2023",
    "section": "Beam dynamics in linacs",
    "text": "Beam dynamics in linacs\n\nThere was debate about 90-degree phase advance limits. Two different talks suggested that linacs could operate above this limit. For example, adding angular momentum to the beam appears to enhance stability.\nDong-O Jeon et al. tried to clarify the relationship between “parametric instabilities” and “particle resonances.” The field seems so small that people will continue to invent their own language. Perhaps we should adopt the language in Ingo Hofmann’s book “Space Charge Physics for Particle Accelerators.”\nChen Xiao and Lars Groening found a way to compute matched/periodic envelopes with space charge in linear coupled focusing systems. I studied this problem for a beam with zero four-dimensional emittance, but Chen handled a more general case. It was puzzling that he did not find unique solutions while I did. I’m excited to continue studying coupled beam dynamics with space charge.\nI presented our work on the impact of high-dimensional phase space correlations on linac beam dynamics."
  },
  {
    "objectID": "posts/2024-03-23_hb/index.html#codes",
    "href": "posts/2024-03-23_hb/index.html#codes",
    "title": "High Brightness Workshop 2023",
    "section": "Codes",
    "text": "Codes\n\nCERN has developed Xsuite, an integrated beam dynamics simulation code based on a collection of CERN codes like MAD and SixTrackLib. The code can handle almost all effects in rings. It is open-source, well-documented, and GPU-accelerated.\nLBNL has developed ImpactX in addition to their WarpX code. ImpactX is similar to PyORBIT, an s-based code used primarily for linacs. The code is well-documented and GPU-accelerated. I liked their figure illustrating the ultimate goal of simulating the entire accelerator chain. Such simulations will require modeling vastly different spatial and temporal scales by connecting the results of different simulation codes.\nI’m currently focused on PyORBIT development because it contains many SNS-specific features and will eventually connect to the SNS control room as an online model. Still, I’m excited about these open-source beam dynamics codes."
  },
  {
    "objectID": "posts/2024-03-23_hb/index.html#other-things",
    "href": "posts/2024-03-23_hb/index.html#other-things",
    "title": "High Brightness Workshop 2023",
    "section": "Other things",
    "text": "Other things\n\nI enjoyed the conference, but the packed schedule left little time to explore Geneva. I would rather have fewer talks and more poster sessions.\nSome speakers covered their slides in tiny text and figures. These speakers rarely addressed anything on the slides. This is a problem at all conferences, but it was especially bad here.\nI appreciated the chance to interact with the small but highly international accelerator community."
  },
  {
    "objectID": "posts/2021-01-25_coupled-parametric-oscillators/index.html",
    "href": "posts/2021-01-25_coupled-parametric-oscillators/index.html",
    "title": "Coupled parametric oscillators",
    "section": "",
    "text": "A previous post examined the analytic solutions to the equation of motion describing a parametric oscillator — an oscillator whose physical properties are time-dependent. This problem was motivated by describing the transverse oscillations of a charged particle in an accelerator. In this post, the treatment will be extended to a coupled parametric oscillator. The oscillator obeys the following equation of motion:\n\\[\n\\begin{aligned}\n    x'' + k_{11}(s)x + k_{13}(s)y + k_{14}(s)y' &= 0, \\\\\n    y'' + k_{33}(s)y + k_{31}(s)x + k_{32}(s)x' &= 0,\n\\end{aligned}\n\\tag{1}\\]\nwhere the prime denotes differentiation with respect to \\(s\\). We will also assume that \\(k_{ij}(s + L) = k_{ij}(s)\\) for some \\(L\\).\nThe first possible source of coupling is the longitudinal magnetic field produced within a solenoid magnet.\n\n\n\n\nFig. 1. A solenoid magnetic field generated by a loop of current. (Source: brilliant.org)\n\n\n\nIf we assume that the solenoid is very long, the field within the coils points in the longitudinal direction and is approximately constant (\\(\\mathbf{B}_{sol} = B_0\\hat{s}\\)). Plugging this into the Lorentz force equation, we find:\n\\[\n\\dot{\\mathbf{v}}\n= \\frac{q}{m} \\mathbf{v} \\times \\mathbf{B}\n= \\frac{qB_0}{m}\\left({v_y\\hat{x} - v_x\\hat{y}}\\right).\n\\tag{2}\\]\nThe motion in \\(x\\) depends on the velocity in \\(y\\), and vice versa. Coupling can also be produced by transverse magnetic fields. Recall the multipole expansion of the transverse magnetic field \\(\\mathbf{B} = (B_x, B_y)\\):\n\n\n\nFig. 2. Multipole expansion of the magnetic field up to fourth order.\n\n\nThere will be nonlinear coupling terms (terms proportional to \\(x^j y^k\\), where \\(j,k &gt; 1\\)) when \\(n &gt; 2\\), but we are interested only in linear coupling. This occurs when the skew quadrupole term \\(a_2\\) is nonzero, which is true when a quadrupole is tilted in the transverse plane. The field couples the motion in one plane to the displacement in the other."
  },
  {
    "objectID": "posts/2021-01-25_coupled-parametric-oscillators/index.html#equations-of-motion",
    "href": "posts/2021-01-25_coupled-parametric-oscillators/index.html#equations-of-motion",
    "title": "Coupled parametric oscillators",
    "section": "",
    "text": "A previous post examined the analytic solutions to the equation of motion describing a parametric oscillator — an oscillator whose physical properties are time-dependent. This problem was motivated by describing the transverse oscillations of a charged particle in an accelerator. In this post, the treatment will be extended to a coupled parametric oscillator. The oscillator obeys the following equation of motion:\n\\[\n\\begin{aligned}\n    x'' + k_{11}(s)x + k_{13}(s)y + k_{14}(s)y' &= 0, \\\\\n    y'' + k_{33}(s)y + k_{31}(s)x + k_{32}(s)x' &= 0,\n\\end{aligned}\n\\tag{1}\\]\nwhere the prime denotes differentiation with respect to \\(s\\). We will also assume that \\(k_{ij}(s + L) = k_{ij}(s)\\) for some \\(L\\).\nThe first possible source of coupling is the longitudinal magnetic field produced within a solenoid magnet.\n\n\n\n\nFig. 1. A solenoid magnetic field generated by a loop of current. (Source: brilliant.org)\n\n\n\nIf we assume that the solenoid is very long, the field within the coils points in the longitudinal direction and is approximately constant (\\(\\mathbf{B}_{sol} = B_0\\hat{s}\\)). Plugging this into the Lorentz force equation, we find:\n\\[\n\\dot{\\mathbf{v}}\n= \\frac{q}{m} \\mathbf{v} \\times \\mathbf{B}\n= \\frac{qB_0}{m}\\left({v_y\\hat{x} - v_x\\hat{y}}\\right).\n\\tag{2}\\]\nThe motion in \\(x\\) depends on the velocity in \\(y\\), and vice versa. Coupling can also be produced by transverse magnetic fields. Recall the multipole expansion of the transverse magnetic field \\(\\mathbf{B} = (B_x, B_y)\\):\n\n\n\nFig. 2. Multipole expansion of the magnetic field up to fourth order.\n\n\nThere will be nonlinear coupling terms (terms proportional to \\(x^j y^k\\), where \\(j,k &gt; 1\\)) when \\(n &gt; 2\\), but we are interested only in linear coupling. This occurs when the skew quadrupole term \\(a_2\\) is nonzero, which is true when a quadrupole is tilted in the transverse plane. The field couples the motion in one plane to the displacement in the other."
  },
  {
    "objectID": "posts/2021-01-25_coupled-parametric-oscillators/index.html#solution",
    "href": "posts/2021-01-25_coupled-parametric-oscillators/index.html#solution",
    "title": "Coupled parametric oscillators",
    "section": "Solution",
    "text": "Solution\nLet’s review the approach we took in analyzing the 1D parametric oscillator. We wrote the solution in pseudo-harmonic form with an amplitude \\(\\sqrt{2 J \\beta(s)}\\) and phase \\(\\mu(s)\\). We then focused on the motion of a particle in \\(x\\)-\\(x'\\) phase space after each focusing period. We observed that the particle jumps around an ellipse: the area of the ellipse is constant and proportional to \\(2 J\\); the dimensions of the ellipse are determined by \\(\\beta\\) and \\(\\alpha = -\\beta' / 2\\); the size of the jumps around the ellipse are determined by \\(\\mu\\). We then wrote the symplectic \\(2 \\times 2\\) transfer matrix \\(\\mathbf{M}\\), which connects the initial and final phase space coordinates through one period, as\n\\[\n\\mathbf{M} = \\mathbf{V}\\mathbf{P}\\mathbf{V}^{-1}.\n\\tag{3}\\]\n\\(\\mathbf{V}^{-1}\\), which is a function of \\(\\alpha\\) and \\(\\beta\\), is a symplectic transformation that deforms the ellipse into a circle while preserving its area, and \\(\\mathbf{P}\\) is a rotation in phase space by the phase advance \\(\\mu\\).\nThis is an elegant way to describe the motion with a minimal set of parameters. The question is: can we do something similar for coupled motion, in which the phase space is four-dimensional, not two-dimensional? To start, let’s track a particle in a lattice with a nonzero skew quadrupole coefficient and plot its phase space coordinates after each period. (All code is in the following collapsed cell.)\n\n\nImports\nimport numpy as np\nimport matplotlib\nfrom matplotlib import animation\nfrom matplotlib import pyplot as plt\nimport scipy\nimport proplot as pplt\nimport psdist.visualization as psv\n\n\n\n\nPlotting\nlabels = [\"x\", \"x'\", \"y\", \"y'\"]\n\n\ndef vector(v, ax=None, origin=(0.0, 0.0), color='black', lw=None, style='-&gt;', head_width=0.4, head_length=0.8):\n    props = dict()\n    props['arrowstyle'] = '{},head_width={},head_length={}'.format(style, head_width, head_length)\n    props['shrinkA'] = props['shrinkB'] = 0\n    props['fc'] = props['ec'] = color\n    props['lw'] = lw\n    ax.annotate('', xy=(origin[0] + v[0], origin[1] + v[1]), xytext=origin, arrowprops=props)\n    return ax\n\n\ndef animate_corner(X, vecs=None, limits=None, vec_kws=None, **kws):\n    if limits is None:\n        maxs = 1.4 * np.max(X, axis=0)\n        maxs[[0, 2]] = np.max(maxs[[0, 2]])\n        maxs[[1, 3]] = np.max(maxs[[1, 3]])\n        limits = [(-m, m) for m in maxs]\n    if vec_kws is None:\n        vec_kws = dict()\n    vec_kws.setdefault('head_width', 0.2)\n    vec_kws.setdefault('head_length', 0.4)\n    kws.setdefault('marker', '.')\n    kws.setdefault('mec', 'None')\n    kws.setdefault('lw', 0.0)\n    kws.setdefault('color', 'black')\n    kws.setdefault('ms', 5.0)\n    \n    grid = psv.CornerGrid(figwidth=4.5, diag=False, limits=limits, labels=labels)\n    \n    lines = [[], [], []]\n    old_lines = [[], [], []]\n    for i in range(3):\n        for j in range(i + 1):\n            ax = grid.axs[i, j]\n            old_line, = ax.plot([], [], alpha=0.25, **kws)\n            old_lines[i].append(old_line)\n            line, = ax.plot([], [], **kws)\n            lines[i].append(line)\n    plt.close()\n\n    def update(frame):\n        for ax in grid.axs:\n            for annotation in ax.texts:\n                annotation.set_visible(False)\n        _X = X[:frame, :]\n        for i in range(3):\n            for j in range(i + 1):\n                ax = grid.axs[i, j]\n                old_lines[i][j].set_data(X[:frame, j], X[:frame, i + 1])\n                lines[i][j].set_data(X[frame, j], X[frame, i + 1])\n                if vecs is not None:\n                    v1 = vecs[0][frame]\n                    v2 = vecs[1][frame]\n                    ax = vector(v1[[j, i+1]], ax=ax, origin=(0, 0), color='blue6', **vec_kws)\n                    ax = vector(v2[[j, i+1]], ax=ax, origin=(v1[[j, i+1]]), color='red6', **vec_kws)     \n        grid.axs[0, 1].annotate(\n            'Period {}'.format(frame), xy=(0.5, 0.5),\n            xycoords='axes fraction', horizontalalignment='center',\n        )\n        \n    return animation.FuncAnimation(grid.fig, update, frames=X.shape[0])\n\n\n\n\nTracking\ndef unit_symplectic_matrix(n=2):\n    \"\"\"Construct 2n x 2n unit symplectic matrix.\n    Each 2 x 2 block is [[0, 1], [-1, 0]]. This assumes our phase space vector\n    is ordered as [x, x', y, y', ...].\n    \"\"\"\n    if n % 2 != 0:\n        raise ValueError(\"n must be an even integer\")\n    U = np.zeros((n, n))\n    for i in range(0, n, 2):\n        U[i : i + 2, i : i + 2] = [[0.0, 1.0], [-1.0, 0.0]]\n    return U\n\n\ndef normalize(eigvecs):\n    \"\"\"Normalize transfer matrix eigenvectors.\"\"\"\n    v1, _, v2, _ = eigvecs.T\n    U = unit_symplectic_matrix(4)\n    for i in (0, 2):\n        v = eigvecs[:, i]\n        val = np.linalg.multi_dot([np.conj(v), U, v]).imag\n        if val &gt; 0:\n            eigvecs[:, i], eigvecs[:, i + 1] = eigvecs[:, i + 1], eigvecs[:, i]\n        eigvecs[:, i : i + 2] *= np.sqrt(2.0 / np.abs(val))\n    return eigvecs\n\n\ndef construct_normalization_matrix(eigvecs):\n    \"\"\"Construct normalization matrix from transfer matrix eigenvectors.\"\"\"\n    v1, _, v2, _ = normalize(eigvecs).T\n    V = np.zeros((4, 4))\n    V[:, 0] = v1.real\n    V[:, 1] = (1.0j * v1).real\n    V[:, 2] = v2.real\n    V[:, 3] = (1.0j * v2).real\n    return V\n\n\ndef rotate_matrix(M, angle=0.0):\n    \"\"\"Get matrix in x-y rotated frame.\"\"\"\n    c = np.cos(angle)\n    s = np.sin(angle)\n    R = np.array([[c, 0, s, 0], [0, c, 0, s], [-s, 0, c, 0], [0, -s, 0, c]])\n    return np.linalg.multi_dot([np.linalg.inv(R), M, R])\n\n\nclass MatrixLattice:\n    \"\"\"Transfer matrix representation of periodic linear system.\"\"\"\n    def __init__(self):\n        self.matrices = []  # [element1, element2, ...]\n        self.M = None  # transfer matrix\n        self.V = None  # normalization matrix\n        self.eigvals = None  # eigenvalues of M\n        self.eigvecs = None  # eigenvalues of M\n        self.v1 = None  # eigenvector 1 \n        self.v2 = None  # eigenvector 2\n        self.eig1 = None  # eigenvalue 1\n        self.eig2 = None  # eigenvalue 2\n        self.params = dict()  # lattice parameters\n        \n    def n_elements(self):\n        \"\"\"Return the number of elements in the lattice.\"\"\"\n        return len(self.matrices)\n    \n    def add(self, mat):\n        \"\"\"Add an element to the end of the lattice.\"\"\"\n        self.matrices.append(mat)\n        self.build()\n        \n    def rotate(self, phi):\n        \"\"\"Apply transverse rotation to all elements.\"\"\"\n        self.M = rotate_matrix(self.M, np.radians(phi))\n        self.analyze()\n        \n    def build(self):\n        \"\"\"Compute the one-turn transfer matrix.\"\"\"\n        n_elements = self.n_elements()\n        if n_elements == 0:\n            return\n        elif n_elements == 1:\n            self.M = self.matrices[0]\n        else:\n            self.M = np.linalg.multi_dot(list(reversed(self.matrices)))\n            \n    def analyze(self):\n        \"\"\"Compute the lattice parameters.\"\"\"\n        self.eigvals, self.eigvecs_raw = np.linalg.eig(self.M)\n        self.eig1, self.eig2 = self.eigvals[[0, 2]]\n        self.v1_raw, self.v2_raw = self.eigvecs_raw[:, [0, 2]].T\n        self.V = construct_normalization_matrix(self.eigvecs_raw.copy())\n        self.eigvecs = normalize(self.eigvecs_raw)\n        self.v1, self.v2 = self.eigvecs[:, [0, 2]].T\n        self.Vinv = np.linalg.inv(self.V)\n        self.analyze_2D()\n        self.analyze_4D()\n        \n    def analyze_2D(self):\n        \"\"\"Compute the 2D Twiss parameters.\"\"\"\n        def analyze_2x2(M):\n            cos_phi = (M[0, 0] + M[1, 1]) / 2.0\n            sign = 1.0\n            if abs(M[0, 1]) != 0:\n                sign = M[0, 1] / abs(M[0, 1])\n            sin_phi = sign * np.sqrt(1.0 - cos_phi**2)\n            beta = M[0, 1] / sin_phi\n            alpha = (M[0, 0] - M[1, 1]) / (2.0 * sin_phi)\n            tune = np.arccos(cos_phi) / (2.0 * np.pi) * sign\n            return alpha, beta, tune\n        \n        alpha_x, beta_x, tune_x = analyze_2x2(self.M[:2, :2])\n        alpha_y, beta_y, tune_y = analyze_2x2(self.M[2:, 2:])\n        self.params['alpha_x'] = alpha_x\n        self.params['alpha_y'] = alpha_y\n        self.params['beta_x'] = beta_x\n        self.params['beta_y'] = beta_y\n        self.params['tune_x'] = tune_x\n        self.params['tune_y'] = tune_y\n        \n    def analyze_4D(self):\n        \"\"\"Compute the 4D Twiss parameters.\"\"\"\n        V = self.V\n        beta_1x = V[0, 0] ** 2\n        beta_2y = V[2, 2] ** 2\n        alpha_1x = -np.sqrt(beta_1x) * V[1, 0]\n        alpha_2y = -np.sqrt(beta_2y) * V[3, 2]\n        u = 1.0 - V[0, 0] * V[1, 1]\n        nu1 = np.arctan2(-V[2, 1], V[2, 0])\n        nu2 = np.arctan2(-V[0, 3], V[0, 2])\n        beta_1y = (V[2, 0] / np.cos(nu1)) ** 2\n        beta_2x = (V[0, 2] / np.cos(nu2)) ** 2\n        alpha_1y = (u * np.sin(nu1) - V[3, 0] * np.sqrt(beta_1y)) / np.cos(nu1)\n        alpha_2x = (u * np.sin(nu2) - V[1, 2] * np.sqrt(beta_2x)) / np.cos(nu2)\n        self.params['alpha_1x'] = alpha_1x\n        self.params['alpha_1y'] = alpha_1y\n        self.params['alpha_2x'] = alpha_2x\n        self.params['alpha_2y'] = alpha_2y\n        self.params['beta_1x'] = beta_1x\n        self.params['beta_1y'] = beta_1y\n        self.params['beta_2x'] = beta_2x\n        self.params['beta_2y'] = beta_2y\n        self.params['u'] = u\n        self.params['nu1'] = nu1\n        self.params['nu2'] = nu2\n        self.params['tune_1'] = np.arccos(self.eig1.real) / (2.0 * np.pi)\n        self.params['tune_2'] = np.arccos(self.eig2.real) / (2.0 * np.pi)\n        \n    def normal_form(self):\n        \"\"\"Return normal form of lattice transfer matrix.\"\"\"\n        return np.linalg.multi_dot([np.linalg.inv(self.V), self.M, self.V])\n        \n    def track_part(self, x, nturns=1, norm_coords=False, element_wise=False):\n        \"\"\"Track a single particle.\"\"\"\n        if norm_coords:\n            x = np.matmul(np.linalg.inv(self.V), x)\n            M_oneturn = self.normal_form()\n        else:\n            M_oneturn = self.M\n        X = [x]\n        for _ in range(nturns):\n            if element_wise:\n                for M in self.matrices:\n                    X.append(np.matmul(M, X[-1]))\n            else:\n                X.append(np.matmul(M_oneturn, X[-1]))\n        return np.array(X)\n    \n\ndef fodo(k1, k2, length, fill_fac=0.5, quad_tilt=0, start='quad', n_parts=1):\n    \"\"\"Create a FODO lattice (focus, off, defocus, off).\"\"\"\n    \n    def matrix_drift(length):\n        \"\"\"Drift transfer matrix.\"\"\"\n        M = np.zeros((4, 4))\n        M[:2, :2] = M[2:, 2:] = [[1, length], [0, 1]]\n        return M\n\n    def matrix_quad(length, k, kind='qf', tilt=0):\n        \"\"\"Focusing quadrupole transfer matrix.\"\"\"\n        k = np.sqrt(np.abs(k))\n        cos = np.cos(k * length)\n        sin = np.sin(k * length)\n        cosh = np.cosh(k * length)\n        sinh = np.sinh(k * length)\n        if kind == 'qf':\n            M = np.array([[cos, sin / k, 0, 0],\n                          [-k*sin, cos, 0, 0],\n                          [0, 0, cosh, sinh / k],\n                          [0, 0, k * sinh, cosh]])\n        elif kind == 'qd':\n            M = np.array([[cosh, sinh / k, 0, 0],\n                          [k * sinh, cosh, 0, 0],\n                          [0, 0, cos, sin / k],\n                          [0, 0, -k * sin, cos]])\n        if tilt:\n            M = rotate_matrix(M, np.radians(tilt))\n        return M\n\n    length_quad = 0.5 * fill_fac * length\n    length_quad = length_quad / n_parts\n    length_drift = 0.5 * (1.0 - fill_fac) * length\n    length_drift = length_drift / n_parts\n    lattice = MatrixLattice()\n    if start == 'quad':\n        for _ in range(n_parts):\n            lattice.add(matrix_quad(0.5 * length_quad, k1, 'qf', quad_tilt))\n        for _ in range(n_parts):\n            lattice.add(matrix_drift(length_drift))\n        for _ in range(n_parts):\n            lattice.add(matrix_quad(length_quad, k2, 'qd', -quad_tilt))\n        for _ in range(n_parts):\n            lattice.add(matrix_drift(length_drift))\n        for _ in range(n_parts):\n            lattice.add(matrix_quad(0.5 * length_quad, k1, 'qf', quad_tilt))\n    elif start == 'drift':\n        for _ in range(n_parts):\n            lattice.add(matrix_drift(0.5 * length_drift))\n        for _ in range(n_parts):\n            lattice.add(matrix_quad(length_quad, k1, 'qf', +quad_tilt))\n        for _ in range(n_parts):\n            lattice.add(matrix_drift(length_drift))\n        for _ in range(n_parts):\n            lattice.add(matrix_quad(length_quad, k2, 'qd', -quad_tilt))\n        for _ in range(n_parts):\n            lattice.add(matrix_drift(0.5 * length_drift))\n    lattice.analyze()\n    return lattice\n\n\n# Generate a FODO lattice. The two quadrupoles are tilted in opposite directions\n# by one degree to generate linear x-y coupling.\nL = 5.0\nk1 = 0.25\nk2 = 0.25\nlattice = fodo(k1, k2, L, fill_fac=0.5, quad_tilt=-1.0, start='drift')\n\n# Set the initial particle phase space coordinates.\nJ1 = 0.5 * 40.0  # amplitude of mode 1\nJ2 = 0.5 * 10.0  # amplitude of mode 2\npsi1 = np.radians(0.0)  # initial phase of eigenvector 1\npsi2 = np.radians(90.0)  # initial phase of eigenvector 2\nx1 = np.real(np.sqrt(2.0 * J1) * lattice.v1 * np.exp(1.0j * psi1)) # mode 1 contribution\nx2 = np.real(np.sqrt(2.0 * J2) * lattice.v2 * np.exp(1.0j * psi2)) # mode 2 contribution\nx = x1 + x2\n\n# Track for 1000 turns.\nn_turns = 1000\nX1 = lattice.track_part(x1, n_turns)\nX2 = lattice.track_part(x2, n_turns)\nX = lattice.track_part(x, n_turns)\n\n# Animate 45 turns in corner plot.\nscale = 1.4\nmaxs = 1.4 * np.max(X, axis=0)\nmaxs[[0, 2]] = np.max(maxs[[0, 2]])\nmaxs[[1, 3]] = np.max(maxs[[1, 3]])\nlimits = [(-m, m) for m in maxs]\nanim = animate_corner(X[:45, :], limits=limits)\n\n\n\n\n\nFig. 3. Period-by-period four-dimensional phase space coordinates of a coupled parametric oscillator.\n\n\nThe particle traces donut-like shapes in \\(x\\)-\\(x'\\) and \\(y\\)-\\(y'\\) instead of ellipses. Here are the shapes after 1000 periods.\n\n\nCode\ngrid = psv.CornerGrid(diag=False, figwidth=4.5, labels=labels, limits=limits)\ngrid.plot_cloud(X, kind='scatter', s=0.75)\ngrid.axs[0, 1].annotate(\n    'Period 1000', xy=(0.5, 0.5), xycoords='axes fraction',\n    horizontalalignment='center', verticalalignment='center'\n)\nplt.close()\n\n\n\n\n\nFig. 4. Phase space coordates after 1000 periods.\n\n\nThere is clearly more than one frequency present.\n\n\nCode\nfig, axs = pplt.subplots(ncols=2, figsize=(8.0, 1.5), sharey=False, spanx=False, space=6)\naxs[0].plot(X[:150, 0], color='black', marker='.', ms=4)\naxs[0].format(ylabel='x [mm]')\n\nn = X.shape[0]\nm = n // 2\nf = (1.0 / n) * np.arange(m)\nxf = (1.0 / m) * abs(scipy.fft.fft(X[:, 0])[:m])\naxs.format(title_kw=dict(fontsize='medium'))\naxs[0].format(xlabel='Period number')\naxs[1].format(xlabel='Frequency')\naxs[1].plot(f[1:], xf[1:], color='black')\naxs[:, 1].format(ylabel='FFT amplitude')\n\n\nThis is typical of a coupled oscillator. The motion in coupled systems can be understood as the superposition of normal modes, each of which corresponds to a single frequency. For example, consider two masses connected with a spring. There are two possible ways for the masses to oscillate at the same frequency. The first is a breathing mode in which they move in opposite directions, and the second is a sloshing mode in which they move in the same direction. The motion is the sum of these two modes. We will try to do something similar for a coupled parameteric oscillator.\n\nTransfer matrix eigenvectors\nIf the phase space coordinate vector \\(\\mathbf{x} = (x, x', y, y')^T\\) evolves according to\n\\[\n\\mathbf{x} \\rightarrow \\mathbf{Mx},\n\\tag{4}\\]\nwhere \\(\\rightarrow\\) represents tracking through one period, it can be shown that \\(\\mathbf{M}\\) is symplectic due to the Hamiltonian mechanics of the system. Consider the eigenvectors of \\(\\mathbf{M}\\):\n\\[\n\\mathbf{Mv} = e^{-i\\mu}\\mathbf{v}.\n\\tag{5}\\]\nThe symplecticity condition causes the eigenvalues and eigenvectors come in two complex conjugate pairs; this gives \\(\\mathbf{v}_1\\), \\(\\mathbf{v}_2\\), \\(\\mu_1\\), \\(\\mu_2\\) and their complex conjugates. The seemingly complex motion is simplified when written in terms of the eigenvectors. We can write any coordinate vector as a linear combination of the real and imaginary components of \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\):\n\\[\n\\mathbf{x} = \\text{Re} \\left\\{\n    \\sqrt{2 J_1}\\mathbf{v}_1e^{-i\\psi_1} +\n    \\sqrt{2 J_2}\\mathbf{v}_2e^{-i\\psi_2}\n\\right\\}.\n\\tag{6}\\]\n\\(\\text{Re}\\{\\dots\\}\\) selects the non-imaginary component of \\(\\{\\dots\\}\\). We have introduced two invariant amplitudes (\\(J_1\\) and \\(J_2\\)) as well as two initial phases (\\(\\psi_1\\) and \\(\\psi_2\\)). Applying the transfer matrix tacks on a phase to each eigenvector. Thus, what we are observing are the 2D projections of the real components of these eigenvectors as they rotate in the complex plane.\n\\[\n\\mathbf{Mx} = \\text{Re} \\left\\{\n    \\sqrt{2 J_1}\\mathbf{v}_1e^{-i\\left(\\psi_1 + \\mu_1\\right)} +\n    \\sqrt{2 J_2}\\mathbf{v}_2e^{-i(\\psi_2 + \\mu_2)}\n\\right\\}.\n\\tag{7}\\]\nLet’s replay the animation, but this time draw a blue arrow for \\(\\mathbf{v}_1\\) and a red arrow for \\(\\mathbf{v}_2\\). We’ve chosen \\(J_1 = 4 J_2\\) and \\(\\psi_2 - \\psi_1 = \\pi/2\\).\n\n\nCode\nanim = animate_corner(X[:45, :], vecs=[X1, X2], limits=limits)\n\n\n\n\n\nFig. 6. Period-by-period coordinates with eigenvector projections as blue/red arrows.\n\n\nMuch simpler. Each eigenvector simply rotates at its frequency \\(\\mu_l\\). It also explains why the amplitude in the \\(x\\)-\\(x'\\) and \\(y\\)-\\(y'\\) planes trade back and forth: it is because the projections of the eigenvectors rotate at different frequencies, sometimes aligning and sometimes anti-aligning. Because of this, the previous invariants \\(J_{x,y}\\) are replaced by \\(J_{1,2}\\). In the four-dimensional phase space, each eigenvector traces an ellipsoid, and the particle moves on a torus (represented below). The amplitudes determine the inner and outer radii, and the two phases determine the location on the surface.\n\n\n\nFig. 7. Representation of the invariant torus in four-dimensional phase space.\n\n\n\n\nEigenvector parameterization\nWe are now going to introduce a set of parameters for these eigenvectors, and in turn the transfer matrix. We already have two phases, so that leaves 8 parameters. Our strategy is to observe that each eigenvector traces an ellipse in both horizontal (\\(x\\)-\\(x'\\)) and vertical (\\(y\\)-\\(y'\\)) phase space. Then, we will simply assign an \\(\\alpha\\) function and \\(\\beta\\) function to each of these ellipses. So, for the ellipse traced by \\(\\mathbf{v}_1\\) in the \\(x\\)-\\(x'\\) plane, we have \\(\\beta_{1x}\\) and \\(\\alpha_{1x}\\), and then for the second eigenvector we have \\(\\beta_{2x}\\) and \\(\\alpha_{2x}\\). The same thing goes for the vertical dimension with \\(x\\) replaced by \\(y\\).\n\n\n\nFig. 8. Four-dimensional Twiss parameters. Each eigenvector traces an ellipse in each two-dimensional plane (\\(x\\)=\\(x'\\), \\(y\\)-\\(y'\\)); each ellipse is assigned an \\(\\alpha\\) and \\(\\beta\\) parameter.\n\n\nThe actual eigenvectors written in terms of the parameters are\n\\[\n\\vec{v}_1 = \\begin{bmatrix}\n    \\sqrt{\\beta_{1x}} \\\\\\\\\n    -\\frac{\\alpha_{1x} + i(1-u)}{\\sqrt{\\beta_{1x}}} \\\\\\\\\n    \\sqrt{\\beta_{1y}}e^{i\\nu_1} \\\\\\\\\n    -\\frac{\\alpha_{1y} + iu}{\\sqrt{\\beta_{1y}}} e^{i\\nu_1}\n\\end{bmatrix},\n\\quad\n\\vec{v}_2 = \\begin{bmatrix}\n    \\sqrt{\\beta_{2x}}e^{i\\nu_2} \\\\\\\\\n    -\\frac{\\alpha_{2x} + iu}{\\sqrt{\\beta_{2x}}}e^{i\\nu_2} \\\\\\\\\n    \\sqrt{\\beta_{2y}} \\\\\\\\\n    -\\frac{\\alpha_{2y} + i(1-u)}{\\sqrt{\\beta_{2y}}}\n\\end{bmatrix}\n\\tag{8}\\]\nSo in addition to the phases \\(\\mu_1\\) and \\(\\mu_2\\) we have \\(\\alpha_{1x}\\), \\(\\alpha_{2x}\\), \\(\\alpha_{1y}\\), \\(\\alpha_{2y}\\), \\(\\beta_{1x}\\), \\(\\beta_{2x}\\), \\(\\beta_{1y}\\), and \\(\\beta_{2y}\\). That’s pretty much it. There are a few other parameters we need to introduce to simplify the notation, but they are not independent. The first is \\(u\\), which, as noted in the figure, determines the areas of the ellipses in one plane relative to the other. The second and third are \\(\\nu_1\\) and \\(\\nu_2\\), which are phase differences between the \\(x\\) and \\(y\\) components of the eigenvectors (in the animation they are either \\(0\\) or \\(\\pi\\)). I won’t discuss these here. The last thing to note is that the parameters reduce to their 1D definitions when there is no coupling in the lattice. So we would have \\(\\beta_{1x}, \\beta_{2y} \\rightarrow \\beta_{x}, \\beta_{y}\\) and \\(\\beta_{2x}, \\beta_{1y} \\rightarrow 0\\), and similar for \\(\\alpha\\). The invariants and phase advances would also revert back to their original values: \\(J_{1,2} \\rightarrow J{x,y}\\) and \\(\\mu_{1,2} \\rightarrow \\mu_{x,y}\\).\n\n\nFloquet transformation\nThese eigenvectors can also be used to construct a transformation which removes both the variance in the focusing strength and the coupling between the planes, turning the coupled parametric oscillator into an uncoupled harmonic oscillator. In other words, we seek a matrix \\(\\mathbf{V}\\) such that\n\\[\\mathbf{V^{-1} M V} = \\mathbf{P} =\n\\begin{bmatrix}\n    \\cos{\\mu_1} & \\sin{\\mu_1}  & 0 & 0 \\\\\n    -\\sin{\\mu_1} & \\cos{\\mu_1}  & 0 & 0 \\\\\n    0 & 0 & \\cos{\\mu_2} & \\sin{\\mu_2} \\\\\n    0 & 0 & -\\sin{\\mu_2} & \\cos{\\mu_2}\n\\end{bmatrix}\n\\tag{9}\\]\nWe can do this simply by rewriting Equation 6 as \\(\\mathbf{x} = \\mathbf{V}\\mathbf{x}_n\\) with\n\\[\n\\mathbf{x}_n = \\begin{bmatrix}\n  \\sqrt{2J_1}\\cos{\\psi_1} \\\\\\\\\n  -\\sqrt{2J_1}\\sin{\\psi_1} \\\\\\\\  \n  \\sqrt{2J_2}\\cos{\\psi_2} \\\\\\\\\n  -\\sqrt{2J_2}\\sin{\\psi_2}\n\\end{bmatrix}\n\\tag{10}\\]\n\\[\n\\mathbf{V} = \\left[\n    \\text{Re}\\{\\mathbf{v}_1\\},\n    -\\text{Im}\\{\\mathbf{v}_1\\},\n    \\text{Re}\\{\\mathbf{v}_2\\},\n    -\\text{Im}\\{\\mathbf{v}_2\\}\n\\right]\n\\]\nLet’s observe the motion in these new coordinates \\(\\mathbf{x}_n\\).\n\n\nCode\nx = x1 + x2\nX1 = lattice.track_part(x1, n_turns, norm_coords=True)\nX2 = lattice.track_part(x2, n_turns, norm_coords=True)\nX1[:, 2:] = 0\nX2[:, :2] = 0\nX = lattice.track_part(x, n_turns, norm_coords=True)\nanim = animate_corner(X[:45, :], vecs=[X1, X2])\n\n\n\n\n\nFig. 8. Period-by-period motion in normalized/Floquet coordinates\n\n\nThe motion is uncoupled after this transformation; i.e., particles move in a circle of area \\(2J_1\\) in the \\(x_n\\)-\\(x_n'\\) plane at frequency \\(\\mu_1\\), and in a circle of area \\(2J_2\\) in the \\(y_n\\)-\\(y_n'\\) plane at frequency \\(\\mu_2\\)."
  },
  {
    "objectID": "posts/2021-01-25_coupled-parametric-oscillators/index.html#conclusion",
    "href": "posts/2021-01-25_coupled-parametric-oscillators/index.html#conclusion",
    "title": "Coupled parametric oscillators",
    "section": "Conclusion",
    "text": "Conclusion\nThe method introduced here describes a coupled parametric oscillator using the minimum number of parameters. Our physical motivation was an accelerator lattice with linear, coupled forces. There is no agreed upon method to do this among accelerator physicists, but this is the method I have used in my research. I’ve left out details which can be found in  [Lebedev2010?,Willeke1989?].\n\n\n\nFig. 1. A solenoid magnetic field generated by a loop of current. (Source: brilliant.org)\nFig. 2. Multipole expansion of the magnetic field up to fourth order.\nFig. 3. Period-by-period four-dimensional phase space coordinates of a coupled parametric oscillator.\nFig. 4. Phase space coordates after 1000 periods.\nFig. 6. Period-by-period coordinates with eigenvector projections as blue/red arrows.\nFig. 7. Representation of the invariant torus in four-dimensional phase space.\nFig. 8. Four-dimensional Twiss parameters. Each eigenvector traces an ellipse in each two-dimensional plane (\\(x\\)=\\(x'\\), \\(y\\)-\\(y'\\)); each ellipse is assigned an \\(\\alpha\\) and \\(\\beta\\) parameter.\nFig. 8. Period-by-period motion in normalized/Floquet coordinates"
  },
  {
    "objectID": "posts/2024-05-16_why-maximize-entropy/index.html",
    "href": "posts/2024-05-16_why-maximize-entropy/index.html",
    "title": "Why maximize entropy?",
    "section": "",
    "text": "I’ve been thinking a lot about entropy in the past year. I’ve been motivated by problems in phase space tomography, where the goal is to reconstruct a two-, four- or six-dimensional phase space distribution from its one- or two-dimensional projections. Such problems have no unique solution. We can imagine the data carving out a feasible set in the space of distributions, where all distributions in the feasible set reproduce the data.\nWe could adopt various strategies to select a single distribution from the feasible set. Some strategies generate low-quality reconstructions when there are few projections. For example, Filtered Backprojection (FBP) typically generates streaking artifacts. Minerbo’s MENT algorithm  [1] does better. In addition to reproducing the measured projections, MENT maximizes the distribution’s entropy.\nWhy maximize entropy? In Minerbo’s original paper, he states:\n\nFrom the standpoint of information theory this approach is conceptually attractive: It yields the image with the lowest information content consistent with the available data. Thus with this approach one avoids introducing extraneous information or artificial structure. The problem of reconstructing a source from a finite number of views is known to be indeterminate. A maximum entropy method thus seems attractive for this problem, especially when the available projection data are incomplete or degraded by noise errors.  [1]\n\nClear enough. Mottershead says similar things but mentions multiplicity in the same breath:\n\nIn this case the inversion is not unique, and we need a mechanism for constructing an estimate of the distribution that incorporates everything we know, and nothing else. The maximum entropy principle offers a natural way for doing this. It argues that, of all the possible distributions that satisfy the observed constraints, the most reasonable one to choose is that one that nature can produce in the greatest number of ways, namely, the distribution having maximum entropy.  [2]\n\nAn idea expressed in conversation with others is that we seek the maximum-entropy distribution because charged particle beams are often near equilibrium and, therefore, in a maximum-entropy state. And such a state, from Boltzmann’s derivation, has the highest multiplicity. This distribution would be the most likely if chosen at random. So is the maximum-entropy distribution the most reasonable, the most likely, or both?\nLike most physics students, I was introduced to entropy in statistical mechanics, where we defined entropy as the logarithm of multiplicity. I was also aware of Shannon’s information theory entropy but hadn’t thought much about the relationship between the two. I recently came across work by Steven Gull and John Skilling from the University of Cambridge. They claimed that entropy maximization can be derived without reference to information theory or physics. I was confused at this point.\nI now subscribe to the idea that entropy maximization is the only logically consistent strategy for inferring probability distributions from incomplete data. I recommend the following papers to trace the development of this idea.\n\nKardar, Mehran. “Statistical physics of particles.” Cambridge University Press, 2007.\nJaynes, Edwin T. “Information theory and statistical mechanics.” Physical Review 106.4 (1957): 620.\nRosenkrantz, Roger D., ed. “ET Jaynes: papers on probability, statistics and statistical physics.” Vol. 158. Springer Science & Business Media, 2012.\nShore, John, and Rodney Johnson. “Axiomatic derivation of the principle of maximum entropy and the principle of minimum cross-entropy.” IEEE Transactions on information theory 26.1 (1980): 26-37.\nJaynes, Edwin Thompson. “Monkeys, kangaroos and N.” Maximum-Entropy and Bayesian Methods in Applied Statistics 26 (1986).\nPressé, Steve, et al. “Principles of maximum entropy and maximum caliber in statistical physics.” Reviews of Modern Physics 85.3 (2013): 1115.\nPressé, Steve, et al. “Nonadditive entropies yield probability distributions with biases not warranted by the data.” Physical review letters 111.18 (2013): 180604.\nPressé, Steve, et al. “Reply to C. Tsallis’“Conceptual inadequacy of the Shore and Johnson axioms for wide classes of complex systems”.” Entropy 17.7 (2015): 5043-5046.\nJizba, Petr, and Jan Korbel. “Maximum entropy principle in statistical inference: Case for non-Shannonian entropies.” Physical review letters 122.12 (2019): 120601.\nPachter, Jonathan Asher, Ying-Jen Yang, and Ken A. Dill. “Entropy, irreversibility and inference at the foundations of statistical physics.” Nature Reviews Physics (2024): 1-12.\n\n\n\n\n\nReferences\n\n[1] G. Minerbo, MENT: A Maximum Entropy Algorithm for Reconstructing a Source from Projection Data, Computer Graphics and Image Processing 10, 48 (1979).\n\n\n[2] C. Mottershead, Maximum Entropy Beam Diagnostic Tomography, IEEE Transactions on Nuclear Science 32, 1970 (1985)."
  },
  {
    "objectID": "posts/2024-04-01_muon/index.html",
    "href": "posts/2024-04-01_muon/index.html",
    "title": "Princeton Muon Collider Workshop",
    "section": "",
    "text": "The Large Hadron Collider (LHC) will soon double its luminosity, but progress in high-energy physics (HEP) will eventually require a new laboratory  [1]. There are a few options. The Future Circular Collider (FCC) would collide ~100 TeV protons in a 100 km ring, using the LHC (7 TeV, 25 km) as a booster ring. The plan would be to start colliding leptons (FCC-ee) in the 2040s and switch to protons (FCC-hh) in the 2070s. The accelerator would be a relatively straightforward extension of the LHC, although there would be some new challenges, especially the required 16 T (!) bending magnets.\nAn alternative approach is to collide leptons. Hadron collisions distribute energy among quarks and gluons, and it can be difficult to extract signals from the debris. Leptons don’t seem to have any internal structure; their collisions are clean and efficient, enabling precision measurements of certain reactions. Such collisions would be especially useful for studying the Higgs boson. Unfortunately, synchrotron radiation limits lepton energy in circular accelerators. Linear accelerators eliminate this problem but need to by very long to probe BSM physics. The proposed International Linear Collider (ILC) would collide electrons with positrons at ~1 TeV in a ~50 km linac. The ILC has undergone a comprehensive design study and is ready to launch. Japan has shown interest, and although there are no official plans, the ILC could start running as soon as 2035.\nA muon collider (MuC) would be a hybrid approach. A MuC would collide muons and antimuons at ~10 TeV in a ~10 km ring (approximately the size of the LHC). Since muons are much heavier than electrons, they can be accelerated to 10 TeV in a compact ring. And since they are leptons, their collisions are more useful than hadrons at the same energy. A MuC is likely the only hope for a collider beyond LHC energy within my lifetime; if all goes well, it could start running in 2045. But there’s a problem: muons decay almost instantly. And another problem: muon beams are hot—they occupy a large volume in position-momentum space. And other problems too, such as neutrino radiation (!). These problems require novel solutions. Thus, the muon collider might be both the fastest and most challenging path to the energy frontier.\nThe US Muon Accelerator Program (MAP) developed a preliminary MuC design some years ago, and the International Muon Collider Collaboration (IMCC) has started where MAP left off. The preliminary design is shown below  [2]:\n\nThe front end generates the muon and antimuon beams by colliding a proton beam with a metal target, the middle section cools the beams, reducing their phase space volume by six orders of magnitude (!), and the final section accelerates and collides the beams at dedicated interaction points. There is a push to deliver a collider design in the next few years, reflecting growing support from the HEP community.\nI recently gave a talk at a workshop on muon colliders at Princeton University. The workshop discussed how the US could address the key technological challenges in the next five years. All presentations are available online. I discussed how the SNS could test the extreme proton bunch compression needed to generate the initial muon beam. It was a unique workshop because of the mix of accelerator and particle physicists. While most of the talks focused on accelerator physics, around a third focused on detectors or HEP theory. A major topic was the beam-induced background (BIB) due to muon decay. I didn’t understand these talks well, but they motivated me to resume my particle physics self-education. (I probably would have gone into HEP if I’d attended a different grad school.)\nIt was interesting to hear the particle physicists’ perspectives. Some expressed a desire for more accelerator-particle physics collaboration, especially among graduate students. I’m not sure how a Ph.D. student could have time to learn both accelerator physics and the complicated data analysis required for experimental particle physics. But I like this idea, and I’m sure many projects would benefit from multiple perspectives and skill sets.\nI also liked hearing about the state of HEP. One experimentalist expressed his opinion that particle physicists undersell their work. He drew a comparison to condensed matter physics, where experiments reveal exciting and unpredicted phenomena despite agreeing with the underlying theory of quantum mechanics. He argued that similar findings are commonplace at the LHC — many new processes do not contradict the Standard Model (SM) but are, nonetheless, unexpected and exciting. I suppose material science discoveries will always get more attention because they can lead to new technologies, while particle physics is more out there. In other words, who cares if tetraquarks exist? But from a fundamental scientific viewpoint, these discoveries are equally exciting.\nI was happy to attend this workshop and look forward to exploring SNS contributions to MuC design. It’s exciting to think about projects twenty (or even fifty) years away.\nA few up-to-date talks on future colliders: IMCC, FCC, ILC.\n\n\n\n\nReferences\n\n[1] V. Shiltsev and F. Zimmermann, Modern and Future Colliders, Reviews of Modern Physics 93, 015006 (2021).\n\n\n[2] C. Accettura et al., Towards a Muon Collider, The European Physical Journal C 83, 864 (2023)."
  },
  {
    "objectID": "posts/2024-11-05_four-dimensional-tomography-from-one-dimensional-projections/index.html",
    "href": "posts/2024-11-05_four-dimensional-tomography-from-one-dimensional-projections/index.html",
    "title": "Four-dimensional phase space tomography from one-dimensional measurements of a hadron beam",
    "section": "",
    "text": "An older post explained how I used MENT-Flow to reconstruct the four-dimensional phase space density of a beam in the SNS ring. I’ve since improved the reconstruction. First, I found a bug that was setting the beam energy to 1 GeV instead of the correct value of 0.8 GeV. Second, I switched from MENT-Flow to MENT. After making these changes, I reproduced the measured beam profiles almost perfectly. I’ve written up these results in a paper which is under review at PRAB.\nThe paper isn’t really about the reconstruction algorithm; it’s more about the experimental design and uncertainty quantification needed to trust the results. For instance, it wasn’t clear how many one-dimensional projections we needed to constrain the four-dimensional density.\nWe performed our experiment at the Spallation Neutron Source (SNS) accelerator. The SNS generates high-power proton pulses via multiturn charge-exchange injection from a linac into an accumulator ring. After all \\(10^{14}\\) protons are accumulated, they’re extracted and sent to the spallation target to make neutrons. We measured the beam in the RTBT (Ring Target Beam Transport) section shown in Figure 1.\n\n\n\n\n\n\nFigure 1: (Click to enlarge.)\n\n\n\nThe beam intensity and energy preclude the use of screens to measure the two-dimensional beam density. Instead, the SNS has a set of four wire scanners. Each wire scanner has a horizontal, diagonal, and vertical wire that sweeps across the beam. By recording the secondary electron emission from the wire, we can measure the particle density as a function of position. We end up with three signals per wire scanner: \\(\\{ f(x), f(y), f(u) \\}\\), where \\(u = (x - y) / \\sqrt(2)\\) is the diagonal axis. The wire scanners run in parallel, generating twelve profiles on each scan.\nEach signal is a one-dimensional projection of the four-dimensional phase space distribution \\(f(x, x', y, y')\\), where \\(x'\\) and \\(y'\\) are the momentum coordinates. In an older study, I found that we could use the variance of each signal to nail down the \\(4 \\times 4\\) covariance matrix \\(\\Sigma\\). Reconstructing the full distribution is a natural next step.\nA key finding in that older study was that the nominal optics lead to an ill-conditioned least-squares problem when computing the covariance matrix. I solved this by adding another set of optics. There isn’t much wiggle room in the wire scanner region (due to shared quadrupole power supplies and beam size constraints), but I found another working point that led to a better-conditioned problem. In our experiment, we measured the beam with the nominal optics, modified optics, and one additional set of optics, generating 36 profiles.\nI started by fitting the covariance matrix to the measured signal variances (\\(\\langle xx \\rangle, \\langle yy \\rangle, \\langle uu \\rangle\\)). Figure 2 shows a tight fit to the data. Using standard least-squares error propagation, I estimated a low sensitivity to errors.\n\n\n\n\n\n\nFigure 2\n\n\n\nNext, I used MENT to reconstruct the distribution. MENT updates a prior \\(f_*(x, x', y, y')\\) to a posterior \\(f(x, x', y, y')\\) by maximizing the entropy of \\(f\\) relative to \\(f_*\\). This ensures the posterior is as simple as possible relative to the prior. A uniform prior often makes sense, but we’ve already estimated the covariance matrix from the least-squares fit, and we don’t want to ignore this information. One could imagine starting from a uniform prior and running MENT with the measured covariance matrix as a constraint. The result is a Gaussian distribution. So I used a Gaussian prior with the measured covariance matrix.\nHere are the reconstruction results. First, the measured profiles (black points) compared to the simulated/predicted profiles:\n\n\n\n\n\n\nFigure 3\n\n\n\nPerfect agreement. The log-scale plots show agreement in low-density regions as well:\n\n\n\n\n\n\nFigure 4\n\n\n\nI’m not sure what’s causing the shoulders in the vertical (\\(y\\)) profiles; it might be cross-talk between wires. But this is an encouraging result. It means we might be able to study halo formation in the ring by improving the wire scanner dynamic range.\nHere are the 2D projections of the 4D distribution that generated those profiles:\n\n\n\n\n\n\nFigure 5\n\n\n\nI didn’t say much about this figure in the paper because I was focused on the measurement, not the physics. But note that running the same measurement with a different injection painting scheme yields a much different result (Figure 6). In both cases, the reconstructed distribution is close to what we expected based on simulations.\n\n\n\n\n\n\nFigure 6\n\n\n\nShould we trust these results? There are many possible errors in the accelerator and measurement model, but we think our modeling errors are quite small. The remaining uncertainty is due to the inverse nature of the reconstruction problem. The measurements render some distributions more likely than others, but they do not identify a unique distribution. An ideal reconstruction algorithm would somehow report the spread of distributions compatible with the measurements. No such algorithm exists at the moment.\nThus I followed the strategy of several other papers: simulate the reconstruction. I used four different initial distributions. The first is the result of a beam physics simulation, so it’s somewhat realistic. The second is a superposition of Gaussian blobs. The third is a “Waterbag” distribution, which has a uniform density inside the unit ball. The fourth is a “Hollow” distribution, which is a hollowed-out version of the Waterbag. Here are the results:\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n \n\n\n\nIt looks like most 2D features are captured by the MENT distribution. That’s good news for our experiments: 2D features tell us a lot about the beam. There’s more uncertainty in the cross-plane projections, such as \\(x\\)-\\(y\\) and \\(x'\\)-\\(y'\\), but the uncertainty isn’t enormous.\nA closer look shows that the internal 4D structure is incorrect for the Waterbag and Hollow distributions. See Figure 7, which shows the 1D projections within a 3D ball in the unplotted coordinates. As the ball shrinks, we approach a 1D slice through the 4D density. You can see that the MENT posterior is uniform, not hollow. Note that both distributions match the data exactly.\n\n\n\n\n\n\nFigure 7\n\n\n\nThe failure to capture the hollow core is somewhat expected from previous studies here, here, and here. Low-dimensional projections average over multiple dimensions and easily hide “holes” in the high-dimensional distribution. There is also a good reason why the MENT posterior is not hollow: the prior is a Gaussian, so MENT will flatten the Gaussian just until the measurements match the data, but no more. In other words, a hollow distribution is very far from the prior.\nI concluded that four-dimensional tomography can be useful in real accelerators when only one-dimensional measurements are available. The reconstruction seems reliable for our purposes, as beams are unlikely to be hollow in the SNS ring. Additional constraints or prior information would be needed to reconstruct arbitrary four-dimensional distributions.\nI think this method will be useful for benchmarking accelerator physics codes. By extracting the beam on different turns, we can measure the distribution as a function of time: \\(f(x, x', y, y', t)\\) and compare to the model predictions.\n\n\n\nFigure 1: (Click to enlarge.)\nFigure 2: \nFigure 3: \nFigure 4: \nFigure 5: \nFigure 6: \nFigure 7:"
  },
  {
    "objectID": "posts/2021-10-16_tomographic-reconstruction-in-four-dimensions/index.html",
    "href": "posts/2021-10-16_tomographic-reconstruction-in-four-dimensions/index.html",
    "title": "Tomographic reconstruction in four dimensions",
    "section": "",
    "text": "A beam of particles in an accelerator is characterized by its distribution in phase space — the space of positions \\(\\{x, y, z\\}\\) and momenta \\(\\{x', y', z'\\}\\). I’m currently working on a project in which we’d like to measure the four-dimensional (4D) phase space distribution \\(f(x, x', y, y')\\) of a fully-accumulated beam in the Spallation Neutron Source (SNS).\nA direct way to do this is to use a series of slits to block all particles outside a small region \\(\\mathbf{x} \\pm \\Delta\\mathbf{x}\\), where \\(\\mathbf{x} = (x, x', y, y')\\), and move \\(\\mathbf{x}\\) through phase space. This method is accurate but slow and not available for our beam (\\(10^{14}\\) protons moving at 90% the speed of light). The best we can do is to measure projections of the distribution; i.e., lower-dimensional views such as\n\\[\nf(x) = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}{f(x, x', y, y') dx' dy dy'}\n\\]\nand estimate the distribution from these projections. This is called tomographic reconstruction.\nTomographic reconstruction is used in a wide variety of fields, particularly in medical imaging where X-rays are used to generate 1D projections of a 2D slice of organic material at various angles. The same idea can be applied to a phase space distribution. Consider \\(f(x, x')\\). It’s straightforward to measure a 1D projection along the \\(x\\) axis by sweeping a vertical conducting wire across the beam path; the problem is that this only corresponds to one projection angle. The trick is to approximate motion in an accelerator as a series of linear transformations of the phase space coordinates: shearing, scaling, and rotation. Thus, the projection of a beam at one location onto the \\(x\\) axis is a scaled projection of a projection onto a rotated axis at a different location. The projection angle can be controlled by varying the electromagnetic fields between the two locations.\nIt’s more challenging to find \\(f(x, x', y, y')\\). It’s simple to reconstruct the \\(4 \\times 4\\) covariance matrix from 1D projections; I’ve implemented this method in the SNS and won’t discuss it here. To obtain the 4D distribution, we generally need at least 2D projections. I’m interested in this approach because the final destination of the SNS beam — the target — is coated with a luminescent material which gives the 2D projection onto the \\(x\\)-\\(y\\) plane. The idea that I’ll begin to explore in this post is whether these 2D projections can be used to reconstruct the 4D phase space distribution. I’ll first go over the common reconstruction algorithms in 2D and mention if they could be extended to 4D."
  },
  {
    "objectID": "posts/2021-10-16_tomographic-reconstruction-in-four-dimensions/index.html#introduction",
    "href": "posts/2021-10-16_tomographic-reconstruction-in-four-dimensions/index.html#introduction",
    "title": "Tomographic reconstruction in four dimensions",
    "section": "",
    "text": "A beam of particles in an accelerator is characterized by its distribution in phase space — the space of positions \\(\\{x, y, z\\}\\) and momenta \\(\\{x', y', z'\\}\\). I’m currently working on a project in which we’d like to measure the four-dimensional (4D) phase space distribution \\(f(x, x', y, y')\\) of a fully-accumulated beam in the Spallation Neutron Source (SNS).\nA direct way to do this is to use a series of slits to block all particles outside a small region \\(\\mathbf{x} \\pm \\Delta\\mathbf{x}\\), where \\(\\mathbf{x} = (x, x', y, y')\\), and move \\(\\mathbf{x}\\) through phase space. This method is accurate but slow and not available for our beam (\\(10^{14}\\) protons moving at 90% the speed of light). The best we can do is to measure projections of the distribution; i.e., lower-dimensional views such as\n\\[\nf(x) = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}{f(x, x', y, y') dx' dy dy'}\n\\]\nand estimate the distribution from these projections. This is called tomographic reconstruction.\nTomographic reconstruction is used in a wide variety of fields, particularly in medical imaging where X-rays are used to generate 1D projections of a 2D slice of organic material at various angles. The same idea can be applied to a phase space distribution. Consider \\(f(x, x')\\). It’s straightforward to measure a 1D projection along the \\(x\\) axis by sweeping a vertical conducting wire across the beam path; the problem is that this only corresponds to one projection angle. The trick is to approximate motion in an accelerator as a series of linear transformations of the phase space coordinates: shearing, scaling, and rotation. Thus, the projection of a beam at one location onto the \\(x\\) axis is a scaled projection of a projection onto a rotated axis at a different location. The projection angle can be controlled by varying the electromagnetic fields between the two locations.\nIt’s more challenging to find \\(f(x, x', y, y')\\). It’s simple to reconstruct the \\(4 \\times 4\\) covariance matrix from 1D projections; I’ve implemented this method in the SNS and won’t discuss it here. To obtain the 4D distribution, we generally need at least 2D projections. I’m interested in this approach because the final destination of the SNS beam — the target — is coated with a luminescent material which gives the 2D projection onto the \\(x\\)-\\(y\\) plane. The idea that I’ll begin to explore in this post is whether these 2D projections can be used to reconstruct the 4D phase space distribution. I’ll first go over the common reconstruction algorithms in 2D and mention if they could be extended to 4D."
  },
  {
    "objectID": "posts/2021-10-16_tomographic-reconstruction-in-four-dimensions/index.html#d-reconstruction",
    "href": "posts/2021-10-16_tomographic-reconstruction-in-four-dimensions/index.html#d-reconstruction",
    "title": "Tomographic reconstruction in four dimensions",
    "section": "2D reconstruction",
    "text": "2D reconstruction\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport proplot as pplt\nfrom skimage import transform\nimport psdist.visualization as psv\n\nI’ll use the following distribution to test the reconstruction algorithms.\n\na = 1.75\nn = 300000\nX = np.vstack([\n    np.random.normal(scale=[1.0, 1.0], loc=[0.0, 0.0], size=(n, 2)),\n    np.random.normal(scale=0.6, loc=[+a, +a], size=(n//5, 2)),\n    np.random.normal(scale=0.6, loc=[+a, -a], size=(n//5, 2)),\n    np.random.normal(scale=0.6, loc=[-a, +a], size=(n//5, 2)),\n    np.random.normal(scale=0.6, loc=[-a, -a], size=(n//5, 2)),\n])\n\nn_bins = 60\nxmax = 5.0\nlimits = [(-xmax, xmax), (-xmax, xmax)]\n\nZ_true, xedges, yedges = np.histogram2d(X[:, 0], X[:, 1], n_bins, limits, density=True)\nxcenters = 0.5 * (xedges[:-1] + xedges[1:])\nycenters = 0.5 * (yedges[:-1] + yedges[1:])\n\nfig, ax = pplt.subplots()\nax.pcolormesh(xcenters, ycenters, Z_true.T)\nax.format(xlabel=\"x\", ylabel=\"x'\")\nplt.close()\n\n\n\n\n\n\nWe now simulate the measurements. Assume we have the ability to rotate the distribution without any shearing or scaling. We then rotate the distribution by different angles and project it onto the \\(x\\) axis. It will turn out that measurement time limits the number of projections we can use; for now, we’ll limit ourselves to 15 projections.\n\ndef rotation_matrix(angle):\n    cs, sn = np.cos(angle), np.sin(angle)\n    return np.array([[cs, sn], [-sn, cs]])\n\n\nn_proj = 15\nangles = np.linspace(0.0, 180.0, n_proj, endpoint=False)\ntransfer_matrices = [rotation_matrix(np.radians(angle)) for angle in angles]\nprojections = np.zeros((n_bins, n_proj))\nfor k in range(n_proj):\n    M = transfer_matrices[k]\n    X_meas = np.apply_along_axis(lambda row: np.matmul(M, row), 1, X)\n    projections[:, k], _ = np.histogram(X_meas[:, 0], n_bins, limits[0], density=True)\n\nfig, ax = pplt.subplots()\nax.pcolormesh(projections.T)\nax.format(xlabel=\"Projection axis\", ylabel=\"Projection number\", ytickminor=False)\nplt.close()\n\n\n\n\n\n\nWe’ll also need some helper functions.\n\ndef apply(M, X):\n    return np.apply_along_axis(lambda row: np.matmul(M, row), 1, X)\n\n\ndef project(Z, indices):\n    if type(indices) is int:\n        indices = [indices]\n    axis = tuple([k for k in range(4) if k not in indices])\n    return np.sum(Z, axis=axis)\n\n\ndef normalize(Z, bin_volume=1.0):\n    count = np.sum(Z)\n    if count == 0.0:\n        return Z\n    return Z / count / bin_volume\n\n\ndef get_bin_volume(limits, n_bins):\n    if type(n_bins) is int:\n        n_bins = len(limits) * [n_bins]\n    return np.prod([((lim[1] - lim[0]) / n) for lim, n in zip(limits, n_bins)])\n\n\ndef process(Z, keep_positive=False, density=False, limits=None):\n    if keep_positive:\n        Z = np.clip(Z, 0.0, None)\n    if density:\n        bin_volume = 1.0\n        if limits is not None:\n            bin_volume = get_bin_volume(limits, Z.shape)\n        Z = normalize(Z, bin_volume)\n    return Z\n\n\ndef plot_rec(Z, Z_true, suptitle=\"\"):\n    fig, axs = pplt.subplots(ncols=3, figsize=(7.25, 2.5))\n    axs[0].pcolormesh(xcenters, ycenters, Z.T, cmap=\"dusk_r\")\n    axs[1].pcolormesh(xcenters, ycenters, Z_true.T, cmap=\"dusk_r\")\n    axs[2].pcolormesh(\n        xcenters,\n        ycenters,\n        (Z - Z_true).T,\n        colorbar=True,\n        colorbar_kw=dict(width=0.075, ticklabelsize=8),\n    )\n    axs.format(xticks=[], yticks=[], suptitle=suptitle)\n    for ax, title in zip(axs, [\"Reconstructed\", \"Original\", \"Error\"]):\n        ax.set_title(title)\n    return fig, axs\n\n\nFiltered Back-Projection (FPB)\nIn filtered back-projection (FBP) each projection is Fourier transformed and then smeared back across the \\(x\\)-\\(x'\\) plane. The Fourier transform acts as a filter and creates a sharper image.\n\nZ = transform.iradon(projections, theta=-angles).T\nZ = process(Z, keep_positive=True, density=True, limits=limits)\nfig, axs = plot_rec(Z, Z_true, 'FBP')\nplt.close()\n\n\n\n\n\n\nFBP generally requires a high number of projections to avoid these streaking artifacts. Let’s see what happens when the number of projections is varied. In each case, we distribute the angles evenly over 180 degrees.\n\n\n\n\n\nWe were also careful to choose an angular range close to 180 degrees to maximize the information carried by the projections.\n\n\n\n\n\nThe angular range in an accelerator is determined by the amount of control we have over the optics between the reconstruction location and the measurement location. It’s possible to run into problems here; for example, varying a magnet too far from its design value could make the beam unacceptably large at a downstream location. Magnets also have limited strengths, and sometimes they can’t be controlled independently. Additionally, achieving a specific projection angle is not always straightforward; usually an optimizer is used to find the correct settings, and this takes time. I’ll come back to these points later.\nFBP can be generalized to 3D reconstruction from 2D projections, but it doesn’t seem straightforward. I don’t know if it could work for a 4D reconstruction from 2D projections. I’m just not sure what back-projection would mean in that case…\n\n\nAlgebraic Reconstruction (ART)\nAlgebraic reconstruction (ART) algorithms are simpler than FBP. The reconstructed \\(x\\)-\\(x'\\) distribution will be defined on a grid. Let \\(\\rho^{(k)}\\) be a vector containing the \\(k\\)th projection, and let \\(\\psi\\) be a vector of the phase space density at the reconstruction location (\\(N^2\\) elements for an \\(N \\times N\\) grid). Since we know the linear transformation connecting the two points, we can write the following matrix equation:\n\\[ \\rho^{(k)} = P^{(k)}\\psi, \\]\nStack these equations for all the projections to get\n\\[ \\rho = P \\psi. \\]\nAll we need to do is invert these equations. The problem is that \\(P\\) could be huge. If there are \\(K\\) projections, \\(P\\) will be have \\(K N \\times N^2\\) elements. Inverting such a matrix could be a pain, so people have developed iterative methods to find the answer. Scikit-image has one ready to go called simultaneous algebraic reconstruction (SART).\n\nZ = transform.iradon_sart(projections, theta=-angles).T\nZ = process(Z, keep_positive=True, density=True, limits=limits)\nfig, axs = plot_rec(Z, Z_true, 'SART (1 iteration)')\nplt.close()\n\n\n\n\n\n\nThe SART algorithm looks like it does better with fewer projections. SART is known to produce a good answer in only one iteration. Running it again with the original reconstruction as an initial estimate can sharpen image but may increase noise. In this case, it looks like running SART again leads to a better reconstruction without increasing noise.\n\nZ = transform.iradon_sart(projections, theta=-angles).T\nZ = transform.iradon_sart(projections, theta=-angles, image=Z.T).T\nZ = process(Z, keep_positive=True, density=True, limits=limits)\nfig, axs = plot_rec(Z, Z_true, 'SART (2 iterations)')\nplt.close()\n\n\n\n\n\n\nART easily generalizes to 4D, but it could become expensive since \\(P\\) would have \\(K N^2 \\times N^4\\) elements. This has been carried out by Wolski (2020) with \\(N = 69\\). I’ll leave investigation of 4D ART for a future post.\n\n\nMENT\nThere is another approach to the problem based on information theory. The idea is that the most probable distribution should be chosen, i.e., find the \\(f(\\mathbf{x})\\) that maximizes the entropy\n\\[ S = -\\int f(\\mathbf{x}) \\log{f(\\mathbf{x})} d\\mathbf{x}\\]\nwith the constraint that the projections of \\(f(\\mathbf{x}\\) are consistent with measurements. The theory behind this algorithm is layed out in Minerbo (1972). The benefit to MENT is that it has a goal in mind and can do very well with only a few projections where ART might introduce significant streaking artifacts. The downside of MENT is that although it can work well with few projections, there is no guarantee, and it may struggle to converge when many projections are used. Unfortunately, this algorithm is a bit tricky to implement; I’m working on it.\nMENT can also work in 4D with 2D projections or 1D projections; the math is pretty much the same.\n\n\nGrid-based methods\nThere is a final method that I should mention: using multi-particle simulation to find the answer. Start with a bunch of particles and transport them to the measurement location. Throw away particles that landed in bins in which the measured projection is zero. Now, generate new particles in the viscinity of the old ones with the number of new particles proportional to the measured distribution in that bin. Repeat until convergence.\nThe advantage of this approach is that it works for any number of dimesions and can also include nonlinear effects in the simulation such as space charge. I’m wondering, though, if this just reduces to a version of ART if the transport is linear; our beam is high energy, which means that space charge can mostly be ignored over short distances. Again, I’ll leave this method for a future post."
  },
  {
    "objectID": "posts/2021-10-16_tomographic-reconstruction-in-four-dimensions/index.html#d-reconstruction-1",
    "href": "posts/2021-10-16_tomographic-reconstruction-in-four-dimensions/index.html#d-reconstruction-1",
    "title": "Tomographic reconstruction in four dimensions",
    "section": "4D reconstruction",
    "text": "4D reconstruction\nI’ve mentioned that at least three methods (ART, MENT, grid-based) will generalize to 4D reconstruction from 2D projections. But what I’d like to look at now is a different, hopefully easier method described in Hock (2013). The method uses 2D projections but only uses 2D reconstruction methods like those described in this post. It’s sort of like a CT scan: 1D projections are used to reconstruct a 2D slice of a human body, and then the slice is moved along the body to reconstruct the 3D image. This would be nice because the conditions for a good 2D reconstruction are well-understood, but might not be clear for higher-dimensional versions of the algorithms.\nFirst, we need a 4D distribution.\n\n# Create a rigid rotating distribution.\nX = np.random.normal(size=(400000, 4))\nX = np.apply_along_axis(lambda row: row / np.linalg.norm(row), 1, X)\nX[:, 3] = +X[:, 0]\nX[:, 1] = -X[:, 2]\n\n# Change the x-y phase difference.\nR = np.zeros((4, 4))\nR[:2, :2] = rotation_matrix(np.pi / 4)\nR[2:, 2:] = rotation_matrix(0.0)\nX = np.apply_along_axis(lambda row: np.matmul(R, row), 1, X)\n\n# Add some noise.\nX += np.random.normal(scale=0.4, size=X.shape)\n\n# Plot the 2D projections.\nn_bins = 80\nlabels = [\"x\", \"x'\", \"y\", \"y'\"]\ng = psv.cloud.corner(\n    X, bins=n_bins, grid_kws=dict(figwidth=5.5), mask=False,\n    labels=labels,\n)\nlimits = [ax.get_xlim() for ax in g.axs[-1, :]]\nplt.close()\n\n\n\n\n\n\nWe can estimate the distribution by binning the particles on a 4D grid.\n\nZ_true, edges = np.histogramdd(X, n_bins, limits, density=True)\ncenters = []\nfor _edges in edges:\n    centers.append(0.5 * (_edges[:-1] + _edges[1:]))\nbin_volume = get_bin_volume(limits, n_bins)\n\n\nDescription of Hock’s method\nNow for the method. Suppose we can independently rotate the \\(x\\)-\\(x'\\) and \\(y\\)-\\(y'\\) projections of the phase space distribution. Let the angles in \\(x\\)-\\(x'\\) be \\(\\left\\{\\mu_{x_1}, \\dots, \\mu_{x_k}, \\dots, \\mu_{x_K}\\right\\}\\) and the angles in \\(y\\)-\\(y'\\) be \\(\\left\\{\\mu_{y_1}, \\dots, \\mu_{y_l}, \\dots, \\mu_{y_L}\\right\\}\\). For each combination of \\(\\mu_x\\) and \\(\\mu_y\\), we measure the beam intensity on a screen. In other words, we create a matrix \\(S\\) such that \\(S_{ijkl}\\) gives the intensity at point \\((x_i, y_j)\\) on the screen for phase advances (angles) \\(\\mu_{x_k}\\) and \\(\\mu_{y_l}\\).\n\nK = 15 # number of angles in x dimension\nL = 15 # number of angles in y dimension\nmuxx = muyy = np.linspace(0., 180., K, endpoint=False)\n\nxx_list = []\nfor mux in muxx:\n    Mx = rotation_matrix(np.radians(mux))\n    xx_list.append(apply(Mx, X[:, :2])[:, 0])\n    \nyy_list = []\nfor muy in muyy:\n    My = rotation_matrix(np.radians(muy))\n    yy_list.append(apply(My, X[:, 2:])[:, 0])\n    \nS = np.zeros((n_bins, n_bins, K, L))\nfor k, xx in enumerate(xx_list):\n    for l, yy in enumerate(yy_list):\n        S[:, :, k, l], _, _ = np.histogram2d(xx, yy, n_bins, (limits[0], limits[2]))\n\n\n\n\n\n\nThe above animation shows a few steps in the scan; since the transfer matrices are rotation matrices, only the cross-plane correlation changes.\nWe can immediately reconstruct the 3D projection the 4D phase space distribution using this data. Consider one row of the beam image — the intensity along the row gives a 1D projection onto the \\(x\\) axis for a vertical slice of the beam. We have a set of these 1D projections at different \\(\\mu_k\\) which we can use to reconstruct the \\(x\\)-\\(x'\\) distribution at this vertical slice using any 1D \\(\\rightarrow\\) 2D reconstruction method. I’ll use SART since this seemed to work well with 15 projections. We repeat this at each slice. We thus have an array \\(D\\) such that \\(D_{j,l,r,s}\\) gives the density at \\(x = x_r\\), \\(x' = x'_s\\) for \\(y = y_j\\) and \\(\\mu_y = \\mu_{y_l}\\).\n\nD = np.zeros((n_bins, L, n_bins, n_bins))\nfor j in range(n_bins):\n    for l in range(L):\n        _Z = transform.iradon_sart(S[:, j, :, l], theta=-muxx).T\n        _Z = transform.iradon_sart(S[:, j, :, l], theta=-muxx, image=_Z.T).T\n        D[j, l, :, :] =_Z\n\nI should also mention that the reconstruction grid doesn’t need to be the same size as the measurement grid. We can now do a similar thing in the vertical plane. For each bin in the reconstructed x-x’ grid, D[:, :, r, s] gives the projections of \\(y\\)-\\(y'\\) at each \\(\\mu_{y_l}\\); thus, \\(y\\)-\\(y'\\) can be reconstructed at each \\(x_r\\) and \\(x'_s\\), and we have an array \\(Z\\) such that \\(Z_{r,s,t,u}\\) gives the density at \\(x = x_r\\), \\(x' = x'_s\\), \\(y = y_t\\), \\(y' = y'_u\\).\n\nZ = np.zeros((n_bins, n_bins, n_bins, n_bins))\nfor r in range(n_bins):\n    for s in range(n_bins):\n        _Z = transform.iradon_sart(D[:, :, r, s], theta=-muyy).T\n        _Z = transform.iradon_sart(D[:, :, r, s], theta=-muyy, image=_Z.T).T\n        Z[r, s, :, :] = _Z\n\n\n\nAccuracy\nSART could make some bins negative, so set those to zero. We then need to normalize Z for comparison with Z_true.\n\nZ = process(Z, keep_positive=True, density=True, limits=limits)\nprint('min(Z) = {}'.format(np.min(Z)))\nprint('max(Z) = {}'.format(np.max(Z)))\nprint('sum(Z) * bin_volume = {}'.format(np.sum(Z) * bin_volume))\nprint()\nprint('min(Z_true) = {}'.format(np.min(Z_true)))\nprint('max(Z_true) = {}'.format(np.max(Z_true)))\nprint('sum(Z_true) * bin_volume = {}'.format(np.sum(Z_true) * bin_volume))\n\nmin(Z) = 0.0\nmax(Z) = 0.17036653561113582\nsum(Z) * bin_volume = 0.9999999999999999\n\nmin(Z_true) = 0.0\nmax(Z_true) = 1.3628864309355835\nsum(Z_true) * bin_volume = 0.9999999999999879\n\n\nI’m not sure the best way to quantify the difference between the distributions. My initial thought is to subtract Z_true from Z, take the absolute value, and divide by the number of bins; this would give the average absolute error over the bins.\n\navg_abs_err_per_bin = np.sum(np.abs(Z - Z_true)) / Z.size\nprint('Average absolute error per bin = {}'.format(avg_abs_err_per_bin))\n\nAverage absolute error per bin = 0.002540311656083561\n\n\nLots of bins are empty so this might not be the most meaningful number. For now, it’s probably more helpful to view the differences between the projections.\n\n\nCode\nfig, axs = pplt.subplots(ncols=4, figsize=(6, 2), spanx=False)\nfor i in range(4):\n    axs[i].plot(centers[i], project(Z_true, i), color='black', label='True')\n    axs[i].plot(centers[i], project(Z, i), color='red8', ls='dotted', label='Reconstructed')\n    axs[i].set_xlabel(labels[i])\naxs[0].legend(loc=(0.0, 1.02), framealpha=0.0, ncol=1)\nplt.close()\n\n\n\n\n\n\n\n\n\nCode\nindices = [(0, 1), (2, 3), (0, 2), (0, 3), (2, 1), (1, 3)]\nfig, axs = pplt.subplots(nrows=6, ncols=3, figwidth=5.0, sharex=False, sharey=False)\nfor row, (i, j) in enumerate(indices):\n    _Z_true = project(Z_true, [i, j])\n    _Z = project(Z, [i, j])\n    axs[row, 0].pcolormesh(centers[i], centers[j], _Z.T)\n    axs[row, 1].pcolormesh(centers[i], centers[j], _Z_true.T)\n    axs[row, 2].pcolormesh(\n        centers[i],\n        centers[j],\n        (_Z - _Z_true).T,\n        colorbar=True,\n        colorbar_kw=dict(width=0.075),\n    )\n    axs[row, 0].annotate(\n        \"{}-{}\".format(labels[i], labels[j]),\n        xy=(0.02, 0.92),\n        xycoords=\"axes fraction\",\n        color=\"white\",\n    )\nfor ax, title in zip(axs[0, :], [\"Reconstructed\", \"True\", \"Error\"]):\n    ax.set_title(title)\naxs.format(xticks=[], yticks=[])\nplt.close()\n\n\n\n\n\n\n\nIn the second plot, the error is given in number of particles. It looks like the method worked well to reconstruct this distribution. [Edit 2023-02-13: This is not a sufficient comparison in 4D: one needs to look at slices, and use more complex distribution.]"
  },
  {
    "objectID": "posts/2021-10-16_tomographic-reconstruction-in-four-dimensions/index.html#feasibility",
    "href": "posts/2021-10-16_tomographic-reconstruction-in-four-dimensions/index.html#feasibility",
    "title": "Tomographic reconstruction in four dimensions",
    "section": "Feasibility",
    "text": "Feasibility\n\nImaging system\nThe SNS target is a steel vessel containing liquid mercury. At the beginning of this post, I mentioned that the SNS has a target imaging system. Here is a diagram showing how the target imaging system works…\n\n\n\n\n\n… and an example of an image.\n\n\n\n\n\nTo reduce noise, the image was averaged over five beam pulses and a Gaussian blur was applied with \\(\\sigma = 3\\). There are four fiducial markers that are visible as dark spots on the corner of the beam. The black ellipse represents a measurement from wire-scanners upstream of the target. Collecting this image is easy.\n\n\nOptics control\nI’ve assumed that the transfer matrices connecting the coordinates are simple rotation matrices. Although that isn’t true in reality, there is a trick we can play. Any transfer matrix \\(M\\) can be writen as the product \\(M = V R V^{-1}\\), where \\(V\\) provides shearing + scaling and \\(R\\) is a rotation by the phase advances \\(\\mu_x\\) and \\(\\mu_y\\). Applying \\(V^{-1}\\) to the coordinates is called normalizing the coordinates. What we can do is normalize (scale) the measured profiles, perform the reconstruction in normalized phase space, then unnormalize the reconstructed distribution.\nIn normalized phase space, the transfer matrices are just rotation matrices (\\(R\\)) and the projection angles are the phase advances. This is an advantage because the phase advances are easier to control than the true projection angles. Also, the reconstructed \\(x\\)-\\(x'\\) and \\(y\\)-\\(y'\\) projections will be somewhat circular in this space, which should reduce errors.\nThe (un)normalization steps will involve interpolation since we’re performing a transformation on an image, and I’m not yet sure how this will affect the reconstruction error. Anyhow, we don’t have to reconstruct in normalized phase space if this turns out to be an issue.\nSo, the question is whether we can independently vary the \\(x\\) and \\(y\\) phase advances at the target, and if so, by how much. There are five independent quadrupoles before the target — three focusing and two defocusing — QH26, QV27, QH28, QV29 and QH30. Before that, there are eight quadrupoles — four focusing and four defocusing — that share two powers supplies. One power supply controls QH18, QH20, QH22, and QH24, and another controls QV19, QV21, QV23, and QV25.\nThere are also a constraints. First, the beam size far away from the target should remain small. We measure the beam size using the \\(\\beta\\) function, which should remain below 35 [m/rad]. The beam becomes much larger close to the target; there, the \\(\\beta\\) function should remain below 100 [m/rad]. Finally, the SNS target has tight constraints on the beam size and the beam density. It’s safest to keep the \\(\\beta\\) functions within 10 or 20 percent of the design value.\nI plugged all this into an optimizer and asked to vary the phase advances in a 180 degree range. After fideling with the starting and ending phases, I was able to more-or-less do this. I used 15 phase advances in \\(x\\) and 15 phase advances in \\(y\\), just like in this post. Each optimization took around 16 seconds, so the script took around an hour to run. Below are the computed phase advances at each step as well as the strengths of the seven magnet power supplies.\n\n\n\n\n\nThere are a few steps near the end of the scan in which the phase advances aren’t exactly correct, but they are close. I can always decrease the phase coverage slighly, or maybe these steps just need a slightly different initial guess. Here are the beta functions for all steps in one plot. The plot ends at the target.\n\n\n\n\n\nThe bottom plot is just the integral of the inverse of the top plot. The beta functions remain within their limits.\n\n\nOutlook\nInitial tests of Hock’s 4D reconstruction method are promising. There are a number of things to investigate. Ideally, the minimum number of projections is used. First, I should play with the number of projections using ART to see how the reconstruction error scales. I would also like to implement MENT, which should do better with fewer projections. I should then test the “direct” 4D reconstruction methods such as ART, MENT, or grid-based and compare them to Hock’s method. I should also investigate how the methods respond to noise, changes in the grid size, and more complex distributions.\nThe script to perform the data collection for this method is pretty simple: compute the correct optics, update the live magnet strengths accordingly, send a beam to the target, and save the target image. The biggest issue is execution time. In our next study at the SNS, we’ll have plenty of time to run the script in addition to our other tasks. Hopefully this will be a helpful diagnostic."
  },
  {
    "objectID": "posts/2021-05-13_matched-envelope/index.html",
    "href": "posts/2021-05-13_matched-envelope/index.html",
    "title": "Computing matched envelopes",
    "section": "",
    "text": "My first peer-reviewed paper was published in Physical Review Accelerators and Beams (PRAB) at the end of April [1]. I thought I would summarize the results of the paper here."
  },
  {
    "objectID": "posts/2021-05-13_matched-envelope/index.html#self-consistent-beam-distributions",
    "href": "posts/2021-05-13_matched-envelope/index.html#self-consistent-beam-distributions",
    "title": "Computing matched envelopes",
    "section": "1. Self-consistent beam distributions",
    "text": "1. Self-consistent beam distributions\n\n1.1. Space charge\nA beam of charged particles generates an electric field, which then exerts a force on each particle, whose motion modifies the electric field, and so on. This problem is familiar to plasma physics. One important feature of charged particle beams is that they are non-neutral, i.e., all particles have the same charge. Thus, all forces are long-range, making the problem difficult to handle analytically.\nHere, I will briefly discuss one way in which space charge limits the performance of particle accelerators. In particular, I will discuss the difficulties caused by the fact that the space charge force on a given particle depends nonlinearly on the particle’s position. For example, consider the following two charge distributions and the radial electric fields they produce.\n\n\nImports\n#hide\nfrom matplotlib import animation\nfrom matplotlib.lines import Line2D\nfrom matplotlib.patches import Ellipse\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport proplot as pplt\nimport psdist.visualization as psv\nfrom scipy.integrate import odeint\n\n\n\n\nCode\n# Generate a uniform density distribution.\nnp.random.seed(6)\nn = int(1e4)\nrho = 2.0 * np.sqrt(np.random.uniform(size=n))\nphi = np.random.uniform(0, 2 * np.pi, size=n)\nX1 = np.vstack([rho * np.cos(phi), rho * np.sin(phi)]).T\n\n# Generate a Gaussian distribution.\nX2 = np.random.normal(scale=1.0, size=(n, 2))\n\n# Plot the distributions.\nfig, axs = pplt.subplots(\n    nrows=2, ncols=2, figwidth=5.5, span=False, wspace=7.0, height_ratios=[1, 0.3]\n)\nfor ax, X in zip(axs[0, :], [X1, X2]):\n    ax.scatter(X[:, 0], X[:, 1], s=0.05, c=\"k\")\naxs[0, :].format(aspect=1.0, yticks=[], xspineloc=\"neither\", yspineloc=\"neither\")\n\n\n# Plot the radial electric field.\ndef Efield(r, kind=\"gaussian\"):\n    if r == 0:\n        return 0.0\n    if kind == \"uniform\":\n        if abs(r) &lt;= 2.0:\n            return 0.25 * r\n        else:\n            return 1.0 / r\n    elif kind == \"gaussian\":\n        return (1.0 / r) * (1.0 - np.exp(-0.5 * r**2))\n\n\nradii = np.linspace(-4.0, 4.0, 100)\nfor ax, kind in zip(axs[1, :], [\"uniform\", \"gaussian\"]):\n    electric_field = np.array([Efield(r, kind) for r in radii])\n    ax.plot(radii, electric_field, color=\"black\")\naxs[1, :].format(\n    ylabel=r\"$E_r$\",\n    xlabel=r\"r / $\\sigma$\",\n    xspineloc=\"bottom\",\n    yspineloc=\"left\",\n    yticks=[0.0],\n    ygrid=True,\n)\nplt.close()\n\n\n\n\n\nFig. 1. Comparison of radial electric field generated by a uniform distribution (left) and Gaussian distribution (right).\n\n\nThe left distribution produces an electric field that is proportional to the radius, i.e., it gives rise to linear space charge forces. The right distribution produces an electric field that depends nonlinearly on the radius, i.e., it gives rise to nonlinear space charge forces.\nNonlinear forces lead to filamentation and effective growth in phase space volume (in a course-grained sense). We typically estimate the phase space volume from the covariance matrix of the distribution: the covariance matrix defines an ellipsoid whose volume is easy to compute. We call this volume the root-mean-square emittance. Emittance growth is generally undesired as it degrades the beam quality and leads to beam loss. Fig. 2. shows an example of the emittance growth that can be caused by the space charge forces in an intense beam propagating in a linear focusing channel.\n\n\n\nFig. 2. Emittance growth in an intense beam propagating in a linear accelerator. The emittances \\(\\varepsilon_{x,y}\\) correspond to the areas in the projected phase spaces \\(x\\)-\\(p_x\\) and \\(y\\)-\\(p_y\\) (Source: [2].)\n\n\nAnother difficulty is specific to circular accelerators (rings). Rings are designed such that each particle performs stable transverse oscillations about some reference trajectory. The oscillations are not simple-harmonic, but they can be described by a small number of parameters.1 One of these parameters is the tune, which is something like a frequency. For certain values of the tune, nonlinear magnetic fields in the ring can drive single-particle resonances.2 To avoid resonance conditions up to order \\(|M_x + M_y|\\), the tunes \\(\\nu_{x, y}\\) must be far from the lines defined by \\(M_x \\nu_x + M_y \\nu_y = N\\), where \\(M_x\\), \\(M_y\\), and \\(N\\) are integers.\nSpace charge decreases the tune of each particle. This wouldn’t be a problem if the shift was the same for each particle, but a nonlinear space charge force causes the tune shift to depend on the particle’s position, resulting in a tune spread that grows with the beam intensity. At high intensities, it becomes unavoidable that some particles cross dangerous low-order resonance lines. This is illustrated in Fig. 3.\n\n\n\nFig. 3. Simulated tune spread in the SNS ring. (Source: [3].)\n\n\nIt seems plausible that a beam producing linear space charge forces would alleviate these problems, enabling higher beam intensities. We already have an example of one such beam in Fig. 1: a uniform-density ellipse. But if we transported this beam through an accelerator, would its uniform density be maintained? There are only a few cases in which the answer is “yes”; we call these cases self-consistent.\n\n\n1.2. Vlasov equilibria\nTo evolve the beam, we need to specify not only the distribution of particle positions but also the distribution of velocities; in other words, we need to specify the distribution in four-dimensional position-momentum space (phase space). We then apply the framework of statistical mechanics, which I will briefly summarize.\nA collisionless, two-dimensional[^By two-dimensional, we mean that the beam extends forever in the \\(z\\) direction.] beam of charged particles may be represented by a distribution function \\(f(\\mathbf{x}, \\mathbf{x}', s)\\), where \\(\\mathbf{x} = (x, y)^T\\) is the transverse position, \\(\\mathbf{x}' = d\\mathbf{x}/ds\\) is the transverse momentum, \\(s = \\beta c t\\) is the axial position, \\(t\\) is the time, \\(\\beta c\\) is the beam velocity, and \\(c\\) is the speed of light. In a linear, uncoupled focusing system represented by \\(\\mathbf{\\kappa(s)}\\), the distribution function evolves according to the Vlasov-Poisson system of equations:\n\\[\n\\frac{\\partial{f}}{\\partial{s}} +\n\\mathbf{x}'\\cdot \\frac{\\partial{f}}{\\partial{\\mathbf{x}}} +\n\\mathbf{x}'' \\cdot \\frac{\\partial{f}}{\\partial{\\mathbf{x}'}}\n= 0.\n\\tag{1}\\]\n\\[\n\\mathbf{x}'' + \\mathbf{\\kappa}(s)\\mathbf{x} =\n-\\frac{q}{mc^2\\beta^2\\gamma^3} \\frac{\\partial \\Phi}{\\partial \\mathbf{x}}.\n\\tag{2}\\]\nHere \\(q\\) is the charge, \\(m\\) is the mass, \\(\\gamma = \\left(1 - \\beta^2\\right)^{-1/2}\\), and the electric potential \\(\\Phi\\) is determined self-consistently from the Poisson equation:\n\\[\n-\\frac{\\partial^2 \\Phi}{\\partial \\mathbf{x}^2} =\n\\frac{q}{\\varepsilon_0} \\int{f(\\mathbf{x}, \\mathbf{x}', s) \\mathbf{dx}'}.\n\\tag{3}\\]\nThis highly nonlinear integro-differential system of equations is difficult to solve analytically. Exact solutions are called equilibrium distributions and have the form \\(f = f(\\{C_i\\}),\\) where \\(\\{C_i\\}\\) are invariants of the motion. When the focusing is time-independent (\\(\\mathbf{\\kappa}(s) = \\mathbf{\\kappa}\\)), the Hamiltonian is an invariant and any function of the Hamiltonian is an equilibrium distribution. But when the focusing is time-dependent, the solution is unclear.\nFor some time, only one solution existed for linear time-dependent systems, but recently, at least one additional solution has been found. Both solutions project to a uniform density ellipse in the \\(x\\)-\\(y\\) plane, producing linear space charge forces. And since they are equilibrium solutions, they are guaranteed to maintain their functional form as they evolve. This is just what we define as a self-consistent distribution; self-consistent distributions are the subset of Vlasov equilibrium distributions that produce linear space charge forces. I will not discuss how these solutions are found in this post. I will instead focus on some of their properties.\n\n\n1.2. KV distribution\nThe first self-consistent distribution was derived by a pair of Russian scientists in 1959 and is known as the KV distribution. Particles in the KV distribution uniformly populate the boundary of an ellipsoid in four-dimensional (4D) phase space. In normalized coordinates, it looks something like\n\\[\nf(x, p_x, y, p_y) = \\delta(x^2 + {p_y}^2 + y^2 + {p_y}^2 - 1),\n\\tag{4}\\]\nwhere \\(\\delta\\) is the Diract delta function. Any 2D projection of this 4D shell is a uniform density ellipse.\n\n\nCode\nX = np.random.normal(size=(int(7e6), 4))\nX = np.apply_along_axis(lambda x: x / np.linalg.norm(x), 1, X)\ngrid = psv.cloud.corner(\n    X,\n    autolim_kws=dict(pad=0.25),\n    labels=[r\"$x$\", r\"$p_x$\", r\"$x$\", r\"$p_y$\"],\n    grid_kws=dict(figwidth=4.5),\n    rms_ellipse=True,\n    rms_ellipse_kws=dict(level=2.0),\n)\nplt.close()\n\n\n\n\n\nFig. 4. 1D and 2D projections of the KV distribution.\n\n\nIt can be (not easily) shown that the electric field within a uniform-density upright ellipse is\n\\[\n\\mathbf{E}(x, y) \\propto\n\\frac{x}{c_x (c_x + c_y)} \\hat{x} +\n\\frac{y}{c_y (c_x + c_y)} \\hat{y},\n\\tag{5}\\]\nwhere \\(c_x\\) and \\(c_y\\) are the semi-axes of the ellipse. Notice that the field is both linear and uncoupled — \\(x\\) component of the field is proportional to \\(x\\); the \\(y\\) component of the field is proportional to \\(y\\).\nFrom here, one can derive a system of differential equations to evolve the distribution. This is an incredible simplification of the Vlasov-Poisson system. The equations track the beam envelope, the elliptical boundary containing the particles in the \\(x\\)-\\(y\\) plane. These envelope equations, as they’re called, have been important for understanding space charge effects. In addition to providing a theoretical benchmark for computer simulations, they capture the approximate behavior of more realistic beams in some cases.3 More details are in  [4].\nLet’s try integrating these equations in a simple periodic focusing system.\n\ndef fodo(s, quad_strength=0.556, cell_length=5.0):\n    s = (s % cell_length) / cell_length\n    delta = 0.125\n    if s &lt; delta or s &gt; 1 - delta:\n        return +quad_strength\n    elif 0.5 - delta &lt;= s &lt; 0.5 + delta:\n        return -quad_strength\n    return 0.0\n\n\ndef kv_derivs(params, s, Q, foc):\n    cx, cxp, cy, cyp = params\n    k0x = foc(s)\n    k0y = -k0x\n    w = np.zeros(4)\n    w[0] = cxp\n    w[2] = cyp\n    w[1] = -k0x * cx + 2.0 * Q / (cx + cy) + 16.0 * epsx**2 / (cx**3)\n    w[3] = -k0y * cy + 2.0 * Q / (cx + cy) + 16.0 * epsy**2 / (cy**3)\n    return w\n\n\ndef track_kv(params, positions, Q, foc):\n    tracked = odeint(kv_derivs, params, positions, args=(Q, foc))\n    sizes = tracked[:, [0, 2]]\n    sizes = sizes * 1000.0  # convert to mm\n    return sizes\n\n\n# Create KV envelope\nalphax, alphay, betax, betay = 0.0, 0.0, 8.017, 1.544\nepsx = epsy = 10e-6\ncx = 2 * np.sqrt(epsx * betax)\ncy = 2 * np.sqrt(epsy * betay)\ncxp = cyp = 0.0\nparams = [cx, cxp, cy, cyp]\n\n# Integrate envelope equations\ncell_length = 5.0\nperiods = 4\nnpts = 1000\npositions = np.linspace(0, cell_length * periods, npts)\nQ = 1.0e-5\nsizes = track_kv(params, positions, Q, fodo)\nsizes0 = track_kv(params, positions, 0.0, fodo)\n\n\n\nCode\ncolors = pplt.Cycle(\"colorblind\").by_key()[\"color\"]\nkws1 = dict(fc=\"lightgrey\", lw=0.75, ec=\"None\")\nkws2 = dict(fill=False, ls=\"--\", color=\"k\", lw=0.5, alpha=0.5)\nstride = 10\numin = np.min(sizes)\numax = np.max(sizes)\numax_pad = 1.25 * umax\n\nfig, axs = pplt.subplots(\n    nrows=2,\n    ncols=2,\n    figsize=(7, 2.5),\n    spany=False,\n    aligny=True,\n    sharey=False,\n    sharex=False,\n    hspace=0.2,\n    height_ratios=[5.0, 1.0],\n    width_ratios=[2.75, 1.0],\n)\naxs[0, 0].format(xlabel=\"\", ylabel=\"Beam size [mm]\", ylim=(umin - 5, umax + 5))\naxs[1, 0].format(xlabel=\"s [m]\", ylabel=r\"$k_x$\", yticks=[0], ylim=(-0.6116, 0.6116))\naxs[:, 0].format(xlim=positions[[0, -1]])\naxs[1, 0].spines[\"top\"].set_visible(False)\naxs[0, 1].format(\n    xticklabels=[],\n    yticklabels=[],\n    xlabel=\"x\",\n    ylabel=\"y\",\n    xlim=(-umax_pad, umax_pad),\n    ylim=(-umax_pad, umax_pad),\n)\naxs[0, 1].format(xspineloc=\"bottom\", yspineloc=\"left\")\naxs[1, 1].axis(\"off\")\naxs[0, 0].format(xticklabels=[])\naxs[0, 0].legend(\n    handles=[Line2D([0], [0], color=colors[0]), Line2D([0], [0], color=colors[1])],\n    labels=[r'$\\sqrt{\\langle{x^2}\\rangle}$', r'$\\sqrt{\\langle{y^2}\\rangle}$'],\n    ncols=1,\n    loc=\"upper left\",\n    fontsize=\"small\",\n    handlelength=1.5,\n)\naxs[1, 0].plot(positions, [fodo(s) for s in positions], color=\"k\", lw=1)\nplt.close()\n\n\n(line1,) = axs[0, 0].plot([], [])\n(line2,) = axs[0, 0].plot([], [])\naxs[0, 0].format(cycle=\"colorblind\")\n(line3,) = axs[0, 0].plot([], [], ls=\"--\", lw=0.5)\n(line4,) = axs[0, 0].plot([], [], ls=\"--\", lw=0.5)\n\n\ndef update(i):\n    i = i * stride\n    line1.set_data(positions[:i], sizes[:i, 0])\n    line2.set_data(positions[:i], sizes[:i, 1])\n    line3.set_data(positions[:i], sizes0[:i, 0])\n    line4.set_data(positions[:i], sizes0[:i, 1])\n    for patch in axs[0, 1].patches:\n        patch.set_visible(False)\n        \n    for _sizes, _kws in zip([sizes, sizes0], [kws1, kws2]):\n        axs[0, 1].add_patch(\n            Ellipse((0, 0), 2.0 * _sizes[i, 0], 2.0 * _sizes[i, 1], **_kws)\n        )\n\n\nanim = animation.FuncAnimation(\n    fig, update, frames=len(positions[::stride]), interval=(1000.0 / 14.0)\n)\n\n\n\n\n\nFig. 5. Evolution of the KV distribution in a FODO lattice. Top left: horizontal and vertical beam size. Bottom left: Lattice focusing strength. Right: beam ellipse in the \\(x\\)-\\(y\\) plane. Dashed lines are without space charge and solid lines are with space charge.\n\n\nNotice that space charge causes mismatch oscillations: without space charge, the beam repeats after each focusing period. More on this in a moment.\n\n\n1.4. Danilov distribution\nRecently, a larger class of self-consistent distributions was discovered. It has the following form.\n\\[\nf(x, p_x, y, p_y) = \\delta(p_x - e_{11} x - e_{12} y) \\delta(p_y - e_{21} x - e_{22} y),\n\\tag{6}\\]\nwhere the \\(e_{ij}\\) terms are constants. Suppose \\(e_{11} = e_{22} = 0\\) and \\(e_{21} = -e_{12} = 1\\) so that \\(p_y = x\\) and \\(p_x = -y\\); this describes a rotating rigid disk. Here are the 1D and 2D projections in this case:\n\n\nCode\nX = np.random.normal(size=(int(7e6), 4))\nX = np.apply_along_axis(lambda x: x / np.linalg.norm(x), 1, X)\nX[:, 3] = +X[:, 0]\nX[:, 1] = -X[:, 2]\ngrid = psv.cloud.corner(\n    X,\n    autolim_kws=dict(pad=0.25),\n    labels=[r\"$x$\", r\"$p_x$\", r\"$x$\", r\"$p_y$\"],\n    grid_kws=dict(figwidth=4.5),\n    rms_ellipse=True,\n    rms_ellipse_kws=dict(level=2.0),\n)\nplt.close()\n\n\n\n\n\nFig. 6. 1D and 2D projections of the Danilov distribution.\n\n\nIn general, the particles in the beam swirl in a vortex pattern within an ellipse, always with a uniform density. It is also apparently possible to construct this type of vortex distribution in six-dimensional phase space, which is not true of the KV distribution.\nWe can again derive equations for the elliptical beam envelope, but they are now going to include coupling between \\(x\\) and \\(y\\). This is because the beam ellipse tiltes in the \\(x\\)-\\(y\\)- plane, producing a term proportional to \\(xy\\) in the electric field. More details can be found in [5].\n\ndef get_tilt_angle(a, b, e, f):\n    return -0.5 * np.arctan2(2.0 * (a * e + b * f), a**2 + b**2 - e**2 - f**2)\n\n\ndef get_radii(a, b, e, f):\n    phi = get_tilt_angle(a, b, e, f)\n    sin, cos = np.sin(phi), np.cos(phi)\n    sin2, cos2 = sin**2, cos**2\n    xx = a**2 + b**2\n    yy = e**2 + f**2\n    xy = a * e + b * f\n    cx = np.sqrt(abs(xx * cos2 + yy * sin2 - 2 * xy * sin * cos))\n    cy = np.sqrt(abs(xx * sin2 + yy * cos2 + 2 * xy * sin * cos))\n    return cx, cy\n\n\ndef danilov_derivs(params, s, Q, foc):\n    k0x = foc(s)\n    k0y = -k0x\n    a, b, ap, bp, e, f, ep, fp = params\n    phi = get_tilt_angle(a, b, e, f)\n    cx, cy = get_radii(a, b, e, f)\n    cos, sin = np.cos(phi), np.sin(phi)\n    cos2, sin2, sincos = cos**2, sin**2, sin * cos\n    T = 2.0 * Q / (cx + cy)\n    w = np.zeros(8)\n    w[0] = ap\n    w[1] = bp\n    w[4] = ep\n    w[5] = fp\n    w[2] = -k0x * a + T * ((a * cos2 - e * sincos) / cx + (a * sin2 + e * sincos) / cy)\n    w[3] = -k0x * b + T * ((e * sin2 - a * sincos) / cx + (e * cos2 + a * sincos) / cy)\n    w[6] = -k0y * e + T * ((b * cos2 - f * sincos) / cx + (b * sin2 + f * sincos) / cy)\n    w[7] = -k0y * f + T * ((f * sin2 - b * sincos) / cx + (f * cos2 + b * sincos) / cy)\n    return w\n\n\ndef track_danilov(params, positions, Q, foc):\n    tracked = odeint(danilov_derivs, params, positions, args=(Q, foc))\n    a, b, ap, bp, e, f, ep, fp = tracked.T\n    xsizes = np.sqrt(a**2 + b**2)\n    ysizes = np.sqrt(e**2 + f**2)\n    sizes = np.vstack([xsizes, ysizes]).T * 1000\n    cx, cy = get_radii(a, b, e, f)\n    radii = np.vstack([cx, cy]).T * 1000\n    angles = np.degrees(get_tilt_angle(a, b, e, f))\n    return sizes, radii, angles\n\n\n# (Calculated matched initial parameters offline)\nparams = np.array([0.0179, 0.0, 0, 0.0022, 0, -0.0079, 0.0051, 0])\npositions = np.linspace(0.0, 20.0, 1000)\nsizes, radii, angles = track_danilov(params, positions, Q, fodo)\nsizes0, radii0, angles0 = track_danilov(params, positions, 0.0, fodo)\n\n\n\nCode\numin = np.min(sizes)\numax = np.max(sizes)\numax_pad = 1.25 * umax\n\nfig, axs = pplt.subplots(\n    nrows=2,\n    ncols=2,\n    figsize=(7, 2.5),\n    spany=False,\n    aligny=True,\n    sharey=False,\n    sharex=False,\n    hspace=0.2,\n    height_ratios=[5.0, 1.0],\n    width_ratios=[2.75, 1.0],\n)\naxs[0, 0].format(xlabel=\"\", ylabel=\"Beam size [mm]\", ylim=(umin - 5, umax + 5))\naxs[1, 0].format(xlabel=\"s [m]\", ylabel=r\"$k_x$\", yticks=[0], ylim=(-0.6116, 0.6116))\naxs[:, 0].format(xlim=positions[[0, -1]])\naxs[1, 0].spines[\"top\"].set_visible(False)\naxs[0, 1].format(\n    xticklabels=[],\n    yticklabels=[],\n    xlabel=\"x\",\n    ylabel=\"y\",\n    xlim=(-umax_pad, umax_pad),\n    ylim=(-umax_pad, umax_pad),\n)\naxs[0, 1].format(xspineloc=\"bottom\", yspineloc=\"left\")\naxs[1, 1].axis(\"off\")\naxs[0, 0].format(xticklabels=[])\naxs[0, 0].legend(\n    handles=[Line2D([0], [0], color=colors[0]), Line2D([0], [0], color=colors[1])],\n    labels=[r'$\\sqrt{\\langle{x^2}\\rangle}$', r'$\\sqrt{\\langle{y^2}\\rangle}$'],\n    ncols=1,\n    loc=\"upper left\",\n    fontsize=\"small\",\n    handlelength=1.5,\n)\naxs[1, 0].plot(positions, [fodo(s) for s in positions], color=\"k\", lw=1)\nplt.close()\n\n\nline1, = axs[0, 0].plot([], [])\nline2, = axs[0, 0].plot([], [])\naxs[0, 0].format(cycle='colorblind')\nline3, = axs[0, 0].plot([], [], ls='--', lw=0.5)\nline4, = axs[0, 0].plot([], [], ls='--', lw=0.5)\n\n\ndef update(i):\n    i *= stride\n    line1.set_data(positions[:i], sizes[:i, 0])\n    line2.set_data(positions[:i], sizes[:i, 1])\n    line3.set_data(positions[:i], sizes0[:i, 0])\n    line4.set_data(positions[:i], sizes0[:i, 1])\n    for patch in axs[0, 1].patches:\n        patch.set_visible(False)\n    axs[0, 1].add_patch(\n        Ellipse(\n            (0, 0), 2.0 * radii[i, 0], 2.0 * radii[i, 1], angles[i], \n            fc='lightgrey', lw=0.75, ec='None'\n        )\n    )\n    axs[0, 1].add_patch(\n        Ellipse(\n            (0, 0), 2.0 * radii0[i, 0], 2.0 * radii0[i, 1], angles0[i], \n            fill=False, ls='--', color='k', lw=0.5, alpha=0.5,\n        )\n    )\n\n    \nanim = animation.FuncAnimation(\n    fig, update, frames=len(positions[::stride]), interval=(1000.0 / 14.0)\n)\n\n\n\n\n\nFig. 7. Evolution of the Danilov distribution in a FODO lattice. Top left: horizontal and vertical beam size. Bottom left: Lattice focusing strength. Right: beam ellipse in the \\(x\\)-\\(y\\) plane. Dashed lines are without space charge and solid lines are with space charge.\n\n\nNotice that the beam tilts even without space charge. This has to do with the phase relationship between \\(x\\) and \\(y\\). When the beam is upright in the \\(x\\)-\\(y\\) plane, \\(x\\) and \\(y\\) are 90 degrees out of phase (think of circular motion); on the other hand, a 0 or 180-degree phase difference would lead to a diagonal line in the \\(x\\)-\\(y\\) plane and no correlations in \\(x\\)-\\(p_y\\) or \\(y\\)-\\(p_x\\).\nWhat is a bit less obvious is how space charge affects the beam when it tilts. Although the forces are still linear, the areas of the \\(x\\)-\\(p_x\\) and \\(y\\)-\\(p_y\\) projections are longer conserved. Below is an example of a turn-by-turn plot (1 turn = 1 period = 5 meters in the above example) in which there are two frequencies involved: a faster oscillation of the beam envelope, and a slower oscillation corresponding to emittance exchange. This kind of emittance exchange is typical of linear coupling.\n\n\n\nFig. 8. Period-by-period evolution of a mismatched Danilov distribution."
  },
  {
    "objectID": "posts/2021-05-13_matched-envelope/index.html#finding-the-matched-solution",
    "href": "posts/2021-05-13_matched-envelope/index.html#finding-the-matched-solution",
    "title": "Computing matched envelopes",
    "section": "2. Finding the matched solution",
    "text": "2. Finding the matched solution\n\n2.1. The problem\nI’ll now move on to describing the problem we addressed in the paper. Notice that the focusing strength repeats itself after five meters; we call this the period length. A beam is matched to the lattice if its covariance matrix \\(\\mathbf{\\Sigma}\\) repeats itself:\n\\[\n\\mathbf{\\Sigma}(s + L) = \\Sigma(s)\n\\tag{7}\\]\nfor all \\(s\\), where \\(s\\) is the position in the lattice, \\(L\\) is the period length, and \\(\\Sigma\\) is the covariance matrix given by\n\\[\n\\mathbf{\\Sigma} = \\begin{bmatrix}\n    \\langle{x x}\\rangle & \\langle{x p_x}\\rangle & \\langle{x y}\\rangle & \\langle{x p_y}\\rangle \\\\\n    \\langle{x p_x}\\rangle & \\langle{p_x p_x}\\rangle & \\langle{y p_x}\\rangle & \\langle{p_x p_y}\\rangle \\\\\n    \\langle{x y}\\rangle & \\langle{y p_x}\\rangle & \\langle{y y}\\rangle & \\langle{y p_y}\\rangle \\\\\n    \\langle{x p_y}\\rangle & \\langle{p_x p_y}\\rangle & \\langle{y p_y}\\rangle & \\langle{p_y p_y}\\rangle \\\\\n\\end{bmatrix}\n\\tag{8}\\]\n(assuming all means are zero). So the matched beam has not only the same shape and orientation in the \\(x\\)-\\(y\\) plane, but also the same spread in velocities and correlations between the positions and velocites. Finding the matched beam amounts to choosing the correct initial \\(\\mathbf{\\Sigma}\\) such that Equation 7 is true. This task is trivial without space charge but difficult with space charge.\nThe matched envelope is useful for theoretical studies. First, it is a sort of minimum energy solution, minimizing the free energy available to drive emittance growth. Second, it is the most radially compact solution for a given beam intensity. Third, knowledge of the matched envelope is required to analyze the stability of the distribution.4 The matched envelope may also be useful in experimental studies. It appears possible to generate an approximate Danilov distribution in a real accelerator, but the method would not work without knowledge of the matched envelope.5\nOur strategy was to solve the problem in simple cases, studying the properties of the matched solution as a function of intensity. There were two challenges to overcome. First, space charge causes the final beam to depend on the initial beam in a potentially complicated way that is unknown before tracking the beam; this is especially true for long lattices and large beam intensities. This meant we would need to iterate to find the solution. Second, it was not immediately clear how to vary the distribution during the search; the full covariance matrix has ten unique elements, and they cannot be varied freely since the covariance matrix must remain positive-definite.\n\n\n2.2. The solution\nConsider the equation of motion for a particle in a coupled lattice:\n\\[\n\\begin{aligned}\n    x'' + k_{11}(s)x + k_{13}(s)y + k_{14}(s)y' &= 0, \\\\\n    y'' + k_{31}(s)x + k_{33}(s)y + k_{23}(s)x' &= 0.\n\\end{aligned}\n\\tag{9}\\]\nWhat does space charge do to these equations? For a tilted, uniform density ellipse, it simply modifies \\(k_{11}\\), \\(k_{13}\\), \\(k_{31}\\), and \\(k_{33}\\). We could replicate the effect of space charge by inserting a large number of linear defocusing elements into the lattice. Call this new lattice the effective lattice, illustrated below.\n\n\n\nFig. 9. Illustration of the effective lattice — a linear lattice that replicates the effect of space charge.\n\n\nThe matched beam generates a periodic effective lattice. Such a lattice can be parameterized using the language of coupled single-particle dyanmics. I discussed this in a previous post., but will repeat the necessary info here.\nThe following figure shows the turn-by-turn trajectory of a single particle in a coupled lattice.\n\n\n\nFig. 10. Turn-by-turn coordinates in a coupled lattice (gray). The projections of the eigenvectors are also shown in blue and red.\n\n\nThe turn-by-turn particle coordinates \\(\\{\\mathbf{x}_i\\}\\) trace a torus in 4D phase space. A matched beam is formed by placing a particle at each of these points since the particles will just trade positions after each turn (\\(\\mathbf{x}_i \\rightarrow \\mathbf{x}_{i+1}\\)).\nLet’s state this again using the eigenvectors of the one-turn transfer matrix. Each particle can be written as a linear combination of these eigenvectors (shown in blue and red):\n\\[\n\\mathbf{x} =\n\\sqrt{2 J_1}\\mathbf{v}_1e^{-i\\psi_1} +\n\\sqrt{2 J_2}\\mathbf{v}_2e^{-i\\psi_2}.\n\\]\nThe eigenvectors have amplitudes (\\(J\\)) and phases (\\(\\psi\\)), and a matched beam is formed by distributing particles along either or both of the eigenvectors with phases ranging uniformly between zero and \\(2\\pi\\). Each eigenvector traces a 4D ellipsoid turn-by-turn. When projected onto any 2D plane, each eigenvector traces an ellipse. Each eigenvector is parameterized by the 4D Twiss parameters \\(\\alpha_{1x}\\), \\(\\beta_{1x}\\), etc.\nOur insight was that all particles in the Danilov distribution lie along one vector in four-dimensional phase space. This vector is an eigenvector of some unknown \\(4 \\times 4\\) transfer matrix. This transfer matrix is just the transfer matrix of the effective lattice generated by the matched beam. Thus, we need only to vary the parameters of this eigenvector to find the matched beam. There are six eigenvector parameters in the Lebedev-Bogacz formalism: the beam size in each dimension, the beam divergence in each dimension, the phase difference between \\(x\\) and \\(y\\), and the ratio between the \\(x\\) and \\(y\\) emittances. We can also choose which eigenvector to use, so there are two possible solutions for each lattice. We can wrap all these parameters into a vector \\(\\mathbf{p}\\).\nWe can frame this as an optimization problem in which we search for the \\(\\mathbf{p}\\) which minimizes the sum of the squared differences between the initial and final moments when tracking through one period. We utilized two different optimization methods. The first was SciPy’s nonlinear least squares, which worked well in most cases. The second strategy was to track the beam over several periods and compute the period-by-period average of \\(\\mathbf{p}\\), then use this average as the seed for the next round. This figure shows the method converging after a few iterations. The relevant code is found here.\n\n\n\nFig. 11. Convergence of the matching routine."
  },
  {
    "objectID": "posts/2021-05-13_matched-envelope/index.html#simple-applications",
    "href": "posts/2021-05-13_matched-envelope/index.html#simple-applications",
    "title": "Computing matched envelopes",
    "section": "3. Simple applications",
    "text": "3. Simple applications\nWe demonstrated the matching routine in a FODO lattice (Fig. 12a). We also added some variations, like changing the horizontal/vertical focusing ratio, tilting the quadrupoles (Fig. 12b), and adding a longitudinal magnetic field to generate coupling (Fig. 12c).\n\n\n\nFig. 12. Simple lattices used for testing the matching routine.\n\n\nLet’s start with the regular, uncoupled FODO lattice. Fig. 13. Shows the matched 2D projections of the beam at the lattice entrance, as well as the evolution of the beam sizes, emittances, and x-y phase difference within the lattice.\n\n\n\n\n\n\n\n\nFig. 13. The “mode 1” (top) and “mode 2” (bottom) matched solutions in an uncoupled FODO lattice. The intensity (\\(Q\\)) is represented by the color scale. In the right columns, solid lines represent \\(x\\) and dashed lines represent \\(y\\).\n\n\nThe main takeaway is that space charge scales the matched beam; it does not change its orientation in phase space. Additional observations are: space charge causes emittance evolution within the lattice; space charge encourages the beam to be round (see the bottom-right subplot); the two solutions differ only in the sign of their angular momentum.\nWhen the quadrupoles are rotated, the matched solution is tilted in the x-y plane. The emittance exchange becomes quite large in this case.\n\n\n\n\n\n\n\n\nFig. 14. The “mode 1” (top) and “mode 2” (bottom) matched solutions in a FODO lattice with skew quadrupoles. The intensity (\\(Q\\)) is represented by the color scale. In the right columns, solid lines represent \\(x\\) and dashed lines represent \\(y\\).\n\n\nHere we noticed an asymmetry in the solutions. The beam always wants to be tilted in the same direction in the \\(x\\)-\\(y\\) plane as soon as space charge is turned on."
  },
  {
    "objectID": "posts/2021-05-13_matched-envelope/index.html#conclusion",
    "href": "posts/2021-05-13_matched-envelope/index.html#conclusion",
    "title": "Computing matched envelopes",
    "section": "4. Conclusion",
    "text": "4. Conclusion\nI’ll stop there. I hope this gave a flavor of what went into this paper. In future posts, I will discuss the importance of this matching routine for experiments.\n\n\n\nFig. 1. Comparison of radial electric field generated by a uniform distribution (left) and Gaussian distribution (right).\nFig. 2. Emittance growth in an intense beam propagating in a linear accelerator. The emittances \\(\\varepsilon_{x,y}\\) correspond to the areas in the projected phase spaces \\(x\\)-\\(p_x\\) and \\(y\\)-\\(p_y\\) (Source: [2].)\nFig. 3. Simulated tune spread in the SNS ring. (Source: [3].)\nFig. 4. 1D and 2D projections of the KV distribution.\nFig. 5. Evolution of the KV distribution in a FODO lattice. Top left: horizontal and vertical beam size. Bottom left: Lattice focusing strength. Right: beam ellipse in the \\(x\\)-\\(y\\) plane. Dashed lines are without space charge and solid lines are with space charge.\nFig. 6. 1D and 2D projections of the Danilov distribution.\nFig. 7. Evolution of the Danilov distribution in a FODO lattice. Top left: horizontal and vertical beam size. Bottom left: Lattice focusing strength. Right: beam ellipse in the \\(x\\)-\\(y\\) plane. Dashed lines are without space charge and solid lines are with space charge.\nFig. 8. Period-by-period evolution of a mismatched Danilov distribution.\nFig. 9. Illustration of the effective lattice — a linear lattice that replicates the effect of space charge.\nFig. 10. Turn-by-turn coordinates in a coupled lattice (gray). The projections of the eigenvectors are also shown in blue and red.\nFig. 11. Convergence of the matching routine.\nFig. 12. Simple lattices used for testing the matching routine.\nFig. 13. The “mode 1” (top) and “mode 2” (bottom) matched solutions in an uncoupled FODO lattice. The intensity (\\(Q\\)) is represented by the color scale. In the right columns, solid lines represent \\(x\\) and dashed lines represent \\(y\\).\nFig. 14. The “mode 1” (top) and “mode 2” (bottom) matched solutions in a FODO lattice with skew quadrupoles. The intensity (\\(Q\\)) is represented by the color scale. In the right columns, solid lines represent \\(x\\) and dashed lines represent \\(y\\)."
  },
  {
    "objectID": "posts/2021-05-13_matched-envelope/index.html#footnotes",
    "href": "posts/2021-05-13_matched-envelope/index.html#footnotes",
    "title": "Computing matched envelopes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI discuss this here and here.↩︎\nI discuss this here.↩︎\nNotice that in Fig. 1., the Gaussian has an approximately uniform core.↩︎\nThis has only been done for the KV distribution.↩︎\nI will discuss this method in future posts.↩︎"
  },
  {
    "objectID": "posts/2023-07-09_biological-design-arguments/index.html",
    "href": "posts/2023-07-09_biological-design-arguments/index.html",
    "title": "Biological design arguments",
    "section": "",
    "text": "Paley famously expressed a version of the biological design argument:\nOppy  [2] has a compact critique of Paley’s argument: Paley’s inference to design stems from three observations: (i) the watch has a principal function, (ii) various parts of the watch have functions, and (iii) the materials from which the parts are constructed are well suited to the functions that those parts have. But it appears that Paley’s inference to design relies instead on the background knowledge that there are no watch factories in nature, so his inference is unlikely to work for biological systems. The argument never gets off the ground.\nAn extension of the biological design argument focuses on “irreducibly complex” systems that would cease functioning if any of their parts were removed  [3]. Behe claims that it would be impossible for any irreducibly complex system to arise from gradual changes since the predecessor of any irreducibly complex system would have to be irreducibly complex. It seems that the proponent of this argument would want to identify the first irreducibly complex system with the first biological system. A broader definition would include non-biological systems whose complexity is explained by theories of physics and chemistry. A narrower definition would exclude some non-initial biological life forms, but evolutionary theory provides a plausible link between these relatively simple life forms and more complex life forms, such as humans. But I see no reason why there could be no natural link between chemical and biological systems.\nAnother problem: if the designer creates a self-organizing world, why stop the self-organization at the origin of life? It would be like designing a computer program that stops halfway through and demands the user hit the space bar before continuing.\nIn conclusion, biological design arguments suffer from various problems; most obviously, they are undercut by modern scientific theories. There are major gaps to fill in these theories, but it is plausible that the laws of physics, chemistry, and biology are all that are needed to connect the early universe to the present universe. This type of explanation is not complete, however. Note that we posit physical laws which link prior states of the world to the present state. All prior states are explained by recursion: the state at time \\(t\\) is explained by the laws and the state at time \\(t’\\), where \\(t’ &lt; t\\); however, we will eventually arrive at either an initial state (\\(t = 0\\)) or an infinite regress of prior states (\\(t \\rightarrow -\\infty\\)). It is difficult to know what else to do if there is an initial state since our explanatory mechanism (laws + prior states) no longer works. It might be troubling if the initial state had no explanation, for then all subsequent states would lack an ultimate explanation. It is unclear whether an infinite explanatory regress solves this problem.1 Furthermore, one might ask for an explanation of the physical laws. Deeper considerations such as these are central to cosmic fine-tuning arguments and nomological arguments."
  },
  {
    "objectID": "posts/2023-07-09_biological-design-arguments/index.html#footnotes",
    "href": "posts/2023-07-09_biological-design-arguments/index.html#footnotes",
    "title": "Biological design arguments",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThese considerations demonstrate a relationship between cosmological and design arguments.↩︎"
  },
  {
    "objectID": "posts/2022-03-22_leibnizian-cosmological-arguments/index.html",
    "href": "posts/2022-03-22_leibnizian-cosmological-arguments/index.html",
    "title": "Leibnizian cosmological arguments",
    "section": "",
    "text": "Leibnizian cosmological arguments conclude that there is a necessary being (something that must exist) which explains the existence of contingent beings (things that do not have to exist). Such arguments address the question “Why does anything exist?” and generally utilize a principle of sufficient reason (PSR): every contingent fact has an explanation. Some recent formulations use a weaker explanatory principle: every contingent concrete being has an explanation, where concrete means “possibly causes something”, or even a modal version: every contingent concrete being possibly has an explanation."
  },
  {
    "objectID": "posts/2022-03-22_leibnizian-cosmological-arguments/index.html#the-principle-of-sufficient-reason-psr",
    "href": "posts/2022-03-22_leibnizian-cosmological-arguments/index.html#the-principle-of-sufficient-reason-psr",
    "title": "Leibnizian cosmological arguments",
    "section": "1. The Principle of Sufficient Reason (PSR)",
    "text": "1. The Principle of Sufficient Reason (PSR)\nThe PSR can be used to run an argument for the existence of a necessary being. Here is the basic form of such an argument  [1]:\n\nA contingent being (a being such that if it exists, it could have not-existed) exists.\nAll contingent beings have a sufficient cause of or fully adequate explanation for their existence.\nThe sufficient cause of or fully adequate explanation for the existence of contingent beings is something other than the contingent being itself.\nThe sufficient cause of or fully adequate explanation for the existence of contingent beings must either be solely other contingent beings or include a non-contingent (necessary) being.\nContingent beings alone cannot provide a sufficient cause of or fully adequate explanation for the existence of contingent beings.\nTherefore, what sufficiently causes or fully adequately explains the existence of contingent beings must include a non-contingent (necessary) being.\nTherefore, a necessary being (a being such that if it exists, it cannot not-exist) exists.\nThe universe, which is composed of only contingent beings, is contingent.\nTherefore, the necessary being is something other than the universe.\n\nPremise 2 is the PSR. Oppy suggests that the PSR, as it is usually formulated, is unacceptably strong and that defensible versions of the PSR are “so weak that it is implausible to suppose that they possess serious metaphysical bite”  [2].1 In this section, I will discuss several points raised by Pruss in  [3] in defense of the PSR.\n\n1.1. Support for the PSR\nThe PSR is a kind of rock-bottom principle that is difficult to arrive at from other principles; thus, one option is to accept the PSR in the absence of any cogent counterarguments. One argument for the PSR is that the denial of the PSR leads to extreme skepticism: if the PSR is false, then my sensory inputs could exist for no reason, meaning that I do not have any knowledge  [4]. And since to assign a probability is to explain a frequency in terms of an underlying regularity, no probability can be assigned to this scenario  [3]. We might go further and claim that the PSR is assumed whenever we scientifically inquire. Although there is debate about the fundamentality of causation in physics, it seems to me that physics is at least in search of explanations. If the PSR is false, then the following “explanation” is available for any fact: there is no explanation. Again, one might argue that there is no meaningful probability that can be assigned to this “explanation”.\n\n\n1.2. Objections to the PSR\nSurely, those who reject the PSR are doing so because they are skeptical of applying it to every contingent fact; if there is just one unexplained contingent fact, then the PSR is false. Therefore, one way to object to the PSR is to identify an unexplained, or potentially unexplained, fact.\n\n1.2.1. Imagination\nHume suggested that the PSR is false because one can imagine an exception to it. This objection is not so interesting to me, so I will move on.\n\n\n1.2.2. Chance\nQuantum mechanics leaves open the possibility of chancy — i.e., random — events  [5].2 If an event is chancy, then it seems that there is no explanation for it. This would violate the PSR.\nHowever, we might think that quantum events are not unexplained even if they are chancy: the laws of quantum mechanics + the system indeterministically produce — and thus explain — the measured state. This may be a less-than-ideal explanation since only patterns in groups of measurements are explained instead of individual measurements, but it is an explanation nonetheless; we are providing a background or framework on which the measurement is not surprising. (In  [6], Alex Malpass notes that, in his opinion, it is an open question whether such an explanation is relevant to cosmological arguments.)\n\n\n1.2.3. Free will\nHere is Oppy’s definition of libertarian free will: “If an agent \\(X\\) acts freely in performing action \\(A\\) in circumstances \\(C\\) at time \\(T\\) in world \\(W\\), then it is not made true by the truth-making core of the world \\(W\\) prior to \\(T\\) that agent \\(X\\) will do \\(A\\) in circumstances \\(C\\).” In other words, there is a possible world that shares the same history as the actual world, but in which the agent acted differently. The PSR is false if there is no explanation for why one world was actualized instead of the other world.\nOne solution to this problem is to reject the libertarian conception of free will.3 Another solution is to provide an explanation of libertarian free choices. We will have to resort to some sort of non-deterministic explanation, as in the previous section. Pruss presents a hypothesis in  [3] that I have reorganized/paraphrased below. (The terms in brackets can be exchanged with the preceding unbracketed terms in each line.)\n\nHypothesis: Free choices are made based on reasons that one is “impressed by”, i.e., that one takes into consideration in making the decision.\nSuppose agent \\(X\\) has a binary choice between \\(A\\) and \\(B\\).\nLet \\(S\\{T\\}\\) be a subset of the reasons that favor \\(A\\{B\\}\\) over \\(B\\{A\\}\\).\nIf \\(X\\) freely chooses \\(A\\{B\\}\\), it is because \\(X\\) is making a free choice between \\(A\\) and \\(B\\) while impressed by the reasons in \\(S\\{T\\}\\); \\(X\\) is also impressed by \\(T\\{S\\}\\), but only acts on the impressive reasons in \\(S\\{T\\}\\).4\n\nMy initial thought is that it seems plausible for libertarian free choices to be explained in this way or a similar way. The strategy seems analogous to the previous attempt to explain chancy events.\n\n\n1.2.4. Modal fatalism\nSuppose that \\(C\\) is the conjunction of every contingent proposition and that \\(E\\) explains \\(C\\). \\(E\\) cannot be contingent because \\(C\\) would then contain \\(E\\) and \\(E\\) would explain itself, which is impossible; \\(E\\) cannot be necessary because a necessary proposition cannot explain a contingent proposition; \\(E\\) cannot exist.\nThis objection points out the difficulty in forming an explanatory link between the necessary and the contingent. Pruss pushes back in a few ways. First, in this objection, we have assumed that if \\(p\\) is necessary and \\(p\\) explains \\(q\\), then \\(p\\) entails \\(q\\). He suggests that there are counterexamples, such as statistical/non-deterministic explanations, and that our normal concept of explanation does not involve entailment. Second, he argues that the PSR survives if libertarian free will is possible and if God — a necessary being — freely chose to instantiate \\(C\\). The existence of God would then explain \\(C\\) without entailing \\(C\\).\nThere are several difficulties with this approach. One difficulty is that libertarian free will may be impossible. Another difficulty is that it may be the case that God’s choices are not contingent. Most theists maintain that God chose the actual world out of the possible worlds. For example, since God is morally perfect, it seems that God would not instantiate a world in which only evil occurred. In fact, it seems that God would instantiate the best possible world (I think there is debate about whether there is such a thing). The question is whether God could have chosen differently. If God could not have chosen differently, then maybe God’s choices are necessary, and therefore \\(C\\) is necessary. This would render the PSR false. I suspect this is just an attack on the idea of libertarian free will and that the theist can probably escape without harm.\n\n\n1.2.5. Infinite chains\nWe’re now going to switch gears and focus on infinite causal/explanatory chains.5 Each element in the chain is explained by its predecessor — this is an internal explanation — but the question arises whether some external explanation is needed for the chain.6\nI’ve found that my intuitions change depending on whether the chain extends infinitely into the past. Let’s start with a chain that extends finitely into the past. Consider a particle \\(p_0\\) that comes into existence at time \\(t = 1\\) for no reason. Maybe we take issue with this. Now consider a particle \\(p_0\\) that exists at time \\(t = 1\\) and was created in the following way: for every integer \\(n \\ge 0\\), particle \\(p_{n + 1}\\) decayed into particle \\(p_{n}\\) at \\(t = 2^{-n}\\). Thus, there are an infinite number of causes within a finite time interval. Although the existence of each particle is explained by its parent, it will seem to many that the result is the same: the particle came into existence uncaused. Therefore, Pruss says, if we demand a cause for one particle, we should demand a cause for an infinite series of particles.\nHe then argues that our same intuitions should apply if the infinite number of causes are not squeezed into a finite time interval but are instead spread out over an infinite past. In other words, each cause in the interval (0, 1] is mapped to a negative time while preserving the ordering: \\(1 \\rightarrow 1\\), \\(1/2 \\rightarrow 0\\), \\(1/4 \\rightarrow -1\\), \\(1/8 \\rightarrow -2\\), \\(\\dots\\). It appears that nothing has changed, so we should either accept or reject the possibility of both scenarios.\nHowever, it is difficult to imagine what an external cause — a reason why the particles are not different particles, or shoes, or books, or why they exist at all — would look like since there is no first time. (The same can be said of the (0, 1] scenario if we assume that time began so that there is no time \\(t \\le 0\\).) We must abandon the idea that this external cause explains only the first member of the chain (since there is no first member) and that the cause is temporally prior to its effect (since there is no first time). Pruss writes:\n\nKant’s example of a metal ball continually causing a depression in a soft material shows that simultaneous causation is conceivable. And apart from full or partial reductions of the notion of causation to something like Humean regularity and temporal precedence, I do not think there is much reason to suppose that the cause of a temporal effect must even be in time.  [3]\n\n(I am not sure exactly what is meant by the last line.) Thus, the cause would have to, in some way, “support” the chain such that if the cause did not exist, the chain would not exist. This is confusing and deserves more thought."
  },
  {
    "objectID": "posts/2022-03-22_leibnizian-cosmological-arguments/index.html#weaker-explanatory-principles",
    "href": "posts/2022-03-22_leibnizian-cosmological-arguments/index.html#weaker-explanatory-principles",
    "title": "Leibnizian cosmological arguments",
    "section": "2. Weaker explanatory principles",
    "text": "2. Weaker explanatory principles\nPruss & Rasmussen utilize a more modest explanatory principle to argue for the existence of a necessary being in their book Necessary Existence. They start with the traditional argument from contingency  [7].\n\nFor any contingent concrete things, there is an explanation of the fact that those things exist.\nConsidering all the contingent concrete things that exist, if there is an explanation of the fact that those things exist, then there is a necessary concrete thing.\n(Therefore) There is a necessary concrete being.\n\nHere, “concrete” means “possibly causes something”, i.e., not an abstract object like the number two. This explanatory principle is not as strong as the PSR because it is restricted to facts about existence, so it is not affected by, say, the possibility of unexplained free actions.7\nThe authors note a few strengths of this argument: it is defensible against traditional objections from Hume  [8] and Kant  [9], it is adaptable to a variety of metaphysical frameworks, and the basic reasoning behind the argument is simple and intuitive  [7]. The authors also note a few weaknesses of the argument: it does not allow explanatory loops, it does not allow completely internal explanations, and it does not allow any exceptions to the explanatory principle. This last weakness is the most concerning due to the possibility of chancy events. Although indeterministic/statistical explanations are on the table, some people may not think that these are adequate explanations and will insist that, if there are chancy events, chancy events are unexplained.\nTo overcome these weaknesses, the authors introduce several modal arguments. Recall from the discussion of ontological arguments that it is mostly accepted that “possibly necessary” is equivalent to “necessary”. Thus, if it can be shown that a necessary being possibly exists, then it follows that a necessary being exists. We will focus only on the first argument in the book, which Leon  [10] rewrites as:\n\nNormally, things that can begin to exist can have a cause of the beginning of their existence.\nContingent concrete reality can begin to exist.\nTherefore, there can be a cause of the beginning of contingent concrete reality’s existence.\nIf there can be a cause of the beginning of contingent concrete reality’s existence, then a necessary being exists.\nTherefore, a necessary being exists.\n\nThe primary advantage of this argument is the weakening of the explanatory principle (first premise). This premise is more difficult to reject. It allows for exceptions such as uncaused contingent beings, explanatory loops, and internal explanations. As Leon notes, this allows the principle to be used as a defeasible rule of thumb. Referring to Koons  [11], Leon writes “To avoid the demands of a well-supported defeasible principle, one must give principled grounds for thinking that it admits of an exception in the particular case at stake.”  [10]. (There is debate about whether the PSR should be used as a defeasible rule of thumb — see  [12–14].)\nThe second premise is more controversial. In support of this premise, Rasmussen & Pruss suggest three approaches. First, one might argue that contingent concrete reality began to exist; this is the task of the Kalam argument. Second, one might argue only that it is plausible that contingent concrete reality began to exist; for example, by finding a viable cosmological model in which time begins or by showing that causal finitism is possibly true. Third, one might argue that it is conceivable that contingent concrete reality began to exist, which provides defeasible evidence of possibility.\nThe primary critique of the second premise is that it is false if origin essentialism is true. On origin essentialism, if we look at our universe, the possible worlds shrink: in our universe, it is either true in all possible worlds, or false in all possible worlds, that contingent concrete reality began to exist. Thus, the word “can” should be removed from the second premise. The authors give several responses to this objection. The first response is a short argument against origin essentialism. The second response is a possible workaround, maintaining origin essentialism. Since origin essentialism is a new concept to me, I will have to return to this discussion some other time.\nThe fourth premise follows from possibly necessary \\(\\rightarrow\\) necessary. Leon offers an alternative to a necessary being; he raises the possibility of factually necessary beings:\n\nPerhaps matter-energy (or whatever matter-energy is ultimately composed) is a factually necessary being. According to such a scenario, the contingent dependent beings (e.g., rocks, trees, planets, you and I, etc.) come into being when two or more contingent independent beings (i.e., factually necessary beings) are combined, and the contingent dependent beings cease to exist when they decompose into their elements. However, the fundamental elements of which contingent dependent beings are composed (i.e., the contingent independent beings/factually necessary beings) cannot pass away, for they are at least de facto indestructible—i.e., nothing in the actual world has what it takes to knock them out of existence. Nor can they be created, for they are eternal, existentially independent, and (assuming origin essentialism and their being uncaused at the actual world) essentially uncaused.  [10]\n\nI am not sure of the reason for this distinction. Why not just say that matter-energy is necessary?\nAnother worry for any of these modal arguments is the existence of parody arguments.8 Oppy constructs a parody argument for every argument in Necessary Existence in his review of the book  [15]."
  },
  {
    "objectID": "posts/2022-03-22_leibnizian-cosmological-arguments/index.html#conclusion",
    "href": "posts/2022-03-22_leibnizian-cosmological-arguments/index.html#conclusion",
    "title": "Leibnizian cosmological arguments",
    "section": "3. Conclusion",
    "text": "3. Conclusion\nOppy has a nice summary of the stances one can take about the foundations/origins of the universe:\n\nSome philosophers suppose that every possible world shares laws and initial causal history with the actual world. Those philosophers divide into two camps: those who suppose that there is just one possible world; and those who suppose that there are many possible worlds. Philosophers in the first camp suppose that causal laws are deterministic; philosophers in the second camp suppose that causal laws are not deterministic. One thing that these philosophers have in common is that they suppose that, if there is an initial causal state, then that initial causal state, and anything that exists in that initial causal state, is necessary: if these philosophers are naturalists, then they suppose that the initial natural state is necessary; if these philosophers are theists, then they suppose that the initial divine state is necessary. Of course, other philosophers suppose that, if there is an initial causal state, then that initial causal state is contingent; and these philosophers divide further on the question of whether there is anything that exists in the initial state that is necessary. It is to be expected that what philosophers have to say about the kinds of arguments that Pruss and Rasmussen discuss is determined by their background views about modality, causation, ontology, epistemology, axiology, and so forth.  [15]\n\nIntuitions clash. For example, physicist Sean Carroll says, “I think that brute facts are things we need to accept; the universe is probably one of them.”  [16]. Perhaps all these options are equally strange, but the idea that the only explanation of the universe is brute contingency seems the most strange to me.\nNon-theists can claim that there is a concrete necessary being (or beings); possible candidates are the universe, fields, particles, etc. In this sense, theists and non-theists can agree on some aspects of the fundamental structure of reality. Theists have an additional belief: that there is only one concrete necessary being that has properties consistent with those traditionally ascribed to God. The question thus arises: is there any way to choose between these views? This question needs to be treated on its own, separate from the cosmological argument."
  },
  {
    "objectID": "posts/2022-03-22_leibnizian-cosmological-arguments/index.html#footnotes",
    "href": "posts/2022-03-22_leibnizian-cosmological-arguments/index.html#footnotes",
    "title": "Leibnizian cosmological arguments",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe are referred to Oppy’s book Philosophical Perspectives on Infinity for a detailed treatment of the PSR. Many of Oppy’s thoughts on infinity are also apparently found in Philosophical Perspectives on Infinity. I should have read this book before Arguing About Gods.↩︎\nSome interpretations of quantum mechanics are deterministic (such as the pilot wave theory) and others are indeterministic. The correct interpretation of quantum mechanics is unknown. (See Tim Maudlin here).↩︎\nMany of those that do not accept the PSR also do not accept the libertarian account of free will, in which case this objection is irrelevant. Although I haven’t studied free will in any detail, I am open to the idea that my choices are not determined by the initial conditions of the universe. It is, of course, a difficult question.↩︎\nMy wife, who is not familiar with (or very interested in) this discussion, came up with essentially the same answer when I presented the dilemma to her.↩︎\nThe most famous explanatory regress is the following explanation for why the Earth does not fall: it sits on the back of a turtle, which sits on the back of a turtle, which sits on the back of a turtle, and so on ad infinitum. It remains unexplained why the entire stack of turtles does not fall, why the stack is comprised of turtles instead of giraffes, etc. Here is another example involving turtles: I see the reflection of a turtle in a mirror; upon turning away from the mirror to view the turtle directly, I see that the light came from another mirror — a reflection of a reflection — and so on ad infinitum.↩︎\nOne way to rule out infinite chains is to accept finitism (the denial of the existence of actual infinities in the real world) or causal finitism (the denial of the existence of infinite causal chains). I like causal finitism, but for now, we are assuming that both finitism and causal finitism are false.↩︎\nAlso note that this is closer to a modal version of the Kalam argument since we are dealing with causes and things beginning to exist.↩︎\nRecall the ontological argument (It is possible that a necessary being exists; therefore, a necessary being exists.) and its parody (It is possible that a necessary being does not exist; therefore, a necessary being does not exist.). The parody argument is taken to show that the modal ontological argument is unsound. We may as well replace the argument with its conclusion: a necessary being exists.↩︎"
  },
  {
    "objectID": "posts/2024-11-10_pinker-sense-of-style/index.html",
    "href": "posts/2024-11-10_pinker-sense-of-style/index.html",
    "title": "Notes on The Sense of Style by Steven Pinker",
    "section": "",
    "text": "So many things can go wrong in a passage of prose. The writing can be bloated, self-conscious, academic; these are habits that classic style, which treats prose as a window onto the world, is designed to break. The passage can be cryptic, abstruse, arcane; these are symptoms of the curse of knowledge. The syntax can be defective, convoluted, ambiguous; these are flaws that can be prevented by an awareness of the treelike nature of a sentence. […] Even if every sentence in a text is crisp, lucid, and well formed, a succession of them can feel choppy, disjointed, unfocused—in a word, incoherent.\nI recently read The Sense of Style by Steven Pinker. It was great! I’ve struggled with my writing since starting my PhD six years ago. This book gave me plenty to keep in mind when writing my next paper."
  },
  {
    "objectID": "posts/2024-11-10_pinker-sense-of-style/index.html#a-window-onto-the-world",
    "href": "posts/2024-11-10_pinker-sense-of-style/index.html#a-window-onto-the-world",
    "title": "Notes on The Sense of Style by Steven Pinker",
    "section": "A window onto the world",
    "text": "A window onto the world\nPinker advocates for classic style, in which:\n\n“A writer, in conversation with a reader, directs the reader’s gaze to something in the world.”\n“The writer can see something that the reader has not yet noticed, and he orients the reader’s gaze so that she can see it for herself. The purpose of writing is presentation, and its motive is disinterested truth.”\nThe writer can count on the reader to “read between the lines, catch his drift, and connect the dots”.\nThe writer puts aside philosophical questions about the subject.\n\nClassic style is clear, focused, and direct. Basically, classic style respects the reader. This is in stark contrast to the “academese” found in many academic papers, such as:\n\nThe move from a structuralist account in which capital is understood to structure social relations in relatively homologous ways to a view of hegemony in which power relations are subject to repetition, convergence, and rearticulation brought the question of temporality into the thinking of structure, and marked a shift from a form of Althusserian theory that takes structural totalities as theoretical objects too one in which the insights into the contingent possibility of structure inaugurate a renewed conception of hegemony as bound up with the contingent sites and strategies of the rearticulation of power.\n\nPinker provides several recommendations to move one’s writing toward classic style. For example:\nUse signposts sparingly. Common signposts include “This paper is organized as follows” and “The previous paragraph explained”. Pinker argues that while these signposts can be useful in oral presentations, they are unnecessary in written works. I agree! I skip these paragraphs when I read scientific papers. If the paper is so long that it needs an outline, then a proper table of contents is helpful. Pinker suggests using signposts sparingly. Examples include asking a question (“What drives halo formation?”) or using “we” to signify that the reader and writer are on a journey together (“As we have seen”, “Let us begin by”, “Now that we have”).\nSpeak directly about the topic. Avoid statements like “In recent years, there has been an explosion of research on…”. Why should the reader care what researchers are doing? It’s better to just explain why the topic is interesting or important. Asking a question is often a nice way to begin.\nAvoid self-conscious writing. Here’s a hilarious example from the book:\n\nThe problem of language acquisition is extremely complex. It is difficult to give precise definitions of the concept of “language” and the concept of “acquisition and the concept of”children”. There is much uncertainty about the interpretation of experimental data and a great deal of controversy surrounding the theories. More research needs to be done.\n\nAvoid quotation marks around common expressions. This is again very common in academic papers.\nAvoid excessive hedging. This is difficult for me because I don’t want to say anything wrong and I don’t like when people are overly confident in academic papers. But Pinker convinced me that it’s easy to swing in the other direction, with too many words like fairly, somewhat, etc.\nAvoid cliches.\nAvoid zombie nouns."
  },
  {
    "objectID": "posts/2024-11-10_pinker-sense-of-style/index.html#the-curse-of-knowledge",
    "href": "posts/2024-11-10_pinker-sense-of-style/index.html#the-curse-of-knowledge",
    "title": "Notes on The Sense of Style by Steven Pinker",
    "section": "The curse of knowledge",
    "text": "The curse of knowledge\n\nWe would rather run the risk of confusing them while at least appearing to be sophisticated than take a chance at belaboring the obvious while striking them as naive or condescending.\n\nThe curse of knowledge is “the difficulty in imagining what it is like for someone else not to know something that you know”. Pinker argues that the curse of knowledge is the best explanation for why there is so much unintentionally bad writing. This one hit home.\nI like Pinker’s explanation in terms of “chunking”. Over time, we develop abstract “chunks”: single words that describe complex ideas. Then we package these chunks into higher-level chunks, and so on. By the end, we’re speaking a foreign language and dealing with very abstract concepts, creating an enormous barrier to entry for new researchers. It can even make our work inaccessible to researchers in the same field of study!\nUnfortunately, it’s not easy to lift the curse. First, there’s pressure to make oneself sound smart in academic papers. I’ve been told to remove sentences because they are too obvious, advised to remove explanations and diagrams, etc.; this feedback begins soon after you begin your PhD and continues until you can comfortably use the same confusing jargon as your peers. Second, it’s just hard to imagine not knowing what you know.\nTo lift the curse of knowledge, Pinker recommends picturing the audience as you write. This strategy is imperfect because we naturally overestimate the audience’s background knowledge. Thus, Pinker recommends reading your paper after an extended break—long enough to forget what you wrote. Finally, it’s wise to let someone else read your paper and to take their feedback seriously."
  },
  {
    "objectID": "posts/2024-11-10_pinker-sense-of-style/index.html#the-web-the-tree-and-the-string",
    "href": "posts/2024-11-10_pinker-sense-of-style/index.html#the-web-the-tree-and-the-string",
    "title": "Notes on The Sense of Style by Steven Pinker",
    "section": "The Web, the Tree, and the String",
    "text": "The Web, the Tree, and the String\nThis fourth chapter contains a detailed explanation of sentence structure and how it can clarify one’s message. Lots to reread here. The main point: writing is using a tree of phrases to turn a web of ideas into a string of words."
  },
  {
    "objectID": "posts/2024-11-10_pinker-sense-of-style/index.html#arcs-of-coherence",
    "href": "posts/2024-11-10_pinker-sense-of-style/index.html#arcs-of-coherence",
    "title": "Notes on The Sense of Style by Steven Pinker",
    "section": "Arcs of coherence",
    "text": "Arcs of coherence\n\nThere is a big difference between a coherent passage of writing and a flaunting of one’s erudition, a running journal of one’s thoughts, or a published version of one’s notes. A coherent text is a designed object: an ordered tree of sections within sections, crisscrossed by arcs that track topics, points, actors, and themes, and held together by connectors that tie one proposition to the next. Like other designed objects, it comes about not by accident but by drafting a blueprint, attending to details, and maintaining a sense of harmony and balance.\n\nCoherence is probably the weakest aspect of my writing. I suppose my problem isn’t with the high-level structure (the ordered tree), but with the connections between sentences. Pinker writes, “Whenever one sentence comes after another, readers need to see a connection between them.” I find myself wanting to write a series of disjointed sentences, all of which support one main point but which sound choppy when read one after the other.\nPinker has quite a few suggestions to improve coherence, including stating the topic up front to focus the reader’s attention, using paragraphs to give the reader breaks, using the right number of connectives (too many = pedantic; too few = incoherent), minimizing negations, and avoiding unnecessary word variation."
  },
  {
    "objectID": "posts/2024-11-10_pinker-sense-of-style/index.html#telling-right-from-wrong",
    "href": "posts/2024-11-10_pinker-sense-of-style/index.html#telling-right-from-wrong",
    "title": "Notes on The Sense of Style by Steven Pinker",
    "section": "Telling right from wrong",
    "text": "Telling right from wrong\n\nThere is a kind of writer who makes issues of usage impossible to ignore. These writers are incurious about the logic and history of the English language and the ways in which it has been used by its exemplary stylists. They have a tin ear for its nuances of meaning and emphasis. Too lazy to crack open a dictionary, they are led by gut feeling and intuition rather than attention to careful scholarship. For these writers, language is not a vehicle for clarity and grace but a way to signal their membership in a social clique. (page 188)\n\nThis last chapter discusses grammar rules. Pinker argues that there is no contradiction between descriptive and prescriptive linguistics (descriptive rules summarize how people use language, while prescriptive rules are conventions). Language is flexible and ever-changing, but conventions exist for a reason.\nThe rest of the chapter takes stances on many different grammar questions, such as who/whom, shall/will, and split infinitives, as well as punctuation rules, formality, diction, etc. A great reference to keep nearby."
  },
  {
    "objectID": "posts/2024-11-10_pinker-sense-of-style/index.html#next-steps",
    "href": "posts/2024-11-10_pinker-sense-of-style/index.html#next-steps",
    "title": "Notes on The Sense of Style by Steven Pinker",
    "section": "Next steps",
    "text": "Next steps\nIt’s discouraging to realize how many things can go wrong in my writing, and it’s hard to know how to improve. If I write more frequently, I could improve, but I could also get worse. It’s like lifting weights, playing piano, or any other motor skill: practice reinforces technique. Improvement requires practicing good technique.\nIt’s impossible to improve all these things at once. I guess the solution is to focus on a few things each time I write. (I recently listened to a podcast with NFL running back Marshawn Lynch, who said he used to focus on a single aspect of his technique at each practice.) Or maybe it’s better to write freely and then focus on one thing at a time during the revision process. I’m not sure.\nMy next step will probably be to distill Pinker’s advice into a few bullet points and keep these by my computer when I write. I also plan to write my outlines on paper before typing anything. Finally, Pinker (and every other writer) recommends reading a lot of books. I’m a slow reader, but I’ve increased my reading time in the last year and hope to increase a bit more next year."
  },
  {
    "objectID": "posts/2024-11-13_keynote-break-apart/index.html",
    "href": "posts/2024-11-13_keynote-break-apart/index.html",
    "title": "Break apart images in Keynote",
    "section": "",
    "text": "In Keynote, you can break png images into parts:"
  },
  {
    "objectID": "posts/2021-04-29_authorship-identification/index.html",
    "href": "posts/2021-04-29_authorship-identification/index.html",
    "title": "Authorship identification",
    "section": "",
    "text": "In this post, I’ll summarize a paper by John Houvardas and Efstathios Stamatatos titled N-Gram Feature Selection for Authorship Identification [1]. The topic of the paper is authorship identification, that is, to identify the author of an unlabeled document given a list of possible authors and some sample of each author’s writing. I’ll first motivate the problem of authorship identification, then briefly introduce the relevant statistical methods, and finally, summarize and implement the methods in the paper."
  },
  {
    "objectID": "posts/2021-04-29_authorship-identification/index.html#a-brief-history-of-stylometry",
    "href": "posts/2021-04-29_authorship-identification/index.html#a-brief-history-of-stylometry",
    "title": "Authorship identification",
    "section": "1. A brief history of stylometry",
    "text": "1. A brief history of stylometry\nThe Federalist Papers are an important collection of 85 essays written by Hamilton, Madison, and Jay in 1787 and 1788. The essays were published under the alias “Plubious”, and although it became well-known that the three men were involved, the authorship of each paper was kept hidden for over a decade. This was actually in the interest of both Hamilton and Madison; both were politicians who had changed positions on several issues and didn’t want their political opponents to use their own words against them. Days before his death, Hamilton allegedly wrote down who he believed to be the correct author of each essay, claiming over 60 for himself. Madison waited several years before publishing his own list, and in the end, there were 12 essays claimed by both Madison and Hamilton. Details on the controversy are in [2].\n\n\n\nFig. 2. Alexander Hamilton (left) and James Madison (right). (Source: Wikipedia.)\n\n\nThere are a few ways one might go about resolving this dispute. One approach is to analyze the actual content of the text. For example, perhaps an essay draws from a reference with which only Madison was intimately familiar, or maybe an essay is similar to Hamilton’s previous work. This approach was used many times over the next 150 years, but perhaps the final word on the subject was by Adair, who in 1944 concluded that Madison likely wrote all 12 essays.\nAn alternative approach is to analyze the style of the text. For example, maybe Madison used many more commas than Hamilton. The field of stylometry attempts to statistically quantify these stylistic differences. David Holmes writes the following about stylometry [3]:\n\nAt its heart lies an assumption that authors have an unconscious aspect to their style, an aspect which cannot consciously be manipulated but which possesses features which are quantifiable and which may be distinctive.\n\nI think this is a valid assumption. The question is which features best characterize the author’s style and which methods are best to use in the analysis of these features. Let’s go back in time a bit to see how stylometry has developed over the past 150 years.\nThe physicist Thomas Mendenhall is considered the first to statistically analyze large literary texts. He presented the following idea in an 1887 paper titled The Characteristic Curves of Composition [4]: it is known that each chemical element emits light with a unique distribution of wavelengths when it is heated; perhaps each author has a unique distribution of word lengths in the texts they have written. It’s a cool idea, and I recommend reading his original paper. Mendenhall tallied word lengths by hand for various books, usually in batches of 1000 words or so. Here is Fig. 2 from his paper which shows the characteristic curves for a few excerpts of Oliver Twist.\n\n\n\nFig. 3. Distribution of word lengths in “Oliver Twist”. Each curve is for a different sample of 1000 words. (Source: [4].\n\n\nMendenhall showed that these curves reveal similarities between different works by the same author. The use of these statistics for authorship identification was left for future work.\nThe next significant advance in the statistical analysis of text was made by Zipf in 1932. Zipf found a relationship between an integer \\(k\\) and the frequency \\(f(k)\\) of the \\(k\\)th most frequent word. This is often called a rank-frequency relationship, where \\(k\\) is the rank. The scaling law can be written as\n\\[\nf(k) \\propto k^{-1}.\n\\tag{1}\\]\nThe idea expressed by this law is that short words are much more frequent than large words. Surprisingly, the law holds up very well, albeit not perfectly, for most texts. Why this is the case is still unknown; a comprehensive review of the current state of the law can be found in [5]. The law also shows up in other situations such as national GDP:\n\n\n\nFig. 4. National GDPs appear to be moving toward the prediction by Zipf’s Law (red line). (Source: [6].)\n\n\nThe success of Zipf’s Law was very encouraging and led to a flurry of new mathematical models. Stylometry reached a landmark case in the 1960s when researchers used the frequency distributions of short function words — words we don’t think about too much like “upon” or “therefore” — to support Adair’s conclusion that Madison wrote the 12 disputed Federalist Papers. At the end of the day, however, models created in the spirit of Zipf’s Law are probably doomed to fail. The “true” underlying model must be very complex due to its dependence on human psychology. There are now many algorithms available which instead build predictive models directly from data, and these can be readily applied to the problem of authorship identification. Here we focus on the use of the Support Vector Machine (SVM)."
  },
  {
    "objectID": "posts/2021-04-29_authorship-identification/index.html#support-vector-machine-svm",
    "href": "posts/2021-04-29_authorship-identification/index.html#support-vector-machine-svm",
    "title": "Authorship identification",
    "section": "2. Support Vector Machine (SVM)",
    "text": "2. Support Vector Machine (SVM)\nI include here the basic idea behind the SVM approach. There are a number of resources that go into the details (such as [7]). I’ll follow the Wikipedia page since it has a nice short summary.\n\n2.1. Maximum margin hyperplane\nConsider a linear binary classifier, i.e., a plane that splits the data into two classes. The equation for a plane in any number of dimensions is\n\\[\ny(\\mathbf{x}) = \\mathbf{w}^T\\mathbf{x} + w_0 = 0\n\\tag{2}\\]\nThis plane is called the decision surface; points are assigned to class 1 if \\(y(\\mathbf{x}) &gt; 0\\) or class 2 when \\(y(\\mathbf{x}) &lt; 0\\). Suppose the data is linearly separable (able to be completely split in two) and that we’ve found a plane that correctly splits the data. We could then scale the coordinates such that all points with \\(y(\\mathbf{x}) \\ge 1\\) belong to class 1 and all points with \\(y(\\mathbf{x}) \\le -1\\) belong to class 2. The separating plane then sits in the middle as in the following figure.\n\n\n\n\nFig. 5. Maximum margin separating plane. (Source: Wikipedia.)\n\n\n\nNotice that the plane could be rotated while still correctly splitting the existing data; the SVM attempts to find the optimal plane by maximizing the orthogonal distance from the decision plane to the closest point. This is known as the margin, and it can be shown that it is inversely proportional to the magnitude of \\(\\mathbf{w}\\). Thus, the SVM tries to minimize \\(|\\mathbf{w}|^2\\) subject to the constraint that all points are correctly categorized. New data is then assigned based on this optimal boundary.\nSome datasets won’t be linearly separable, in which case we can add a penalty function in order to minimize the number of miscategorized points. So, for N samples we minimize\n\\[\n\\frac{1}{2}|\\mathbf{w}|^2 +  C\\sum_{i=1}^{N}{\\max\\left[0, 1 - {t_i y(\\mathbf{x}_i)}\\right]}.\n\\tag{3}\\]\nwhere \\(t_i\\) is the true class of point \\(i\\) (\\(\\pm 1\\)) and \\(C\\) is a positive constant. Correctly classified points don’t contribute anything to the sum since \\(t_i y(\\mathbf{x}_i)\\) will be greater than or equal to one. Let’s try this on non-linearly separable data sampled from two Gaussian distributions in 2D space. The Python package scikit-learn has a user-friendly interface for the SVM implementation in LIBLINEAR which we use here.\n\n\nImports\nimport collections\nimport os\nimport string\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom plotly import graph_objects as go\nimport plotly.io as pio\nimport proplot as pplt\nimport pandas as pd\nfrom scipy.stats import norm\nimport seaborn as sns\nimport sklearn.metrics\nfrom sklearn import svm\n\n\n\n\nCode\ndef padded_ranges(X, pad=0.5):\n    xmin, ymin = np.min(X, axis=0) - pad\n    xmax, ymax = np.max(X, axis=0) + pad\n    return (xmin, xmax), (ymin, ymax)\n    \n    \ndef plot_dec_boundary(clf, ax=None, xlim=(-100.0, 100.0), i=0, **kws):\n    kws.setdefault('c', 'black')\n    w0 = clf.intercept_ if type(clf.intercept_) is float else clf.intercept_[i]\n    (w1, w2) = clf.coef_[i]\n    line_x = np.array(xlim)\n    line_y = -(w1 / w2) * line_x - (w0 / w2)\n    ax.plot(line_x, line_y, **kws)\n    \n    \ndef plot_dec_regions(clf, xlim=None, ylim=None, ax=None, nsteps=500, **kws):\n    kws.setdefault('alpha', 0.05)\n    kws.setdefault('zorder', 0)\n    xx, yy = np.meshgrid(\n        np.linspace(xlim[0], xlim[1], nsteps), \n        np.linspace(ylim[0], ylim[1], nsteps),\n    )\n    Z = np.c_[xx.ravel(), yy.ravel()]\n    y_pred = clf.predict(Z)\n    zz = y_pred.reshape(xx.shape)\n    ax.contourf(xx, yy, zz, **kws)\n\n\n# Generate two Gaussian distributions\nnp.random.seed(0)\nn = 200\nX = np.vstack([\n    np.random.normal(size=(n, 2), loc=[0.0, 0.0], scale=2.0),\n    np.random.normal(size=(n, 2), loc=[5.0, 5.0], scale=2.5)\n])\ny = n * [1] + n * [-1] \n\n# Find SVM decision boundary \nclf = svm.LinearSVC(C=1)\nclf.fit(X, y)\n\n# Plot\nfig, ax = pplt.subplots(xspineloc='neither', yspineloc='neither')\nax.scatter(X[:, 0], X[:, 1], s=20, alpha=0.35, c=y, cmap='PiYG_r')    \nxlim, ylim = padded_ranges(X, pad=0.5)\nplot_dec_boundary(clf, ax=ax)\nplot_dec_regions(clf, xlim, ylim, ax=ax, cmap='PiYG_r')\nax.format(xlim=xlim, ylim=ylim)\nax.annotate('Decision\\nboundary', xy=(0.55, 0.02), xycoords='axes fraction')\nplt.close()\n\n\n\n\n\nFig. 6. An SVM decision boundary for a two-class data set. Each point is colored by its class.\n\n\nThe points are colored by their true classes, and the background is shaded according to the SVM prediction at each point. It can be important to try at least a few different values of \\(C\\), which determines the trade-off between correctly classifying all samples and maximizing the margin, and to observe the effect on the accuracy as well as the algorithm convergence. Parameters such as this one which change the algorithm behavior but aren’t optimized by the algorithm itself are commonly known as hyperparameters.\n\n\n2.2. Kernel trick\nIn some cases the linear model is going to be bad; a frequently used example is “target” dataset.\n\n\nCode\nn = 400\nr1 = np.sqrt(np.random.uniform(0.0, 0.2, size=(n,)))\nr2 = np.sqrt(np.random.uniform(0.5, 1.0, size=(n,)))\nt1 = np.random.uniform(0, 2*np.pi, size=(n,))\nt2 = np.random.uniform(0, 2*np.pi, size=(n,))\nX = np.vstack([\n    np.vstack([r1 * np.cos(t1), r1 * np.sin(t1)]).T,\n    np.vstack([r2 * np.cos(t2), r2 * np.sin(t2)]).T,\n])\ny = n * [1] + n * [-1]\nxlim, ylim = padded_ranges(X, pad=0.1)\n\nfig, ax = pplt.subplots(xspineloc='neither', yspineloc='neither')\nax.scatter(X[:, 0], X[:, 1], s=20, alpha=0.35, c=y, cmap='PiYG_r')\nax.format(xlim=xlim, ylim=ylim)\nplt.close()\n\n\n\n\n\nFig. 7. Two-dimensional data set which no linear model can classify.\n\n\nA line won’t work; ideally, we would draw a circle around the inner cluster to split the data. The kernel trick can be used to alleviate this problem by performing a transformation to a higher dimensional space in which the data is linearly separable. For example, consider the transformation\n\\[\n(x_1, x_2) \\rightarrow (x_1^2, x_2^2, \\sqrt{2} x_1 x_2)\n\\tag{4}\\]\n\n\nCode\nx1, x2 = X.T\nu = x1**2\nv = np.sqrt(2) * x1 * x2\nw = x2**2\nfig = go.Figure(\n    data=go.Scatter3d(\n        x=u, y=v, z=w, mode='markers',\n        marker=dict(color=y, size=3, opacity=0.5),\n    ),\n)\nfig.update_scenes(xaxis_visible=False, yaxis_visible=False, zaxis_visible=False)\nfig.show()\n\n\n\nFig. 8. Data after applying the nonlinear transformation in Equation 4. This is an interactive plot.\n\n\nIt’s clear from rotating this plot that the transformed data can be split with a 2D plane. This need not be the transformation used by the SVM — in fact, many transformations can be used — but it demonstrates the idea. The linear boundary in the transformed space can then be transformed into a nonlinear boundary in the original space. One way to plot this boundary is to predict a grid of points, then make a contour plot (the boundary is shown in grey).\n\n\nCode\nclf = svm.SVC(kernel='rbf')\nclf.fit(X, y)\n\nfig, ax = pplt.subplots(xspineloc='neither', yspineloc='neither')\nax.scatter(X[:, 0], X[:, 1], s=20, alpha=0.35, c=y, cmap='PiYG_r')\nax.format(xlim=xlim, ylim=ylim)\nplot_dec_regions(clf, xlim=xlim, ylim=ylim, ax=ax, cmap='PiYG_r')\nplt.close()\n\n\n\n\n\nFig. 9. Result of SVM with RBF kernel\n\n\nThere are still several advantages to the linear SVM. First, it is much faster to train, and second, the kernel trick may be unnecessary for high-dimensional data. As we’ll see, text data can involve a large number of very high-dimensional samples, so we’ll be sticking with linear kernels.\n\n\n2.3. Multi-class SVM\nA binary classifier can also be used for multi-class problems. Here we use the one-versus-rest (OVR) approach. Suppose we had \\(N\\) classes denoted by \\(c_1\\), \\(c_2\\) … \\(c_N\\). In the OVR approach, we train \\(N\\) different classifiers; the ith classifier \\(L_i\\) tries to split the data into two parts: \\(c_i\\) and not \\(c_i\\). Then we observe a new point and ask each classifier \\(L_i\\) how confident it is that the point belongs to \\(c_i\\). The point is assigned to the class with the highest score. We can extend our previous example to three Gaussian distributions to get a sense of how the decision boundaries are formed.\n\n\nCode\n# Create three Gaussian distributions\nnp.random.seed(0)\nn = 200\nX = np.vstack([\n    np.random.normal(size=(n, 2), loc=[0, 0], scale=2.0),\n    np.random.normal(size=(n, 2), loc=[5, 5], scale=2.5),\n    np.random.normal(size=(n, 2), loc=[-6, 6], scale=2.5),\n])\ny = n * [1] + n * [0] + n * [-1] \n\n# Find SVM decision boundary \nclf = svm.LinearSVC(C=1, multi_class='ovr', max_iter=10000)\nclf.fit(X, y)\n\n# Plot the data\ncmap=pplt.Colormap(('pink9', 'steel', 'darkgreen'))\nfig, ax = pplt.subplots(xspineloc='neither', yspineloc='neither')\nax.scatter(X[:, 0], X[:, 1], s=20, alpha=0.35, c=y, cmap=cmap)\n\n# Plot decision boundary\nxlim, ylim = padded_ranges(X)\nfor i in range(3):\n    ls = ['-', '--', 'dotted'][i]\n    plot_dec_boundary(clf, ax=ax, i=i, ls=ls, label=f'class {i + 1} boundary')\nax.format(xlim=xlim, ylim=ylim)\nax.legend(ncols=1, loc=(1.1, 0.6))\nplot_dec_regions(clf, xlim, ylim, ax=ax, cmap=cmap)\nplt.close()\n\n\n\n\n\nFig. 10. Linear SVM trained on three-class data.\n\n\nThe same idea holds with more classes and dimensions. Notice that there are some regions which are claimed by multiple classifiers, so it’s not a perfect method."
  },
  {
    "objectID": "posts/2021-04-29_authorship-identification/index.html#n-grams-and-feature-selection-methods",
    "href": "posts/2021-04-29_authorship-identification/index.html#n-grams-and-feature-selection-methods",
    "title": "Authorship identification",
    "section": "3. N-grams and feature selection methods",
    "text": "3. N-grams and feature selection methods\nAs I mentioned in the introduction, the paper I’m following is called N-Gram Feature Selection for Authorship Identification. In short, the paper used n-gram frequencies (defined in a moment) as features in the classification task and developed a new method to select the most significant or “dominant” n-grams. This was tested on a collection of short news articles. Let’s step through their method.\n\n3.1. Data set description\nThe Reuters Corpus Volume 1 (RCV1) data set is a big collection of news articles labeled by topic. Around 100,000 of these have known authors, and there are around 2000 different authors. A specific topic was chosen, and only authors who wrote at least one article which fell under this topic were considered. From this subset of authors, the top 50 in terms of number of articles written were chosen. 100 articles from each author were selected — 5000 in total — and these were evenly split into a training and testing set. The resulting corpus is a good challenge for authorship identification because the genre is invariant across documents and because the authors write about similar topics. Hopefully this leaves the author’s style as the primary distinguishing factor. The data set can be downloaded here. The files are organized like this:\n\n\n\nFig. 11. Organization of RCV1 data set.\n\n\nThere are plenty of functions available to load the data and to extract features from it, but I’ll do everything manually just for fun. To load the data, let’s first create two lists of strings, texts_train and texts_test, corresponding to the 2500 training and testing documents. The class id and author name for each document are also stored.\n\ndef load_files(outer_path):\n    texts, class_ids, class_names = [], [], []\n    for class_id, folder in enumerate(sorted(os.listdir(outer_path))):\n        folder_path = os.path.join(outer_path, folder)\n        for filename in os.listdir(folder_path):\n            class_ids.append(class_id)\n            class_names.append(folder)\n            file = open(os.path.join(folder_path, filename), 'r')\n            text = file.read().replace(' ', '_')\n            texts.append(text)\n            file.close()\n    return texts, class_ids, class_names\n\n\ntexts_train, y_train, authors_train = load_files('reuters_data/train')\ntexts_test, y_test, authors_test = load_files('reuters_data/test')\n\n\n\n\n\n\n\n\n\n\n\nAuthor Name\nAuthor ID\nTraining Text\n\n\n\n\n0\nAaronPressman\n0\nA_group_of_leading_trademark_specialists_plans...\n\n\n1\nAaronPressman\n0\nProspects_for_comprehensive_reform_of_U.S._ban...\n\n\n2\nAaronPressman\n0\nAn_influential_economic_research_group_is_prep...\n\n\n3\nAaronPressman\n0\nThe_Federal_Communications_Commission_proposed...\n\n\n4\nAaronPressman\n0\nAn_international_task_force_charged_with_resol...\n\n\n...\n...\n...\n...\n\n\n2495\nWilliamKazer\n49\nChina_could_list_more_railway_companies_and_is...\n\n\n2496\nWilliamKazer\n49\nThe_choice_of_Singapore_for_the_listing_of_Chi...\n\n\n2497\nWilliamKazer\n49\nChina_ushered_in_1997,_a_year_it_has_hailed_as...\n\n\n2498\nWilliamKazer\n49\nChina_on_Tuesday_announced_a_ban_on_poultry_an...\n\n\n2499\nWilliamKazer\n49\nChina's_leaders_have_agreed_on_a_need_to_stimu...\n\n\n\n\n2500 rows × 3 columns\n\n\n\n\nThe following histogram shows the distribution of document lengths in the training set; it’s expected that the short average document length will greatly increases the difficulty of the classification task relative to longer works such as books.\n\n\nCode\nword_counts = [len(text) for text in texts_train]\n\nfig, ax = pplt.subplots(figsize=(5, 1.5))\nax.hist(word_counts, bins='auto', color='black')\nax.format(xlabel='Document length (characters)', ylabel='Num. docs')\nplt.close()\n\n\n\n\n\nFig. 11. Distribution of document lengths in training set.\n\n\n\n\n3.2. N-grams\nAn obvious feature candidate is word frequency; a less obvious one is n-gram frequency. A character n-gram is a string of length n. For example, the 3-grams contained in red_bike! are red, ed_, d_b, *_bi, bik, ike, ke!*. These shorter strings may be useful because they capture different aspects of style such as the use of punctuation or certain prefixes/suffixes. They also remove any ambiguities in word extraction and work for all languages. To use these features in the SVM classifier, we need to create a feature matrix \\(X\\) where \\(X_{ij}\\) is the frequency of the jth n-gram in the ith document. Thus, each document is represented as a vector in \\(k\\) dimensional space, where \\(k\\) is the number of unique n-grams selected from the training documents. We’ll also normalize each vector so that all points are mapped onto the surface of the \\(k\\)-dimensional unit sphere while preserving the angles between the vectors; this should help the SVM performance a bit.\n\ndef get_ngrams(text, n):\n    return [text[i - n : i] for i in range(n, len(text) + 1)]\n\n\ndef get_ngrams_in_range(text, min_n, max_n):\n    ngrams = []\n    for n in range(min_n, max_n + 1):\n        ngrams.extend(get_ngrams(text, n))\n    return ngrams\n\n\ndef sort_by_val(dictionary, max_items=None, reverse=True):\n    n_items = len(dictionary)\n    if max_items is None or max_items &gt; n_items:\n        max_items = n_items\n    sorted_key_val_list = sorted(dictionary.items(), key=lambda item: item[1], \n                                 reverse=reverse)\n    return {k: v for k, v in sorted_key_val_list[:max_items]}\n\n\nclass NgramExtractor:\n    \n    def __init__(self, ngram_range=(3, 5)):\n        self.vocab = {}\n        self.set_ngram_range(ngram_range)\n        \n    def set_ngram_range(self, ngram_range):\n        self.min_n, self.max_n = ngram_range\n        \n    def build_vocab(self, texts, max_features=None):\n        self.vocab, index = {}, 0\n        for n in range(self.min_n, self.max_n + 1):\n            ngrams = []\n            for text in texts:\n                ngrams.extend(get_ngrams(text, n))\n            counts = sort_by_val(collections.Counter(ngrams), max_features)\n            for ngram, count in counts.items():\n                self.vocab[ngram] = (index, count)\n                index += 1\n    \n    def create_feature_matrix(self, texts, norm_rows=True):\n        X = np.zeros((len(texts), len(self.vocab)))\n        for text_index, text in enumerate(texts):\n            ngrams = get_ngrams_in_range(text, self.min_n, self.max_n)\n            for ngram, count in collections.Counter(ngrams).items():\n                if ngram in self.vocab:\n                    term_index = self.vocab[ngram][0]\n                    X[text_index, term_index] = count\n        if norm_rows:\n            X = np.apply_along_axis(lambda row: row / np.linalg.norm(row), 1, X)\n        return X\n\nNow we need to decide which value(s) of n to use as features. Let’s look at the distribution of n-grams in the training documents.\n\nextractor = NgramExtractor(ngram_range=(1, 15))\nextractor.build_vocab(texts_train)\nlen_counts = collections.Counter([len(ngram) for ngram in extractor.vocab])\n\nfig, ax = pplt.subplots(figsize=(5, 3))\nx, y = zip(*len_counts.items())\nax.barh(x, y, color='k', alpha=1.0, width=0.5)\nax.format(xscale='log', xformatter='log', ylabel='n', xlabel='n-gram count',\n          yticks=x, ytickminor=False)\nax.grid(axis='x')\nplt.close()\n\n\nfig.save('./_output_n_gram_count.png', dpi=250)\n\n\n\n\nFig. 12. Distribution of character n-grams in the training text.\n\n\nThe total number of n-grams with 1 \\(\\le\\)n \\(\\le\\) 15 is about 31 million; training a classifier on data with this number of dimensions is probably infeasible, and even more so on a larger data set. Previous studies have had success with fixing the value of n to be either 3, 4, or 5, so the authors chose to restrict their attention to these values. Their new idea was to use all n-grams in the range 3 \\(\\le\\)n \\(\\le\\) 5. This leaves a few hundred thousand features.\nThe next section will discuss statistical methods to prune the features; for now, though, we’ll implement the simple method of keeping the \\(k\\) most frequent across all the training documents. As long as this doesn’t affect the accuracy too much, we reap the benefits of a reduction in computational time and the ability to fix the feature space dimensionality for the comparison of different feature types. To see why many low-frequency terms may be unimportant, suppose one of the authors wrote a single article about sharks in the training set. The term “shark” would have a small global frequency and be very useful in the training set since no other writers write about sharks, but it’s probably a good idea to discard it since it is unlikely to appear in the testing set. We must be careful, however, because some low-frequency terms could be important. These are probably terms that an author uses rarely but consistently over time. Maybe they like to use “incredible” as an adjective; the global frequency of “incred” would be much less than, say, “that_”, but it’s valuable because its frequency distribution will likely be the same in future writing. A quick test on our data set shows that \\(k\\) = 15,000 is a good number. Let’s try this out on the 15,000 most frequent 3-grams.\n\nngram_range = (3, 3)\nmax_features = 15000\nnorm_rows = True\n\nextractor = NgramExtractor(ngram_range)\nextractor.build_vocab(texts_train, max_features)\nX_train = extractor.create_feature_matrix(texts_train, norm_rows)\nX_test = extractor.create_feature_matrix(texts_test, norm_rows)\n\nHere are some of the values in X_train. The columns have been sorted by descending frequency from left to right.\nWe can now feed this array to the SVM and make predictions on the testing data. I’ll keep the \\(C\\) parameter fixed at \\(C = 1\\) in all cases since this is what is done in the paper (I tried a few different values of \\(C\\) and there wasn’t a large effect on the accuracy). Here is the confusion matrix obtained after training and testing:\n\n\nCode\nclf = svm.LinearSVC(C=1)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\naccuracy = sklearn.metrics.accuracy_score(y_test, y_pred)\ncmat = sklearn.metrics.confusion_matrix(y_test, y_pred)\n\nfig, ax = pplt.subplots()\nax.imshow(cmat, cmap='mono', colorbar=True, colorbar_kw=dict(width='1em', label='Count'))\nax.format(title_kw=dict(fontsize='medium'), ylabel='True class', xlabel='Predicted class')\nplt.close()\n\n\n\n\n\nFig. 13. Confusion matrix for linear SVM after training. The total accuracy is 70%.\n\n\n\n\n3.3. Feature selection\nIn the rest of this post, we’ll study how to use statistical methods to further eliminate features from this initial set of 15,000. This process of selecting features which are “best” in a statistical sense is known as feature selection.\n\n3.3.1. Information gain\nA classical statistical measure of feature “goodness” is called information gain (IG). The idea is that knowing whether or not a term t is found in a document of a known class \\(c\\) gives information about \\(c\\), and that some terms will contribute more information than others. The information gain can be written as  [8]\n\\[\nIG(t) = p(t) \\sum_{i=1}^{m}p(c_i | t) \\log p(c_i | t) +\np(\\bar{t}) \\sum_{i=1}^{m}p(c_i | \\bar{t}) \\log p(c_i | \\bar{t}) -\n\\sum_{i=1}^{m}p(c_i) \\log p(c_i).\n\\tag{5}\\]\nThe probability of choosing term \\(t\\) out of all terms in the corpus is given by \\(p(t)\\), and \\(p(t) + p(\\bar{t}) = 1\\). Similarly, \\(p(c_i)\\) is the probability that a randomly chosen document belongs to class \\(c_i\\), and \\(p(c_i) + p(\\bar{c_i}) = 1\\). The probability that a document belongs to \\(c_i\\) given that it contains \\(t\\) is \\(p(c_i | t)\\), or \\(p(c_i | \\bar{t})\\) if it doesn’t contain \\(t\\). The strategy is then to keep the terms with the highest information gain scores.\n\nclass InfoGainSelector:\n    \n    def __init__(self):\n        self.idx = None\n        \n    def fit(self, X, y):\n        # Compute probability distributions.\n        n_docs, n_terms = X.shape\n        n_classes = len(np.unique(y))\n        P_c_and_t = np.zeros((n_classes, n_terms))\n        for doc_index, class_index in enumerate(y):\n            P_c_and_t[class_index, :] += (X[doc_index, :] &gt; 0).astype(int)\n        P_c_and_t /= np.sum(P_c_and_t)\n        P_t = np.sum(P_c_and_t, axis=0)\n        P_c = np.sum(P_c_and_t, axis=1)\n        P_c_given_t = P_c_and_t / P_t\n        P_c_given_tbar = 1.0 - (1.0 - P_c_and_t) / (1.0 - P_t)\n        \n        # Compute information gain for each feature.\n        def XlogX(X):\n            return X * np.log2(X, out=np.zeros_like(X), where=(X &gt; 0))\n        scores = np.zeros(n_terms)\n        scores += np.sum(P_t * XlogX(P_c_given_t), axis=0)\n        scores += np.sum((1 - P_t) * XlogX(P_c_given_tbar), axis=0)\n        scores -= np.sum(XlogX(P_c))\n        self.idx = np.argsort(scores)\n        \n    def select(self, X, k=-1):  \n        return X[:, self.idx[:k]]\n\nWe’ll now compare 4 sets of 15,000 features: 3-grams, 4-grams, 5-grams, and equal parts 3/4/5-grams, each time using IG to select the best \\(k\\) features and plotting the accuracy vs. \\(k\\). I’ll start from \\(k\\) = 1 to 200.\n\n\nCode\nextractor = NgramExtractor()\nselector = InfoGainSelector()\nclf = svm.LinearSVC(C=1)\n\n\ndef compare_acc_ig(ngram_ranges=None, kmin=0, kmax=1, kstep=1):\n    n_keep = np.arange(kmin, kmax + kstep, kstep).astype(int)\n    accuracies = np.zeros((len(ngram_ranges), len(n_keep)))\n    for i, ngram_range in enumerate(ngram_ranges):\n        extractor.set_ngram_range(ngram_range)\n        max_features = 5000 if ngram_range == (3, 5) else 15000\n        extractor.build_vocab(texts_train, max_features)\n        X_train = extractor.create_feature_matrix(texts_train)\n        X_test = extractor.create_feature_matrix(texts_test)\n        selector.fit(X_train, y_train)\n        for j, k in enumerate(n_keep):\n            X_train_red = selector.select(X_train, k)\n            X_test_red = selector.select(X_test, k)\n            clf.fit(X_train_red, y_train)\n            y_pred = clf.predict(X_test_red)\n            accuracies[i, j] = sklearn.metrics.accuracy_score(y_test, y_pred)\n    return accuracies\n\n\ndef plot_accs(accuracies, kmin=0, kmax=1, kstep=1):\n    fig, ax = pplt.subplots()\n    ax.format(cycle='538')\n    labels = [\"n = 3\", \"n = 4\", \"n = 5\", \"n = (3, 4, 5)\"]\n    ks = np.arange(kmin, kmax + kstep, kstep)\n    for i in range(4):\n        m = [\"D\", \"s\", \"^\", \"s\"][i]\n        mfc = [None, None, \"w\", \"w\"][i]\n        ax.plot(ks, accuracies[i, :], marker=m, ms=4, mew=1.0, mfc=mfc, label=labels[i])\n    ax.format(xlabel=\"Number of features selected (k)\", ylabel=\"Accuracy\")\n    pad = 0.05 * (kmax + kmin + kstep)\n    ax.format(xlim=(kmin - pad, kmax + pad))\n    ax.legend(ncols=1, loc='lower right')\n    return fig, ax\n\n\nngram_ranges = [(3, 3), (4, 4), (5, 5), (3, 5)]\nkmin, kmax, kstep = (1, 201, 20)\naccuracies = compare_acc_ig(ngram_ranges, kmin, kmax, kstep)\nfig, ax = plot_accs(accuracies, kmin, kmax, kstep)\nplt.close()\n\n\n\n\n\nFig. 14. Information Gain (IG) accuracy vs. number of features (\\(k\\)) for \\(1 \\le k \\le 200\\)\n\n\nThe accuracy at \\(k\\) = 1 is 0.04, so using the feature with the highest IG score is actually twice as effective as random guessing! By the end of the plot 3-grams and variable length n-grams have taken a clear leaad, with 5-grams in last place. The performance gap between the different n-grams also appears to be growing with \\(k\\).\nThe next region we’ll look at is \\(200 \\le k \\le 2000\\).\n\n\nCode\nkmin, kmax, kstep = (200, 2000, 200)\naccuracies = compare_acc_ig(ngram_ranges, kmin, kmax, kstep)\nfig, ax = plot_accs(accuracies, kmin, kmax, kstep)\nplt.close()\n\n\n\n\n\nFig. 15. Information Gain (IG) accuracy vs. number of features (\\(k\\)) for \\(200 \\le k \\le 2000\\)\n\n\nNow the gap is decreasing as we approach an upper performance limit at higher \\(k\\), especially for 3-grams. We’ll now look at the region which is plotted in the paper: \\(2,000 \\le k \\le 10,000\\).\n\n\nCode\nkmin, kmax, kstep = (2000, 10000, 1000)\naccuracies = compare_acc_ig(ngram_ranges, kmin, kmax, kstep)\nfig, ax = plot_accs(accuracies, kmin, kmax, kstep)\nplt.close()\n\n\n\n\n\nFig. 16. Information Gain (IG) accuracy vs. number of features (\\(k\\)) for \\(2000 \\le k \\le 10000\\)\n\n\nI noticed that 5-grams make a big jump from last place to first place. I’m not sure if I have any deep insights into this behavior, but it’s interesting that the best n-gram to choose depends on the number of features selected. Now, I should compare with Fig. 1 from the paper:\n\n\n\nFig. 17. Authorship identification results using information gain for feature selection. (From [1].)\n\n\nThe first difference is the maximum achieved accuracy which is a few percentage points higher. The second difference is that the authors found 3-grams to be worst at low \\(k\\) and best at high \\(k\\).and the opposite for 5-grams. I’ll leave this as an open problem.\n\n\n3.3.2. LocalMaxs algorithm\nLet’s look at the top IG scoring n-grams from from the variable-length feature set.\n\n\nCode\nextractor.set_ngram_range((3, 5))\nextractor.build_vocab(texts_train, max_features=5000)\nX_train = extractor.create_feature_matrix(texts_train)\nX_test = extractor.create_feature_matrix(texts_test)\nselector.fit(X_train, y_train)\n\ndef get_term(i):\n    for key, (idx, count) in extractor.vocab.items():\n        if idx == i:\n            return key\n        \nfor rank, i in enumerate(selector.idx[:10], start=1):\n    print('{:02}. {}'.format(rank, get_term(i)))\n\n\nNotice all the variants of the which were included. IG has no way of knowing that these are basically the same. This motivates the definition of something called “glue”. Consider the word bigram Amelia Earhart. These two words are very likely to be found next to each other and could probably be treated as a single multi-word unit; it is as if there is glue holding the two words together. The amount of glue is probably higher than that between, say, window and Earhart. A technique has been developed to quantify this glue and extend its calculation to word n-grams instead of just word bi-grams [9]. The same idea can then be applied to character n-grams.\nLet \\(g(C)\\) be the glue of character n-gram \\(C = c_1 \\dots c_n\\). Assuming we had a way to calculate the glue, how could this concept be used for feature selection? One solution is called the LocalMaxs algorithm. First define an antecedent \\(ant(C)\\) as an (n-1)-gram which is contained in \\(C\\), e.g., “string” \\(\\rightarrow\\) “strin” or “tring”. Then define a successor \\(succ(C)\\) as an (n+1)-gram which contains \\(C\\), e.g., “string” \\(\\rightarrow\\) “strings” or “astring”. C is selected as a feature if\n\\[\ng(C) \\ge g(ant(C)) \\,\\, and \\,\\, g(C) &gt; g(succ(C))\n\\tag{6}\\]\nfor all ant(C) and succ(C). Since we’re dealing with 3 \\(\\le\\) n \\(\\le\\) 5, only the latter condition is checked if n = 3, and only the former condition is checked for n = 5. Eq. (6) says that the glue of a selected feature shouldn’t increase by adding a character to or removing a character from the start or end of the n-gram, i.e., the glue is at a local maximum with respect to similar n-grams. Now that the selection criteria are established, we can move on to calculating the glue. Here there are several options, but the one used in the paper is called symmetrical conditional probability (SCP). If we have a bigram \\(C = c_1c_2\\), then\n\\[\nSCP(c_1c_2) = p(c_1|c_2) \\cdot p(c_2|c_1) = \\frac{p(c_1,c_2)^2}{p(c_1)p(c_2)},\n\\tag{7}\\]\nso SCP is a measure of how likely one character is given the other and vice versa. This formula can be applied to an n-gram \\(C = c_1\\dots c_n\\) by performing a pseudo bigram transformation, which means splitting the n-gram into two parts at a chosen dispersion point; for example, “help” could be split as “h*elp”, “he*lp”, or “hel*p”, where * is the dispersion point. Splitting \\(C\\) as \\(c_1 \\dots c_{n-1}\\)*\\(c_n\\) would give\n\\[\nSCP((c_1 \\dots c_{n-1})c_n) = \\frac{p(c_1 \\dots c_n)^2}{p(c_1 \\dots c_{n-1})p(c_n)}.\n\\tag{8}\\]\nOf course, the answer will depend on the dispersion point. We therefore introduce the FairSCP which averages over the possible dispersion points:\n\\[\nFairSCP(c_1 \\dots c_n) = \\frac{p(c_1 \\dots c_n)^2}{\\frac{1}{n-1}\\sum_{i=1}^{n-1} p(c_1 \\dots c_i)p(c_{i+1} \\dots c_n)}.\n\\tag{9}\\]\nIn summary, LocalMaxs loops through every n-gram in the vocabulary, computes the glue as \\(g(C) = FairSCP(C)\\), and keeps the n-gram if Eq. (6) is satisfied. It differs from IG selection in that the features are not ranked, so the number of selected features is completely determined by the text. The method is implemented below.\n\ndef antecedents(ngram):\n    return [ngram[:-1], ngram[1:]]\n\n\ndef successors(ngram, characters=None):\n    if characters is None:\n        characters = string.printable\n    successors = []\n    for character in characters:\n        successors.append(character + ngram)\n        successors.append(ngram + character)\n    return successors\n\n\nclass LocalMaxsExtractor(NgramExtractor):\n    \n    def __init__(self, ngram_range=(3, 5)):\n        super().__init__(ngram_range)\n        self.counts_list = [] # ith element is dictionary of unique (i+1)-gram counts \n        self.sum_counts_list = [] # ith element is the sum of `counts_list[i].values()`\n        \n    def build_vocab(self, texts, max_features=None):\n        # Count all n-grams with n &lt;= self.max_n\n        self.counts_list, self.sum_counts_list = [], []\n        candidate_ngrams = {}\n        for n in range(1, self.max_n + 1):\n            ngrams = []\n            for text in texts:\n                ngrams.extend(get_ngrams(text, n))\n            counts = collections.Counter(ngrams)\n            self.counts_list.append(counts)\n            self.sum_counts_list.append(sum(counts.values()))\n            if self.min_n &lt;= n &lt;= self.max_n:\n                candidate_ngrams.update(sort_by_val(counts, max_features))\n        self.available_characters = self.counts_list[0].keys()    \n        # Select candidate n-grams whose glue is at local maximum \n        self.vocab, index = {}, 0\n        for ngram, count in candidate_ngrams.items():\n            if self.is_local_max(ngram):\n                self.vocab[ngram] = (index, count)\n                index += 1\n                    \n    def is_local_max(self, ngram):\n        glue, n = self.glue(ngram), len(ngram)\n        if n &lt; self.max_n:\n            for succ in successors(ngram, self.available_characters):\n                if self.glue(succ) &gt;= glue:\n                    return False\n        if n &gt; self.min_n:\n            for ant in antecedents(ngram):\n                if self.glue(ant) &gt; glue:\n                    return False \n        return True\n                    \n    def glue(self, ngram):\n        n = len(ngram)\n        P = self.counts_list[n - 1].get(ngram, 0) / self.sum_counts_list[n - 1]\n        if P == 0:\n            return 0.0\n        Avp = 0.0\n        for disp_point in range(1, n):\n            ngram_l, ngram_r = ngram[:disp_point], ngram[disp_point:]\n            n_l, n_r = disp_point, n - disp_point\n            P_l = self.counts_list[n_l - 1].get(ngram_l, 0) / self.sum_counts_list[n_l - 1]\n            P_r = self.counts_list[n_r - 1].get(ngram_r, 0) / self.sum_counts_list[n_r - 1]\n            Avp += P_l * P_r\n        Avp = Avp / (n - 1)\n        return P**2 / Avp\n\nThe first thing we should do is check the the glue of the derivative n-grams the, *_the*, etc.\n\nextractor = LocalMaxsExtractor(ngram_range=(3, 5))\nextractor.build_vocab(texts_train)\nfor ngram in ['the', '_the', 'the_', '_the_']:\n    glue = extractor.glue(ngram)\n    selected = extractor.is_local_max(ngram)\n    freq = extractor.counts_list[len(ngram) - 1][ngram]\n    print('{:&lt;5}: glue = {:.4f}, selected = {}'.format(ngram, glue, selected))\n\nIt seems to be working correctly. Now we’d like to compare the performance to IG. There’s no way to directly compare since LocalMaxs doesn’t rank features; however, it’s possible to vary the size of the initial set of features from which LocalMaxs makes its selections. Below, this initial size is varied from 3,000 to 24,000 using equal parts 3/4/5 grams as features.\n\n\nCode\nlm_extractor = LocalMaxsExtractor(ngram_range=(3, 5))\nclf = svm.LinearSVC(C=1)\nmax_features_list = np.arange(2000, 8000, 1000).astype(int)\nlm_accuracies, lm_vocabs = [], []\nfor max_features in max_features_list:\n    lm_extractor.build_vocab(texts_train, max_features)\n    X_train_red = lm_extractor.create_feature_matrix(texts_train)\n    X_test_red = lm_extractor.create_feature_matrix(texts_test)\n    clf.fit(X_train_red, y_train)\n    y_pred = clf.predict(X_test_red)\n    lm_accuracies.append(sklearn.metrics.accuracy_score(y_test, y_pred))\n    lm_vocabs.append(lm_extractor.vocab)\n    \nn_keep = [len(vocab) for vocab in lm_vocabs]\nig_extractor = NgramExtractor(ngram_range=(3, 5))\nig_extractor.build_vocab(texts_train, max_features=5000)\nX_train = ig_extractor.create_feature_matrix(texts_train)\nX_test = ig_extractor.create_feature_matrix(texts_test)\nig_selector = InfoGainSelector()\nig_selector.fit(X_train, y_train)\nig_accuracies, ig_vocabs = [], []\nfor lm_vocab in lm_vocabs:\n    k = len(lm_vocab)\n    X_train_red = ig_selector.select(X_train, k)\n    X_test_red = ig_selector.select(X_test, k)\n    clf.fit(X_train_red, y_train)\n    y_pred = clf.predict(X_test_red)\n    ig_accuracies.append(sklearn.metrics.accuracy_score(y_test, y_pred))\n    ig_vocabs.append(extractor.vocab)\n    \nfig, ax = pplt.subplots(figsize=(4, 3))\nkws = dict(marker='.', ms=10, lw=0)\nax.plot(n_keep, lm_accuracies, label='n = (3, 4, 5) — LocalMaxs', **kws)\nax.plot(n_keep, ig_accuracies, label='n = (3, 4, 5) — IG', **kws)\nax.format(xlabel='Number of features selected', ylabel='Accuracy')\nax.legend(ncols=1)\nax.format(xmin=900, xmax=2800)\nplt.close()\n\n\n\n\n\nFig. 18. IG vs. LocalMaxs feature selection for 3/4/5-grams.\n\n\nAs you can see, LocalMaxs achieves a higher accuracy with the same number of features. The neat thing is that the vocabularies are totally different; for example, at the last data point, only about 15% of the n-grams are found in both sets! Let’s count the number of related n-grams in the two sets, where x is related to y if x is an antecedent or successor of y.\n\n\nCode\ndef count_related(ngrams):\n    count = 0\n    for n1 in ngrams:\n        for n2 in ngrams:\n            if n1 != n2 and n1 in n2:\n                count += 1\n                break\n    return count\n\nlm_ngrams = list(lm_vocabs[-1])\nvocab_size = len(lm_ngrams)\nig_vocab = list(ig_extractor.vocab)\nig_ngrams = [ig_vocab[i] for i in ig_selector.idx[:vocab_size]]\nshared = len([ig_ngram for ig_ngram in ig_ngrams if ig_ngram in lm_ngrams])\n\nprint('Vocab size: {}'.format(vocab_size))\nprint('n-grams selected by both IG and LM: {}'.format(shared))\nprint('IG related n-grams: {}'.format(count_related(ig_ngrams)))\nprint('LM related n-grams: {}'.format(count_related(lm_ngrams)))\n\n\nAs mentioned earlier, IG selects many related terms such as the and the_. The LocalMaxs vocabulary is much “richer”, as the authors put it. Here is the corresponding figure from the paper (ignore the white squares):\n\n\n\nResults of the proposed method using only variable-length n-grams and variable-length n-grams plus words longer than 5 characters. (Source: [1].)\n\n\nFor some reason, their implementation extracted way more features than mine did. I don’t have access to the author’s code, and I couldn’t find any implementation of LocalMaxs online, so it’s hard for me to say what’s happening. At least my implementation exhibits some expected behavior (less related terms, better performance at lower feature numbers)."
  },
  {
    "objectID": "posts/2021-04-29_authorship-identification/index.html#conclusion",
    "href": "posts/2021-04-29_authorship-identification/index.html#conclusion",
    "title": "Authorship identification",
    "section": "4. Conclusion",
    "text": "4. Conclusion\nIn the future, I may apply these methods to my own data set; I’m particularly interested in what would happen with Chinese characters. A different problem I’d like to examine is that of artist identification; the problem would be to match a collection of paintings with their painters. The Web Gallery of Art is a database I found after a quick search, and I’m sure there are others. This would give me the chance to learn about image classification.\n\n\n\nFig. 1. The authorship identification task.\nFig. 2. Alexander Hamilton (left) and James Madison (right). (Source: Wikipedia.)\nFig. 3. Distribution of word lengths in “Oliver Twist”. Each curve is for a different sample of 1000 words. (Source: [4].\nFig. 4. National GDPs appear to be moving toward the prediction by Zipf’s Law (red line). (Source: [6].)\nFig. 5. Maximum margin separating plane. (Source: Wikipedia.)\nFig. 6. An SVM decision boundary for a two-class data set. Each point is colored by its class.\nFig. 7. Two-dimensional data set which no linear model can classify.\nFig. 9. Result of SVM with RBF kernel\nFig. 10. Linear SVM trained on three-class data.\nFig. 11. Organization of RCV1 data set.\nFig. 11. Distribution of document lengths in training set.\nFig. 12. Distribution of character n-grams in the training text.\nFig. 13. Confusion matrix for linear SVM after training. The total accuracy is 70%.\nFig. 14. Information Gain (IG) accuracy vs. number of features (\\(k\\)) for \\(1 \\le k \\le 200\\)\nFig. 15. Information Gain (IG) accuracy vs. number of features (\\(k\\)) for \\(200 \\le k \\le 2000\\)\nFig. 16. Information Gain (IG) accuracy vs. number of features (\\(k\\)) for \\(2000 \\le k \\le 10000\\)\nFig. 17. Authorship identification results using information gain for feature selection. (From [1].)\nFig. 18. IG vs. LocalMaxs feature selection for 3/4/5-grams.\nResults of the proposed method using only variable-length n-grams and variable-length n-grams plus words longer than 5 characters. (Source: [1].)"
  },
  {
    "objectID": "posts/2024-10-30_n-dimensional-ment-particle-sampling/index.html",
    "href": "posts/2024-10-30_n-dimensional-ment-particle-sampling/index.html",
    "title": "N-dimensional MENT via particle sampling",
    "section": "",
    "text": "This preprint suggests a new implementation of MENT. MENT is an algorithm to reconstruct a distribution from its projections. It’s a beautiful approach to the reconstruction problem based on the principle of maximum relative entropy (ME) — a very general principle grounded in probability theory. The algorithm turns out to be extremely robust and works for almost all problems I’ve encountered. What’s really nice is that it solves a constrained optimization problem exactly, without any regularization parameter.\nMENT was originally developed for 2D reconstructions (i.e. “tomography”), but its author Gerald Minerbo (LANL) was aware that it could work in higher dimensions. In his original paper, Minerbo used MENT to reconstruct a 3D distribution from 2D projections  [1]. A few years later, he used MENT to reconstruct a 4D distribution from 1D projections  [2]. This was in 1981! It seems like people forgot about this paper. The next paper on 4D tomography wasn’t until the 2000s  [3]. Wong  [4] recently revived Minerbo’s work and applied 4D MENT to experimental data at the SNS.\nIt’s not clear if MENT can scale to 6D (or higher). There’s a key step in the algorithm that involves projecting an N-dimensional probability density function (pdf) \\(\\rho(x)\\) onto an M-dimensional plane. For each point on the M-dimensional plane, we have to compute an integral over (N-M)-dimensional space. This can get expensive. My idea is to estimate the projection by sampling particles from \\(\\rho(x)\\) and binning those particles on the projection axis. This is sort of like Monte Carlo integration. The tough part is sampling the particles, which is not necessarily easier than the numerical integration.\nI tried two sampling methods. I call the first method grid sampling (GS). When N &lt;= 4, it’s possible to evaluate \\(\\rho(x)\\) on a grid, forming an N-dimensional image or discrete probability distribution. It’s super fast to sample particles from the image using numpy.random.choice. Then, for each bin, we sample a point from a uniform distribution within the bin. Grid sampling isn’t absolutely necessary in 4D problems, but it’s convenient. There are basically two parameters: the grid resolution and a smoothing parameter to hide the checkerboard pattern from the discretized pdf. It’s easy to choose the grid resolution: make it as high as possible. I find \\(32^4\\) bins are enough for most distributions, but my computer has no problem storing \\(50^4\\) or more bins.\nI tested this approach by fitting a 4D distribution to 1D projections in a highly nonlinear system. The system is essentially a 2D harmonic oscillator with a nonlinear force applied after each oscillation period. The following plots show the 1D projections in the top rows. There isn’t enough data here to constrain the 4D density, so the reconstruction is inaccurate. But that’s not the point of this example: the only point was to fit the data.\n\n\n\n\n\n\n\n\n\n\nGrid sampling doesn’t work in higher dimensions because of memory constraints, but there are other algorithms to sample from high-dimensional distributions. One is called the Metropolis-Hastings (MH) algorithm, which is a variant of Markov Chain Monte Carlo (MCMC). MH requires access to the unnormalized probability density, but nothing else. It’s surprisingly simple.\nThe only problem with MCMC is that it’s slow. MCMC is often used for Bayesian inference, where the goal is to compute an expectation value under the distribution. Expectation values can usually be estimated from a few hundred or a few thousand samples, but computing projections requires many more samples—possibly millions if we want to capture low-density regions.\nI sped up the sampler by running a bunch of chains in parallel. I read a few papers that warned against this, but I don’t quite understand why. I took the following attitude: If I run a chain for T steps and I think it’s converged to the target distribution, and then I run another chain for T steps with a different starting point, then combining both chains should give a chain of length 2T which has also converged to the target distribution. If that’s true, I should be able to run the chains in parallel. In my experiments, I ran hundreds of parallel chains for thousands of steps to sample around a million points in a few seconds. That gave reliable values for the projected density in MENT.\nFor an initial test, I defined a 6D Gaussian mixture distributions, which is bunch of Gaussian blobs with random positions and sizes. This could be a challenging case for MCMC because the distribution is multimodal. For the training data, I selected the 2D marginal projections: for an N-dimensional distribution, there are N(N – 1)/2 projections. Here’s the result for N=6:\n\nIt worked! And just for fun, here’s the same test for N=12:\n\nSee the preprint for more details. Also the MENT code is here.\n\n\n\n\nReferences\n\n[1] G. Minerbo, MENT: A Maximum Entropy Algorithm for Reconstructing a Source from Projection Data, Computer Graphics and Image Processing 10, 48 (1979).\n\n\n[2] G. N. Minerbo, O. R. Sander, and R. A. Jameson, Four-Dimensional Beam Tomography, IEEE Transactions on Nuclear Science 28, 2231 (1981).\n\n\n[3] K. M. Hock and A. Wolski, Tomographic Reconstruction of the Full 4D Transverse Phase Space, Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment 726, 8 (2013).\n\n\n[4] J. C. Wong, A. Shishlo, A. Aleksandrov, Y. Liu, and C. Long, 4D Transverse Phase Space Tomography of an Operational Hydrogen Ion Beam via Noninvasive 2D Measurements Using Laser Wires, Phys. Rev. Accel. Beams 25, 042801 (2022)."
  },
  {
    "objectID": "posts/2024-06-09_mentflow-part2/index.html",
    "href": "posts/2024-06-09_mentflow-part2/index.html",
    "title": "Maximum-entropy phase space tomography using normalizing flows (part 2)",
    "section": "",
    "text": "The last post introduced an approximate maximum-entropy tomography solver using Generative Phase Space Tomography (GPSR) and normalizing flows. Although the method is straightforward, it wasn’t clear how well it would work. Sometimes we measure the beam and analyze the data offline; such experiments aim to examine the phase space structure, which tells us about the upstream beam dynamics. In these cases, slow reconstruction methods work just fine. But in other cases, it would be nice to reconstruct the beam quickly, perhaps in minutes. Then we could update our plan during the experiment —we typically get one or two experiments per month—or use the measurement to make predictions and update the accelerator in response.\nWe also weren’t sure how well the entropic regularization would work. How close could we get to the exact constrained maximum-entropy solution? Loaiza-Ganem, Gao, and Cunningham  [1] saw positive results on one of the few analytically known maximum entropy distributions in two dimensions. Their focus was on statistical moment constraints, but we felt this needed to be examined in more detail for projection-constrained optimization and the specific case of six-dimensional data.\nWe performed a few numerical experiments to answer these questions. The experiments should be reproducible by following the instructions here."
  },
  {
    "objectID": "posts/2024-06-09_mentflow-part2/index.html#which-flow",
    "href": "posts/2024-06-09_mentflow-part2/index.html#which-flow",
    "title": "Maximum-entropy phase space tomography using normalizing flows (part 2)",
    "section": "Which flow?",
    "text": "Which flow?\nNormalizing flow transformations are diffeomorphisms—smooth, invertible transformations that preserve the topological features of the base distribution. If the base density is nonzero everywhere, the transformed distribution is also nonzero everywhere; therefore, flows can’t perfectly represent disconnected modes. Stimper, Scholkopf, and Hernandez-Lobato illustrate this in Figure 1.\n\n\n\n\n\n\nFigure 1: Normalizing flows cannot perfectly map a unimodal distribution to a bimodal distribution. Source:  [2].\n\n\n\nFlows can still push the density arbitrarily close to zero, so they can represent many multimodal distributions in practice. The problem is that building complicated, invertible transformations is expensive—fully connected neural networks are not invertible. Two approaches have emerged to build powerful flow transformations.\n\nDiscrete flows\nA discrete flow is a series of discrete maps, just like computational models that represent an accelerator lattice as a series of symplectic transfer maps. For a map composed of \\(T\\) layers\n\\[\n\\mathcal{F} = \\mathcal{F}_T \\circ \\mathcal{F}_{T-1} \\circ \\dots \\circ \\mathcal{F}_{2} \\circ \\mathcal{F}_1,\n\\tag{1}\\]\nthe coordinates transform as\n\\[\n\\mathbf{z}_t = \\mathcal{F}_{t} (\\mathbf{z}_{t - 1}),\n\\tag{2}\\]\nand the Jacobian determinant as\n\\[\n\\left| \\det J_{\\mathcal{F}}(\\mathbf{z}_0) \\right| =\n\\prod_{t=1}^T {\n    \\left| \\det J_{\\mathcal{F}_t}(\\mathbf{z}_{t - 1}) \\right|.\n}\n\\tag{3}\\]\nThe Lil’Log blog has nice explanations of some discrete flow models  [3]. While propagating particles through the flow layers is easy enough, we may need many such layers to approximate complicated distributions. Thus, discrete flow models can have a huge number of parameters.\n\n\nContinuous flows\nA continuous flow defines a velocity field \\(\\mathbf{v}(\\mathbf{z}(t), t)\\). We can propagate particles through the flow by integrating an ordinary differential equation (ODE):\n\\[\n\\frac{d\\mathbf{z}}{dt} = \\mathbf{v}(\\mathbf{z}(t), t).\n\\tag{4}\\]\nAnd we can compute the change in density from the associated probability flow:\n\\[\n\\frac{d}{dt} \\log\\rho(\\mathbf{z}(t)) = -\\nabla \\cdot \\mathbf{v}(\\mathbf{z}(t), t)\n\\tag{5}\\]\nGrathwohl et al.  [4] described a way to backpropagate derivatives through Equation 4 and approximately solve Equation 5. The neural network representing the velocity field \\(\\mathbf{v}(\\mathbf{z}(t), t)\\) does not need to be invertible. Thus, continuous flows are much more flexible than discrete flows of a similar model size. You can also do cool things with continuous flows like finding optimal transport maps between distributions  [5].\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 2: Two figures from the paper that introduced continuous normalizing flows, FFJORD: Free-form continuous dynamics for scalable reversible generative models  [4]. (a) Continuous transformation of 1D distribution. (b) Continuous flow (FFJORD) vs. similar size discrete flow (GLOW).\n\n\n\nHowever, continuous flows can also be expensive to evaluate because they require solving an ODE. One example I cited was Green, Ting, and Kamdar , who used FFJORD to estimate the density of stars in 6D phase space. Their training took up to a few days on a GPU! In my experiments with continuous flows, I’ve seen good 6D density estimation in less time, but this still illustrates the potential computational cost. (When trained on data samples, it’s now much more efficient to use a technique called flow matching,  [6]. Cambridge MLG has a great blog post on flow matching.\n\n\nNeural Spline Flow\nContinuous flows took quite a while to train on my single GPU (MPS on Apple M1 Max). On the other hand, several discrete flows could not model complicated 2D distributions without extremely large neural networks. The Neural Spline Flow (NSF) model  [7] was more powerful and efficient than anything else I tried.\nWe used an autoregressive flow, where the transformation in each layer is given by\n\\[\nz_i \\rightarrow \\tau (z_i; c_i(z_1, \\dots, z_{i -1})),\n\\tag{6}\\]\nwhere \\(\\mathbf{z} = [z_1, \\dots , z_n]^T\\). Autoregressive transformations have two components: a transformer and the conditioner. The transformer, \\(\\tau\\), is an invertible function. The transformation of the \\(i\\)th dimension is conditioned on the conditioner \\(c_i(z_1, \\dots, z_{i -1})\\); in other words, its parameters depend on \\(c_i\\). The conditioner for the \\(i\\)th dimension is a function only of the dimensions \\(j &lt; i\\). So, if our dimensions were \\([x, y, z]\\), the first conditioner would be a function of \\(x\\), the second conditioner would be a function of \\(x\\) and \\(y\\), and the third conditioner would be a function of \\(x\\) \\(y\\), and \\(z\\).\n\n\n\n\n\n\nFigure 3: Illustration of the autoregressive flow transformation. In this example, \\(\\mathbf{h}_i = c_i(\\mathbf{z}_{&lt;i}) = c_i(z_1, \\dots, z_{i -1})\\). Source:  [8].\n\n\n\nThe reason for this architecture is twofold. First, the transformation is invertible for any conditioner \\(c_i\\). Second, because the transformation along dimension \\(i\\) depends only on dimensions \\(j &lt; i\\), the transformation’s Jacobian is triangular, and the determinant of a triangular Jacobian is efficient to compute. The disadvantage of the autoregressive flow is that it’s \\(n\\) times slower in one direction. In Figure 3, sampling is \\(n\\) times slower than density evaluation. We need to sample quickly, so we simply reverse the architecture for fast sampling and slow density evaluation. (Actually, our model does not need to be invertible! All we need to do is compute the transformation’s Jacobian determinant. I’ve had some problems computing the Jacobian using autodifferentiation, but if we got that working, we could use fully connected neural networks for maximum-entropy tomography.)\nWe still need to implement the transformer and conditioner. For the conditioner, we use a masked neural network. The idea is to block some connections between the neurons in a neural network to ensure \\(c_i(\\mathbf{z}) = c_i(\\mathbf{z}_{&lt;i})\\). Easy enough! For the transformer, we need an invertible 1D function \\(\\tau\\). NSF introduces a transformer based on splines, i.e., piecewise-defined functions. The conditioner provides \\(K\\) different spline knots \\(\\{ x^{(k)} , y^{(k)} \\}\\); as shown in Figure 4, the function must pass through these knots. The conditioner also provides the derivatives at the knots.\n\n\n\n\n\n\nFigure 4: Monotonic rational-quadratic splines used in the autoregressive transformer function. Source:  [7].\n\n\n\nWe then write a rather complicated function of these inputs (knot locations and derivatives) that ends up being a quadratic function of the input variable (\\(x\\)) divided by another quadratic function of the input variable. This function has an analytic derivative and, therefore, an analytic Jacobian determinant. And the function can be inverted quickly by finding the roots of a quadratic equation."
  },
  {
    "objectID": "posts/2024-06-09_mentflow-part2/index.html#d-reconstructions-from-1d-projections",
    "href": "posts/2024-06-09_mentflow-part2/index.html#d-reconstructions-from-1d-projections",
    "title": "Maximum-entropy phase space tomography using normalizing flows (part 2)",
    "section": "2D reconstructions from 1D projections",
    "text": "2D reconstructions from 1D projections\nOur first numerical experiments were 2D reconstructions from linear projections, meaning 1D projections after a linear transformation. The goal was to compare to the MENT algorithm. We’re still working on MENT, and it deserves its own post; the key point for our purposes is that if MENT converges, it generates an exact constrained maximum-entropy solution. Thus, MENT is almost like an analytic solution and is an extremely valuable benchmark. 2D examples can also help evaluate the flow’s representational power, as anything beyond 3D is difficult to visualize. We adapted toy distributions from machine learning benchmarks as the ground-truth in these examples. We assumed we could vary the projection angles evenly between 0 and 180 degrees, providing the information-maximizing set of measurements.\nFigure 5, Figure 6, and Figure 7 compare three reconstruction models: MENT, MENT-Flow, and an unregularized neural network (NN). Each column corresponds to the same training data: a fixed number of projections at evenly spaced projection angles. The faint gray lines indicate the projection angles, and the projections are plotted on the bottom section of the figure. We also overlay the simulated projections from each of the three reconstructions (they overlap).\n\n\n\n\n\n\nFigure 5: Reconstruction of 2D “two spirals” distribution.\n\n\n\n\n\n\n\n\n\nFigure 6: Reconstruction of 2D “rings” distribution.\n\n\n\n\n\n\n\n\n\nFigure 7: Reconstruction of 2D “swissroll” distribution.\n\n\n\nThe model works! MENT-Flow doesn’t quite make it to the MENT solution, i.e., its entropy is a bit lower, but the differences aren’t too noticeable unless you zoom in.\nNotice that all the distributions in a given column generate the same projections. The neural network finds some interesting solutions. Take a look at the fourth column of Figure 6. In the last row, we have a tilted square-ish shape in which almost all particles are shoved to the corners of the square. In the other rows, the MENT solution returns a uniform-density inner ring surrounded by eight evenly spaced clusters on the edge of a low-density sphere. At first, it looks like there is no way these distributions could generate the same projections, but they do! The 1D projections are identical: left-right symmetric with two higher-density inner modes and two lower-density outer modes. It’s easy to see how the MENT solution generates these projections. Surprisingly, in the NN solution, the clusters at the corners of the tilted square are arranged such that the 1D projections always have four modes, with the two inner modes of higher density.\nThe important point is that the measurements do not discriminate between these solutions; only our prior—a Gaussian distribution in this case—makes a difference. The MENT solutions are closer to the prior. Consider the cases on the far left, where we provide only two measurements. These measurements are the marginal distributions \\(\\rho(x)\\) and \\(\\rho(x'\\)). There is no dependence between \\(x\\) and \\(x'\\) in the prior distribution: \\(\\rho_*(x, x') = \\rho(x)\\rho(x')\\). The measurements provide no information about the relationship between \\(x\\) and \\(x'\\). Therefore, the MENT posterior distribution does not contain any dependence between \\(x\\) and \\(x'\\): \\(\\rho(x, x') = \\rho(x)\\rho(x')\\). A similar principle applies in the remaining cases: MENT does not update the prior unless forced to by the data.\nWhen evaluating these results, it’s important to keep in mind that one cannot do better than MENT with a fixed set of data unless one is lucky. MENT only enforces logically consistent inference. That’s it! If the reconstruction is poor, it implies we need more constraints or a better prior distribution. It’s also worth noting that MENT is entirely flexible. We could obtain any posterior by varying the prior. The idea that MENT is somehow inflexible or overly restrictive comes, I think, from an assumption that the prior must be uniform (perhaps because MENT was first formulated using absolute entropy, not relative entropy).\nAs I mentioned, MENT-Flow solutions are approximate because we use a penalty method for the constrained optimization. We minimize \\(L = -H + \\mu D\\), where \\(H\\) is the entropy and \\(D\\) is the data mismatch. We start with \\(\\mu = 0\\), then gradually increase \\(\\mu\\) until \\(D\\) is small enough. There is a risk of overfitting if \\(\\mu\\) becomes too large too quickly. I don’t have an automatic stopping criterion. I find it best to check the simulated vs. measured projections by eye because we usually know approximately how tight our fit should be based on the measurement quality (and how much we trust our beam physics simulation). It makes sense to monitor the reconstructed distribution at the same time. It’s usually obvious when the NSF flow is overfitting because of unnatural-looking high-frequency terms.\n\n\n\n\n\n\nFigure 8: A typical training run. The plot shows the negative entropy in black, and the data fit in grey. Both quantities should be as small as possible. The distribution moves from the unconstrained to the constrained point of maximum entropy as the penalty parameter increases at each epoch. The jumps in the loss curves correspond to jumps in the penalty parameter."
  },
  {
    "objectID": "posts/2024-06-09_mentflow-part2/index.html#d-reconstructions-from-1d-projections-1",
    "href": "posts/2024-06-09_mentflow-part2/index.html#d-reconstructions-from-1d-projections-1",
    "title": "Maximum-entropy phase space tomography using normalizing flows (part 2)",
    "section": "6D reconstructions from 1D projections",
    "text": "6D reconstructions from 1D projections\nFigure 5, Figure 6, and Figure 7 show that MENT-Flow works as intended in 2D. What about 6D? We don’t have an efficient 6D version of MENT—if we did, we would use it—but the entropy calculation does not depend on dimension. As a comparison, we continued to train a neural network to find solutions farther from the prior. (This turns out to be a very useful tool in real experiments.) The perhaps bigger question is whether the flow would fit 6D distributions efficiently. The 2D experiments took a few minutes to run on a single GPU (of course, many hyperparameters affect the runtime).\nDesigning the 6D numerical experiments was daunting. 6D tomography is brand new. A group from SLAC recently proposed a promising measurement beamline consisting of a single quadrupole, dipole, and vertical deflecting RF cavity [https://arxiv.org/abs/2404.10853], but that idea wasn’t proposed until later. Rather than design a beamline, we decided to consider a more abstract problem.\nUnlike 2D tomography, there isn’t a clear way to manipulate the beam to maximize the information encoded in the measurement set. In 2D tomography, we know we should rotate the distribution around 180 degrees, typically with equally spaced angles. Things are way less clear in high dimensions. To start, we needed to select the dimension of the measurements: 1D or 2D. We selected 1D projections because it is easier to generalize the notion of “projection angle” to high dimensions when the projections are 1D. Note that a 1D projection is defined by a point on the unit sphere, or a unit vector \\(\\mathbf{v}\\). This is the projection axis. The integration axis is an (\\(n - 1\\))-dimensional plane orthogonal to \\(\\mathbf{v}\\). Although there is no well-defined “projection angle,” there is a well-defined angle between projections: \\(\\cos\\theta = \\mathbf{v}_I \\cdot \\mathbf{v}_j\\). It seems like we want to maximize the average angle between vectors. In other words, we want to distribute unit vectors as uniformly as possible on the unit sphere. We can easily generate random samples from a uniform distribution on the sphere by sampling from an \\(n\\)-dimensional Gaussian distribution and dividing each point by its radius. So that’s what we decided to do. The reconstruction should converge to the true distribution in the limit of many projections.\n\n\n\n\n\n\nFigure 9: Illustration of random 1D projection axes sampled from the the unit sphere in 6D phase space.\n\n\n\n\nGMM\nOur first experiments used a 6D Gaussian mixture distribution as the ground truth. The distribution is the superposition of seven Gaussian distributions with randomly chosen means and variances. We examined reconstructions from 25 random projections; see Figure 10. (Click on the figures to enlarge!)\n\n\n\n\n\n\n\n\n\n\n\n(a) Data (Flow)\n\n\n\n\n\n\n\n\n\n\n\n(b) Data (NN)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Reconstruction (Flow)\n\n\n\n\n\n\n\n\n\n\n\n(d) Reconstruction (NN)\n\n\n\n\n\n\n\nFigure 10: Reconstruction of 6D GMM distribution from 25 random 1D projections. Red = true, blue = reconstructed.\n\n\n\nThe true and simulated 1D measurement measurements overlap in both models. Yet the 6D distributions are again quite different. To compare subplots, look along the diagonal, moving from lower left to upper right. The entropy penalty pulls the reconstruction much closer to the uniform prior; there are more high-frequency terms in the NN reconstruction. Also notice that the distribution’s modes are better resolved in the MENT-Flow reconstruction. In particular, observe the \\(x'\\)-\\(y\\) projection in both plots. (There should be five modes visible.)\nUsing 100 projections provides much tighter constraints, and the models are much closer together. But they are still not the same! See Figure 11. This example shows that MENT-Flow can fit complex 6D distributions to large measurement sets in reasonable time (around 10-15 minutes for my training hyperparameters).\n\n\n\n\n\n\n\n\n\n\n\n(a) Data (Flow)\n\n\n\n\n\n\n\n\n\n\n\n(b) Data (NN)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Reconstruction (Flow)\n\n\n\n\n\n\n\n\n\n\n\n(d) Reconstruction (NN)\n\n\n\n\n\n\n\nFigure 11: Reconstruction of 6D GMM distribution from 100 random 1D projections. Red = true, blue = reconstructed.\n\n\n\n\n\nRings\nWe were also interested in testing MENT-Flow on hollow distributions. In our direct 5D/6D phase space measurements at the SNS, we’ve observed hollow structures in 3D and 5D distributions that are not visible from low-dimensional projections. Contrast with Figure 10 and Figure 11, where we can see most of the modes from 2D views. It’s best to illustrate with a simple example. Consider the “rings” distribution from Figure 6. It’s straightforward to generalize the distribution to \\(n\\) dimensions, so we did this and ran the same 6D reconstruction as just described. A 25-projection reconstruction is shown in Figure 12.\n\n\n\n\n\n\n\n\n\n\n\n(a) Data (Flow)\n\n\n\n\n\n\n\n\n\n\n\n(b) Data (NN)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Reconstruction (Flow)\n\n\n\n\n\n\n\n\n\n\n\n(d) Reconstruction (NN)\n\n\n\n\n\n\n\nFigure 12: Reconstruction of 6D “rings” distribution from 25 random 1D projections. Red = true, blue = reconstructed. (c-d) 2D distribution within a shrinking 4D sphere in the unplotted coordinates.\n\n\n\nThe 1D projections are boring; they don’t even hint at the structure in the 6D space! To make each figure in the bottom row, we select particles within a shrinking 4D ball in the transverse (\\(x\\)-\\(x'\\)-\\(y\\)-\\(y'\\)) plane, then plot the longitudinal (\\(z\\)-\\(z'\\)) distribution of the selected particles. This is a spherical slice. On the far left, we see the full projection \\(\\rho(z, z')\\). On the far right, we see the two rings inside the 4D transverse core. MENT-Flow produces a slightly modified version of the Gaussian prior. A higher-density core surrounded by a low-density cloud has emerged, but the data do not provide tight constraints on the distribution. Still, the (approximate) maximum-entropy distribution is preferred to the NN solution, which ejects all particles from the core.\nThese points are even clearer in Figure 13, where we use 100 projections as training data. There still isn’t enough data to resolve two rings, but the MENT-Flow reconstruction shows a clear high-density spherical core surrounded by a low-density particle cloud. I was quite surprised that the neural network split the core! One would think 100 1D projections would be enough to constrain the 6D distribution, but this is false. (These results depend on the random seed used to define the projection axes, but similar patterns are common.)\n\n\n\n\n\n\n\n\n\n\n\n(a) Data (Flow)\n\n\n\n\n\n\n\n\n\n\n\n(b) Data (NN)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Reconstruction (Flow)\n\n\n\n\n\n\n\n\n\n\n\n(d) Reconstruction (NN)\n\n\n\n\n\n\n\nFigure 13: Reconstruction of 6D “rings” distribution from 100 random 1D projections. Red = true, blue = reconstructed."
  },
  {
    "objectID": "posts/2024-06-09_mentflow-part2/index.html#conclusion",
    "href": "posts/2024-06-09_mentflow-part2/index.html#conclusion",
    "title": "Maximum-entropy phase space tomography using normalizing flows (part 2)",
    "section": "Conclusion",
    "text": "Conclusion\nNumerical experiments show that MENT-Flow works and could be useful in 4D/6D phase space tomography when measurements are sparse. There are several interesting research problems to pursue, some related to MENT, some to MENT-Flow, some to GPSR, and some to phase space tomography in general. In the next post, I’ll describe how we used MENT-Flow to reconstruct the 4D phase space density of an intense proton beam from 1D measurements in the SNS.\n\n\n\nFigure 1: Normalizing flows cannot perfectly map a unimodal distribution to a bimodal distribution. Source:  [2].\nFigure 2 (a): \nFigure 2 (b): \nFigure 3: Illustration of the autoregressive flow transformation. In this example, \\(\\mathbf{h}_i = c_i(\\mathbf{z}_{&lt;i}) = c_i(z_1, \\dots, z_{i -1})\\). Source:  [8].\nFigure 4: Monotonic rational-quadratic splines used in the autoregressive transformer function. Source:  [7].\nFigure 5: Reconstruction of 2D “two spirals” distribution.\nFigure 6: Reconstruction of 2D “rings” distribution.\nFigure 7: Reconstruction of 2D “swissroll” distribution.\nFigure 8: A typical training run. The plot shows the negative entropy in black, and the data fit in grey. Both quantities should be as small as possible. The distribution moves from the unconstrained to the constrained point of maximum entropy as the penalty parameter increases at each epoch. The jumps in the loss curves correspond to jumps in the penalty parameter.\nFigure 9: Illustration of random 1D projection axes sampled from the the unit sphere in 6D phase space.\nFigure 10 (a): Data (Flow)\nFigure 10 (b): Data (NN)\nFigure 10 (c): Reconstruction (Flow)\nFigure 10 (d): Reconstruction (NN)\nFigure 11 (a): Data (Flow)\nFigure 11 (b): Data (NN)\nFigure 11 (c): Reconstruction (Flow)\nFigure 11 (d): Reconstruction (NN)\nFigure 12 (a): Data (Flow)\nFigure 12 (b): Data (NN)\nFigure 12 (c): Reconstruction (Flow)\nFigure 12 (d): Reconstruction (NN)\nFigure 13 (a): Data (Flow)\nFigure 13 (b): Data (NN)\nFigure 13 (c): Reconstruction (Flow)\nFigure 13 (d): Reconstruction (NN)"
  },
  {
    "objectID": "posts/2023-07-10_fine-tuning/index.html",
    "href": "posts/2023-07-10_fine-tuning/index.html",
    "title": "Fine-tuning arguments",
    "section": "",
    "text": "The universe would be fine-tuned for life if the set of possible life-permitting universes were much smaller than the set of possible universes. An immediate difficulty here is how to define the set of possible universes. The universe is characterized by states, laws, and constants. We can consider a state to be a snapshot of the universe at a single time; for example, in classical physics, a state could be the position and momentum of every particle \\(\\{\\mathbf{x}_i, \\mathbf{p}_i\\}\\), or in quantum physics, a state could be the wavefunction \\(\\Psi (\\mathbf{x})\\). Laws describe relationships between states and are not measured directly; they emerge as part of a theory to explain the available data. Finally, fundamental constants are empirically determined scalars, i.e., not determined by the proposed theory. We need to determine whether the constants, laws, and prior states could have been different and, if so, whether they are fine-tuned for life.\nIt is hard to say whether laws could have been different, but let’s suppose they could have been different and ask whether they are fine-tuned. Here we have an infinite-dimensional function space of possible laws. We could start by just removing the laws we know of, i.e., removing gravity or electromagnetism. It seems plausible from this thought experiment that both attracting and repelling forces are necessary for any complex structures to emerge. But it is hard to see how else to proceed. Nonetheless, it is clear that the laws are, in a certain sense, simple and elegant, leading to order and predictability rather than chaos. It is not hard to see how an argument for theism might flow from such considerations  [1–3].\nLet’s fix the laws and consider variations in the constants and prior states of the universe; this problem is more tractable. I will let the “fine-tuning data” refer to all calculations of the sensitivity of life-permitting conditions to changes to the constants or prior states, as well as all measurements that support the theories used to perform these calculations. A comprehensive discussion of the fine-tuning data is found in  [4]. The SEP article on fine-tuning  [5] has a shorter list of apparently fine-tuned parameters which I repeat here.\n\nThe strength of gravity relative to electromagnetism\nThe strength of the strong force relative to the electromagnetic force\nThe ratio of up quark mass to down quark mass\nThe strength of the weak force\nThe value of the cosmological constant\nThe energy density in the early universe\nThe fluctuation amplitudes in the early universe\nThe entropy of the early universe [ [6];  [7–12]\n\nIn some cases, the relevant parameters could not change by more than one part in \\(10^{50}\\) (or some other enormous value), rending the probability of life, given that the parameters were selected from a sufficiently wide uniform distribution, to be essentially zero. These calculations are striking but are also controversial. Consider the following issues  [13].\n\nFuture physical theories may have no fine-tuned parameters.1\nAssuming a complete theory of physics is still fine-tuned, how should we assign probability distributions in the space of possible worlds?2\nAssuming a uniform distribution is appropriate for each parameter, what range can the various parameters take? A finite range seems arbitrary, and there are problems defining a uniform distribution over an infinite range.\nAre we exploring the multidimensional parameter space rather than varying one parameter at a time?\n\nBecause of the lack of clear answers to these questions, it is unclear how confident we should be that the universe is fine-tuned for life. Nonetheless, I assign a moderate credence to the claim that the universe is fine-tuned for life, partly because fine-tuning is accepted by a significant fraction of physicists and philosophers. Thus, it makes sense to assume fine-tuning exists and work out its implications."
  },
  {
    "objectID": "posts/2023-07-10_fine-tuning/index.html#evidence-of-fine-tuning",
    "href": "posts/2023-07-10_fine-tuning/index.html#evidence-of-fine-tuning",
    "title": "Fine-tuning arguments",
    "section": "",
    "text": "The universe would be fine-tuned for life if the set of possible life-permitting universes were much smaller than the set of possible universes. An immediate difficulty here is how to define the set of possible universes. The universe is characterized by states, laws, and constants. We can consider a state to be a snapshot of the universe at a single time; for example, in classical physics, a state could be the position and momentum of every particle \\(\\{\\mathbf{x}_i, \\mathbf{p}_i\\}\\), or in quantum physics, a state could be the wavefunction \\(\\Psi (\\mathbf{x})\\). Laws describe relationships between states and are not measured directly; they emerge as part of a theory to explain the available data. Finally, fundamental constants are empirically determined scalars, i.e., not determined by the proposed theory. We need to determine whether the constants, laws, and prior states could have been different and, if so, whether they are fine-tuned for life.\nIt is hard to say whether laws could have been different, but let’s suppose they could have been different and ask whether they are fine-tuned. Here we have an infinite-dimensional function space of possible laws. We could start by just removing the laws we know of, i.e., removing gravity or electromagnetism. It seems plausible from this thought experiment that both attracting and repelling forces are necessary for any complex structures to emerge. But it is hard to see how else to proceed. Nonetheless, it is clear that the laws are, in a certain sense, simple and elegant, leading to order and predictability rather than chaos. It is not hard to see how an argument for theism might flow from such considerations  [1–3].\nLet’s fix the laws and consider variations in the constants and prior states of the universe; this problem is more tractable. I will let the “fine-tuning data” refer to all calculations of the sensitivity of life-permitting conditions to changes to the constants or prior states, as well as all measurements that support the theories used to perform these calculations. A comprehensive discussion of the fine-tuning data is found in  [4]. The SEP article on fine-tuning  [5] has a shorter list of apparently fine-tuned parameters which I repeat here.\n\nThe strength of gravity relative to electromagnetism\nThe strength of the strong force relative to the electromagnetic force\nThe ratio of up quark mass to down quark mass\nThe strength of the weak force\nThe value of the cosmological constant\nThe energy density in the early universe\nThe fluctuation amplitudes in the early universe\nThe entropy of the early universe [ [6];  [7–12]\n\nIn some cases, the relevant parameters could not change by more than one part in \\(10^{50}\\) (or some other enormous value), rending the probability of life, given that the parameters were selected from a sufficiently wide uniform distribution, to be essentially zero. These calculations are striking but are also controversial. Consider the following issues  [13].\n\nFuture physical theories may have no fine-tuned parameters.1\nAssuming a complete theory of physics is still fine-tuned, how should we assign probability distributions in the space of possible worlds?2\nAssuming a uniform distribution is appropriate for each parameter, what range can the various parameters take? A finite range seems arbitrary, and there are problems defining a uniform distribution over an infinite range.\nAre we exploring the multidimensional parameter space rather than varying one parameter at a time?\n\nBecause of the lack of clear answers to these questions, it is unclear how confident we should be that the universe is fine-tuned for life. Nonetheless, I assign a moderate credence to the claim that the universe is fine-tuned for life, partly because fine-tuning is accepted by a significant fraction of physicists and philosophers. Thus, it makes sense to assume fine-tuning exists and work out its implications."
  },
  {
    "objectID": "posts/2023-07-10_fine-tuning/index.html#responses-to-fine-tuning",
    "href": "posts/2023-07-10_fine-tuning/index.html#responses-to-fine-tuning",
    "title": "Fine-tuning arguments",
    "section": "2. Responses to fine-tuning",
    "text": "2. Responses to fine-tuning\nWe can map the responses to fine-tuning to the following responses to an all-sixes configuration of many dice at a casino table:\n\nMany Rolls: There were probably many prior rolls.3\nMany Tables: There were probably many simultaneous rolls.\nIntentional Agent: An intentional agent arranged the dice in the all-sixes configuration.\nLucky Roll: The dice were rolled once. Unlikely events happen occasionally.\nBrute Contingency: The dice could have landed in a different configuration, but no explanation is needed for the observed configuration.\nBrute Necessity: The dice could not have landed in a different configuration.\n\nHere, the all-sixes configuration is analogous to a life-supporting universe. Response 2 corresponds to the multiverse hypothesis: that there are many universes, each with different laws and initial conditions. Response 3 corresponds to the design hypothesis: that an intentional agent selected the laws and initial conditions of the universe. Response 4 corresponds to the claim that the laws and initial conditions were unlikely, but that unlikely events do not necessarily require explanations; even if random, the selection process provides a sufficient explanation. Response 5 suggests that there is no deeper explanation at all for the way things are; it is a brute fact. Response 6 corresponds to the claim that the laws and initial conditions are fixed by metaphysical necessity.\n\n\n\n\n\nflowchart TB\n  A[Evidence of fine-tuning] \n  A --&gt; B[Accept]\n      B --&gt; D[Explanation needed]\n          D --&gt; F[Design]\n          D --&gt; G[Multiverse]\n          D --&gt; H[Chance]\n      B --&gt; E[No explanation needed]\n          E --&gt; I[Brute contingency]\n          E --&gt; J[Necessity]\n  A --&gt; C[Reject]\n\n\n\n\n\n\n\nI think we can rule out Response 1 (Many Rolls), which would correspond to a cyclic universe model in which the “initial conditions” were randomly selected on each cycle. I have no idea if such a model exists. I also think we can rule out Response 2 (Lucky Roll) if any better explanations are available (since the chances involved are incredibly small).\nThere are different ways to tease out the degree to which the fact that our universe is life-supporting (\\(L\\)) supports a hypothesis \\(H\\). On the Bayesian view, \\(E\\) supports \\(H\\) if \\[\n\\frac{Pr(H | L)}{Pr(\\neg H | L)} = \\frac{Pr(L | H)}{Pr(L | \\neg H)}\\frac{Pr(H)}{Pr(\\neg H)} &gt; 1.\n\\]\nIn addition to the likelihood \\(Pr(L | H)\\), we must consider the prior probability \\(Pr(H)\\). Assigning priors is challenging when \\(H\\) is a large-scale hypothesis such as theism. Assigning likelihoods is also challenging; we must do our best to be honest about what our hypothesis predicts. But this equation is at least a guide to comparing hypotheses.\n\n2.1. No explanation needed\nConsider first the denial that any explanation of fine-tuning is needed. It could be that our universe, and the fact that the constants are fine-tuned for life, is a brute fact. For example, it could be that the constants could have been different, but we cannot do anything to explain why they are what they are. Although all explanations must stop somewhere, I’m not a big fan of brute contingency. In my studies of cosmological arguments, I’ve become attracted to the idea that there is at least one necessary being — an initial or eternal matter-energy configuration, God, or something like that. This brings us to the possibility that the fine-tuned constants are fixed by metaphysical necessity. This sits better with me, but on the other hand, it is a strange idea that, say, the electron’s charge had to be -1. A great discussion about positing necessary entities is in  [14].\n\n\n2.2. The multiverse hypothesis\nA popular response to the fine-tuning data is to posit an ensemble of universes, each with different parameters. Such a hypothesis — call it \\(M\\) — could render the probability of life \\(P(L | M)\\) quite high if the number of universes is large. The support for \\(M\\) would be strong if \\(P(L | \\neg M) \\ll 1\\) and \\(\\neg (P(M) \\ll 1)\\).\n\n2.2.1. The prior probability of the multiverse\nI am tempted to assign a low prior probability to the multiverse hypothesis (\\(P(M) \\ll 1\\)). It is not obvious that there is more than one universe; it is not even clear what that would mean. But surprisingly, multiverse hypotheses have crept into modern physics. In my understanding, part of the motivation for the multiverse hypothesis — in the context of cosmic fine-tuning — comes from String Theory + Inflation, which predicts a large number of vacuum states with different effective constants and laws.4 Each “universe” (in my understanding) is an isolated spacetime region rather than a separate reality. String Theory is speculative at this point, so I’m not sure how much stock we should hold in this idea; nonetheless, it does not seem unreasonable to suppose that some future, well-tested theory could predict a “multiverse” of effective constants and laws.5\nOn another front, there are worries that the multiverse leads to absurd consequences. For example, consider the problem of Boltzmann Brains: a random fluctuation is much more likely to generate a ten-second-old universe that consists of just me, complete with false memories and sensory inputs, rather than a billion-year-old universe. But it is unreasonable for me to believe that I am such a fluctuation: my belief would entail that the physics I used to come to my belief is false. That is a problem! (Since this is not an issue unique to the multiverse, perhaps we should let it be  [15]). Another example is that there are worries about how to think about probability in an infinite multiverse.\n\n\n2.2.2. The probability of life in a multiverse\nLet’s put those worries aside and treat the multiverse as a serious potential solution to the fine-tuning problem, assigning a non-negligible prior \\(P(M)\\). We now turn to the probability of life in the multiverse, \\(P(L | M)\\). The first issue — and this will reappear in the design hypothesis — is that we need to know more about the multiverse and its dynamics. It is not clear, a priori, whether all universes would be equally likely; it seems possible that some universes could be more likely than others, rendering life-permitting universes incredibly likely or incredibly unlikely. Or the underlying multiverse theory could include yet more fundamental fine-tuned constants.\nThen we have problems arising from an infinite multiverse. Assuming all universes are equally likely, the probability that at least one universe is life-supporting grows with the number of universes. But if the number of universes is infinite, the probability is either 1 or undefined (\\(\\infty / \\infty\\))  [15]. If the probability is 1, one might argue that \\(M\\) is not predictive (everything happens somewhere). I am not bothered by \\(Pr(L | M) = 1\\) if \\(M\\) is well-motivated; otherwise, it appears to be an ad hoc, too-good-to-be-true solution to the fine-tuning problem.\nLet’s put those worries aside and assume there is a large (maybe infinite) ensemble of universes, a small fraction of which support life. Still, some argue that while a multiverse increases the probability that some universe is life-permitting, it does not increase the probability that this universe is life-permitting. In other words, the multiverse hypothesis makes the same mistake as the Inverse Gambler’s Fallacy. In our dice rolling example, if there are many tables, the probability that at least one table rolls all sixes will increase with the number of tables, but the probability that the fourth table rolls all sixes does not depend on the number of tables.\nThe above analogy doesn’t seem right. It is not as if we are sitting in a universe, waiting to see if it is life-permitting; rather, we can only find ourselves in a life-permitting universe. It is as if we were standing outside the casino and were only let in if at least one all-sixes configuration was rolled (and were then brought to an all-sixes table). In this new analogy, the probability that we find ourselves at an all-sixes table scales with the number of tables. This issue is somewhat complicated. Saad  [16] takes great care to make the dice-rolling relevantly like our situation with respect to the fine-tuning data, concluding that “even once those complicating factors are taken into account, fine-tuning should boost our confidence in the existence of other universes.”\nIn conclusion, the multiverse hypothesis could be a reasonable solution to the fine-tuning problem. I am not convinced by the Inverse Gambler’s Fallacy objection. However, serious issues must be addressed — perhaps by a future, fleshed-out version of the hypothesis.\n\n\n\n2.3. The design hypothesis\nAnother popular response to fine-tuning is to posit an intelligent designer. Such a hypothesis — call it \\(T\\) — could render the probability of life \\(P(L | T)\\) quite high, depending on the designer’s character. The support for \\(T\\) would be strong if \\(P(L | \\neg T) \\ll 1\\) and \\(\\neg (P(T) \\ll 1)\\). One difficulty in assessing the design hypothesis is that it is non-specific. There could be one designer, two designers, or an infinity of designers; the designer could be good or evil, weak or powerful, etc. The prior probability of any of these alternative hypotheses should be quite low, and these designers may have very different dispositions toward life, rendering \\(P(L | T) = 0\\) in some cases and \\(P(L | T) = 1\\) in others. Therefore, I propose to replace the generic design hypothesis with some version of theism, where theism is the claim that “… the foundation of reality consists of a mind which possesses a few reasonably natural features (power, goodness, knowledge) to a reasonably natural degree (i.e., a maximal degree)”  [17]. While Oppy argues that any such move will lower the prior probability of the hypothesis, I am pulled somewhat in the opposite direction; perhaps traditional theism has a higher probability than the many other theisms listed above.\n\n2.3.1. The prior probability of design\nCutter and Crummet  [17] note that “The prior probability of theism is the result of two factors: (i) its intrinsic probability, i.e., its probability conditional on no evidence, and (ii) its fit with our background knowledge.” I don’t think the intrinsic probability of theism is too small. The above definition of theism is simple. Furthermore, I find many alternate models of God to be arbitrary and, therefore, unlikely (examples include an evil god, an infinity of gods, a flying spaghetti monster, etc.). I recognize that some find theism inconceivable (how can something be outside spacetime, how can a mind exist without a body, etc.), but I don’t feel that strongly.\nThe fit with our background knowledge is more complicated. As I mentioned, theism is a large-scale hypothesis — arguments for and against theism consider a wide range of phenomena. As such, arguments for theism can sometimes pull in multiple directions. For example, one might find arguments from evil to be evidence against the existence of God and cosmological arguments to be evidence for the existence of God. It is challenging to weigh these considerations against each other. But for the fine-tuning debate, all we need is a rough measure of the plausibility of theism. If the fine-tuning probabilities are correct, our credence in theism may need to be very small to warrant discarding the theistic hypothesis. I think there are enough interesting and thoughtful versions of theism, arguments for theism, and responses to arguments against theism to treat theism seriously as a potential solution to the fine-tuning problem.\n\n\n2.3.2. What would God create?\nOther critiques of the design hypothesis concern the probability of life conditioned on the existence of a designer. If we don’t know anything about the designer, then it seems equally likely for the designer to prefer a universe with or without life. But as I mentioned above, I don’t find the prior probability of theism drastically lower than the prior probability of a generic designer, and on theism, God surely has preferences consistent with God’s own nature. The question is whether we could know these preferences.\nAlthough it might be impossible to know God’s reasons for performing a specific action, it doesn’t seem too difficult to imagine why God would have the disposition to create a predictable, elegant, life-friendly universe; these all seem like good things. On the other hand, considering the amount of evil and suffering in the world, one may think that God would have created a different world or no world at all, making the probability of life, given theism, small. These types of statements of God’s preferences don’t seem too problematic.\nBut if God prefers a life-permitting universe, then surely God will bring about a life-permitting universe. Wouldn’t this make \\(Pr(L | T) = 1\\)? The theist will want to give an account of God’s free will that does not render the universe necessary. I think this problem is related to the problem of free will for omniscient beings. I don’t find this to be a terribly troubling problem.\nHere is another issue raised by Hans Halvorson: why would God prefer a fine-tuned universe? Halvorson argues that since fine-tuning decreases the probability of life, fine-tuning is evidence against a life-friendly designer. Suppose I want to give $10,000 in cash to my friend, but instead of simply handing them the money, I made them guess a number between 1 and \\(n\\), only giving them the money if they selected the number I was thinking of. As my chosen \\(n\\) grows, so does the probability that I didn’t want my friend to have the money. I found this argument quite convincing. I’m unsure what reason God would have for designing the universe this way. Of course, I’m cautious about concluding that there is no such reason.\n\n\n2.3.3. Stalking-horse naturalism\nAlex Malpass has proposed the “stalking-horse naturalism” hypothesis, which is simply naturalism plus a “mysterious disposition” of the constants to obtain their actual values  [18]. There is an argument to be made that this hypothesis does just as well as theism. I thought Luke Barnes had a nice response: we might ask about the probability that the universe had the dispositions that it does; why life-permitting dispositions rather than non-life-permitting dispositions? The answer must be brute necessity, brute contingency, or chance. While the theist can tell a story about why God has life-permitting dispositions, no similar story is available for this “mysterious disposition”.\n\n\n2.3.5. God and the multiverse\nLastly, it is important to note that theism is compatible with a multiverse. I’d like to explore this idea further  [19]."
  },
  {
    "objectID": "posts/2023-07-10_fine-tuning/index.html#conclusion",
    "href": "posts/2023-07-10_fine-tuning/index.html#conclusion",
    "title": "Fine-tuning arguments",
    "section": "3. Conclusion",
    "text": "3. Conclusion\n\nThere are questions about whether fine-tuning is a real feature of the world.\nI think a response is needed if the life-permitting space of physically possible worlds is as small as claimed.\nI don’t like brute contingency, and brute necessity seems wrong for things as arbitrary as the values of constants.\nThe multiverse hypothesis and the design hypothesis are roughly on par.\n\nIt is difficult to compare the two, given all the background knowledge that affects the prior probability of each theory.\nIt is difficult to evaluate the probability of life on either hypothesis.\nThere are independent arguments for theism, but no such arguments for a multiverse. In other words, theism is a unified explanation of a wide range of phenomena. For this reason, I find the theistic hypothesis to be a slightly better response than the multiverse hypothesis.\n\n\nIt is increasingly clear that the various arguments for theism are intertwined. Before moving on to arguments from evil, I plan to spend more time on teleological and cosmological arguments such as the nomological arguments  [2,3], arguments from psychophysical harmony  [17], arguments from the effectiveness of mathematics  [1], and the Thomistic cosmological argument recently defended by Edward Feser. Or I may skip to arguments from evil and come back to these at some point."
  },
  {
    "objectID": "posts/2023-07-10_fine-tuning/index.html#footnotes",
    "href": "posts/2023-07-10_fine-tuning/index.html#footnotes",
    "title": "Fine-tuning arguments",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOn the other hand, all future theories could contain a nonzero number of fine-tuned parameters. Or future theories could predict a probability distribution for each parameter, rending the actual parameter values either very likely or very unlikely.↩︎\nA uniform distribution seems appropriate given our ignorance, but who is to say the parameters were actually drawn from a uniform distribution?↩︎\nIf we knew the roll was fair, then given our background knowledge, it would be reasonable to suppose that somebody rolled the dice many times until the all-sixes configuration occurred. Without our background knowledge, we get the Inverse Gambler’s Fallacy.↩︎\nI think this is the \\(10^{500}\\) number I always hear about.↩︎\nThere is also some motivation from outside of physics, from the modal realism of David Lewis, but I find that view to be crazy.↩︎"
  },
  {
    "objectID": "posts/2024-10-13_impact/index.html",
    "href": "posts/2024-10-13_impact/index.html",
    "title": "The impact of phase space correlations on the beam dynamics in linear accelerators",
    "section": "",
    "text": "Almost one year ago, CERN hosted the “68th ICFA Advanced Beam Dynamics Workshop on High-Intensity and High-Brightness Hadron Beams”, also known as “HB”. I gave a talk on our work at the SNS Beam Test Facility (BTF). I intended to share that work on this blog, but never got around to it. Better late than never:"
  },
  {
    "objectID": "posts/2024-10-13_impact/index.html#the-initial-beam",
    "href": "posts/2024-10-13_impact/index.html#the-initial-beam",
    "title": "The impact of phase space correlations on the beam dynamics in linear accelerators",
    "section": "The initial beam",
    "text": "The initial beam\nI usually start these talks by reviewing the LEDA (Low Energy Test Accelerator) experiment at Los Alamos National Laboratory  [1]. LEDA was a proton source followed by 50 quadrupole magnets arranged in an alternating focus-defocus pattern (FODO). A series of experiments recorded the one-dimensional particle distribution at the end of the accelerator for different sets of quadrupole strengths (optics). In some cases, simulations reproduced the measurements in regions of high particle density, i.e., the beam core. But no simulation reproduced the low-density “tails” or “halo”.\n\n\n\n\n\n\nFigure 1: Measured and simulated beam profiles in the LEDA experiment  [2].\n\n\n\nHalo formation is driven by space charge (electric forces between particles). Thus, the discrepancies might have been due to an inaccurate initial distribution in 6D phase space. (They only estimated second-order moments.) On the other hand, errors in the accelerator model might have been to blame. Reproducing halo-level features requires modeling a delicate interplay between applied and self-generated fields.\nAt the BTF, we’re trying to improve on LEDA’s results. Our primary advantage is a suite of novel phase space diagnostics. One of these diagnostics images the 6D phase space distribution at the beginning of the lattice  [3]; another diagnostic measures the beam halo in 2D phase space at the end of the lattice  [4]. In the first half of my postdoc, I focused on measurements of the distribution at both locations; see conference reports here and here. We observed major differences between our predictions and measurements. One culprit may have been the paperclip layout of the beamline: bending 180 degrees generated dispersion, and we were unsure if we were modeling the dipole magnets correctly. In any case, linacs do not typically bend, so we upgraded the BTF to a new straight layout. This took several months.\nDuring the upgrade, I examined a question that could be partially answered by computer simulations: How important is the initial 6D phase space structure? In other words, if we ignore correlations between some dimensions, how will this affect the beam dynamics? An easy way to answer this question is to use the measured 6D distribution. However, 6D measurements currently have low resolution (\\(\\approx 10^6\\) points) and low dynamic range (\\(\\approx 10^1\\)); it’s unclear if these values are sufficient to predict halo-level features.\nThere’s another way to generate the initial beam which avoids direct 6D measurements. The initial beam is not really the initial beam; it’s the beam at the location labeled “First Emittance Station” in Figure 2.\n\n\n\n\n\n\nFigure 2: The SNS Beam Test Facility (BTF).\n\n\n\nThe beam emerges from the source as a continuous stream of ions. The RFQ (Radio Frequency Quadrupole) accelerates the beam to 2.5 MeV and converts the continuous stream to a train of bunches. Before the RFQ, the longitudinal distribution is spatially uniform with a tiny energy spread; thus, defining the initial beam would only require a 4D (transverse) phase space measurement. However, we would then need to simulate the journey through the RFQ, which involves complex dynamics over hundreds of focusing periods with strong space charge. So, we opted for the more difficult 6D measurement after the RFQ.\nStill, we could try this approach. We don’t have any diagnostics before the RFQ (there’s no room), but we do have 2D diagnostics at a dedicated Ion Source Test Stand. The ion source is different than the one in BTF, but it should produce a similar distribution. We took some old measurement data of a 50 mA beam in the Ion Source Test Stand and tracked it through the RFQ. The RFQ code (PARMTEQ) predicted a 42 mA beam current, but the real RFQ generated 26 mA. That’s a huge unexplained discrepancy! We’re still unsure what caused this because we can’t peer inside the RFQ. Unphased, we artificially changed the beam current to 26 mA in the simulation and tracked it to the first measurement station.\n\n\n\n\n\n\nFigure 3: PARMTEQ model bunch generation. An initial 2D measurement is transported through the RFQ using PARMTEQ and through the first section of the BTF using PyORBIT."
  },
  {
    "objectID": "posts/2024-10-13_impact/index.html#parmteq-vs.-reality",
    "href": "posts/2024-10-13_impact/index.html#parmteq-vs.-reality",
    "title": "The impact of phase space correlations on the beam dynamics in linear accelerators",
    "section": "PARMTEQ vs. reality",
    "text": "PARMTEQ vs. reality\nThe PARMTEQ simulations generate a fully correlated 6D bunch without a direct measurement. The tradeoff is that we’re unsure how realistic this distribution is. We know PARMTEQ gets the basic physics correct, but the model contains various approximations, such as an assumed cylindrical symmetry when solving the Poisson equation. The only way to check is via direct measurements.\nA previous paper found reasonable agreement in high-dimensional slices  [5]; I set out to perform a more comprehensive comparison. A year before HB, we performed 5D measurements of the initial beam, mapping the density as a function of \\(x\\), \\(p_x\\), \\(y\\), \\(p_y\\), and \\(p_z\\)  [6]. The missing dimension, \\(z\\), is strongly correlated with \\(p_z\\), so most features are visible from the five measured variables. 5D measurements are much faster than 6D measurements because we can image two dimensions at once on a screen (and because there is one less dimension to measure). Because of the boosted resolution and dynamic range, we can visualize sharper features in low-density regions of phase space.\nHere are the 1D and 2D projections of the measured and predicted (PARMTEQ) distributions. Note that I use \\(x' = p_x / p_z\\) and \\(y' = p_y / p_z\\) for the transverse momentum. I also use \\(w\\) instead of \\(p_z\\), where \\(w = E - E_0\\) is the deviation from the design energy.\n\n\n\n\n\n\nFigure 4: Measured (black) and predicted (red) phase space distributions in the BTF.\n\n\n\nThese contours are on a logarithmic scale, showing three orders of magnitude in density. It’s not a total disaster, but it’s kinda bad. Particularly troublesome is the \\(x\\)-\\(p_x\\) distribution, which is much wider than measured. However, look what happens after a linear transformation:\n\n\n\n\n\n\nFigure 5: Normalized phase space distribution.\n\n\n\nMuch better! All I did was normalize both distributions to identity covariance so that\n\\[\\begin{equation}\n\\Sigma =\n\\begin{bmatrix}\n\\langle xx   \\rangle & \\langle xp_x   \\rangle & \\langle xy   \\rangle & \\langle xp_y   \\rangle & \\langle x p_z  \\rangle \\\\\n\\langle xp_x \\rangle & \\langle p_xp_x \\rangle & \\langle yp_x \\rangle & \\langle p_xp_y \\rangle & \\langle p_xp_z \\rangle \\\\\n\\langle xy   \\rangle & \\langle yp_x   \\rangle & \\langle yy   \\rangle & \\langle yp_y   \\rangle & \\langle y p_z  \\rangle \\\\\n\\langle xp_y \\rangle & \\langle p_xp_y \\rangle & \\langle yp_y \\rangle & \\langle p_yp_y \\rangle & \\langle p_yp_z \\rangle \\\\\n\\langle xp_z \\rangle & \\langle p_xp_z \\rangle & \\langle yp_z \\rangle & \\langle p_yp_z \\rangle & \\langle p_zp_z \\rangle\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 1\n\\end{bmatrix}\n\\end{equation}\\]\nFrom now on, I’ll let \\(x\\), \\(p_x\\), \\(y\\), \\(p_y\\), and \\(p_z\\) refer to these “normalized” coordinates. Figure 5 shows that the distributions are similar, up to a linear transformation. PARMTEQ has gotten the nonlinear stuff right, even though it predicts a much larger transmission than measured in the real RFQ. This suggests that losses occur very early in the bunch formation process and have little impact on the higher-order distribution moments.\nFigure 5 does not prove that the distributions are identical in the full five-dimensional space. There are two higher-dimensional correlations we’ve identified in our 5D measurement. First, we know the energy distribution (\\(p_z\\)) depends on the transverse coordinates. The energy distribution is bimodal near the transverse core but unimodal outside the core. You can’t see this in the 1D and 2D projections. One way to visualize the relationship is to use “elliptical slices”. Figure 6 shows the \\(p_z\\) distribution of particles within a radial band in \\(x\\)-\\(p_x\\)-\\(y\\)-\\(p_y\\) space. The model bunch has the correct relationship between energy and transverse radius.\n\n\n\n\n\n\nFigure 6: Energy distribution within ellipsoidal shells in the transverse plane.\n\n\n\nThe above feature develops somewhere in the RFQ. We’ve also measured hollowing in the 3D space \\(x\\)-\\(y\\)-\\(z\\). This feature develops after the RFQ. As the beam transitions from strong to weak focusing at the RFQ exit, space charge launches a density wave that flattens the initially peaked distribution to a more uniform and, eventually, hollow distribution. The beam freely expands in the longitudinal plane, creating a strong linear correlation such that (\\(z \\approx p_z\\)). Again, we see similar features in the model bunch in Figure 7. We conclude that the RFQ simulation reproduces nearly all measured features in both low-dimensional and high-dimensional phase space, up to a linear transformation.\n\n\n\n\n\n\nFigure 7: Transverse charge distribution (\\(x\\)-\\(y\\)) as a function of energy (\\(p_z\\)).\n\n\n\nThis result is interesting and not too obvious; I’d like to include it in a future publication alongside a more detailed study of the beam dynamics in the RFQ. The result is also useful because linear correlations are easy to measure. We can just map the PARMTEQ bunch to the measured linear correlations, and, voila, we have a fully correlated 6D distribution that should be very similar to the real distributioin."
  },
  {
    "objectID": "posts/2024-10-13_impact/index.html#how-important-are-6d-correlations-in-the-sns-linac",
    "href": "posts/2024-10-13_impact/index.html#how-important-are-6d-correlations-in-the-sns-linac",
    "title": "The impact of phase space correlations on the beam dynamics in linear accelerators",
    "section": "How important are 6D correlations in the SNS linac?",
    "text": "How important are 6D correlations in the SNS linac?\nWe can also use the model bunch to examine what could happen in the SNS linac. Predicting beam loss in the SNS is our ultimate goal. This is much more difficult than our task in the BTF: the SNS is around 500 meters long compared to 10 meters in the BTF; the SNS accelerates the beam using hundreds of RF cavities, while the BTF has no acceleration; we are much less certain about the accelerator parameters; etc. Figure 8 gives a sense of scale. Still, there is nothing stopping us from assuming, for the sake of argument, that our linac model is correct. We can hypothesize that our physics model is a reasonable approximation of the real world. For example, most people believe the particle-in-cell (PIC) method captures the effect of space charge. We can also imagine a world in which the linac parameters — quadrupole strengths, rf cavity phases, etc. — are equal to the simulation parameters. Finally, in this hypothetical world, we can assume the initial distribution is a linear transformation of the “model” bunch generated by PARMTEQ.\n\n\n\n\n\n\nFigure 8: Diagram of the SNS accelerator. The linac is around 400 meters long.\n\n\n\nWe can now ask how changes to the initial bunch propagate down in the linac. We’re particularly interested in cross-plane correlations. What if we only knew the projected density onto each 2D phase space: \\(f(x, p_x)\\), \\(f(y, p_y)\\), and \\(f(z, p_z)\\)? Without additional information, the only logical 6D distribution compatible with these projections is the product of the 2D distributions (the distribution that maximizes entropy):\n\\[\n\\begin{equation}\nf(x, p_x, y, p_y, z, p_z) = f(x, p_x) f(y, p_y) f(z, p_z).\n\\end{equation}\n\\tag{1}\\]\nWe’ll refer to the bunch in Equation 1 as decorrelated. All cross-plane correlations, including higher-order correlations, vanish in the decorrelated bunch. To decorrelated the bunch, we simply shuffle the particle indices:\n\\[\n\\begin{equation}\n\\begin{aligned}\n    \\{ x_i, {p_x}_i \\} &\\rightarrow \\{ x_i, {p_x}_i \\}, \\\\\n    \\{ y_i, {p_y}_i \\} &\\rightarrow \\{ y_j, {p_y}_j \\}, \\\\\n    \\{ z_i, {p_z}_i \\} &\\rightarrow \\{ z_k, {p_z}_k \\},\n\\end{aligned}\n\\end{equation}\n\\tag{2}\\]\nwhere \\(i\\), \\(j\\), and \\(k\\) are random permutations of the indices.\nWe tracked these two bunches (correlated and decorrelated) through the first section of the linac in our PyORBIT model and compared the trajectories. This isn’t new: my colleague did this in 2021, finding that the two beams diverged in their rms sizes (Figure 9).\n\n\n\n\n\n\nFigure 9: RMS beam size evolution for a correlated/decorrelated initial beam in the SNS linac. Presented at IPAC (2021).\n\n\n\nI planned to continue these studies and find out exactly how the 6D phase space correlations influenced the beam dynamics, especially halo formation. When I reproduced Figure 9, though, I noticed something strange. The rms bunch length (\\(z\\)) spiked halfway through the simulation.\n\n\n\n\n\n\nFigure 10: Some particles are lagging behind the bunch…\n\n\n\nThe phase space distributions at these locations showed that some particles were falling way behind the bunch. Eventually, transverse apertures removed these particles from the simulation. It doesn’t make sense to keep these particles in the bunch, so I added energy and phase apertures throughout the lattice to remove particles as soon as they deviated from the synchronous coordinates. Longitudinal apertures ensured a well-behaved rms bunch length, but I no longer saw any differences in the transverse bunch sizes.\n\n\n\n\n\n\nFigure 11: RMS beam sizes after adding longitudinal apertures. Compare to Figure 9.\n\n\n\nWhat’s going on? It turns out that the lost particles were affecting the space charge calculation. To compute the beam’s electric field, we solve the Poisson equation on a grid:\n\\[\\begin{equation}\n\\nabla \\cdot \\nabla \\phi(x, y, z) = \\frac{ \\rho(x, y, z) } {\\epsilon_0},\n\\end{equation}\\]\nwhere \\(\\phi\\) is the electric potential and \\(\\rho\\) is the charge density. In PyORBIT, the grid expands to include all particles. In Figure 10, the grid would expand in the middle plot to include the \\(z\\) coordinates behind the bunch. This would leave almost all particles in one \\(z\\) bin, giving an inaccurate charge density and space charge forces.\nSo that’s settled. But it also raises the question: do cross-plane correlations matter at all? Figure 11 shows that the one-dimensional beam density is independent of the initial cross-plane correlations, even at the level of beam halo. Thus the answer is no: according to our model, cross-plane correlations do not affect the beam dynamics in the SNS. The more detailed view in Figure 12 shows that the differences between the distributions disappear quickly, within a few meters. You can see the hollow initial \\(z\\) distribution in the 1D lineout compared to the peaked \\(z\\) distribution; this represents a correlation because the distribution is only hollow near \\(x = y = p_x = p_y = 0\\). The lineouts merge soon after acceleration begins. The distributions are both highly correlated by the end of the figure.\n\n\n\n\n\n\nFigure 12: Longitudinal phase space evolution in the linac. Each plot is the longitudinal phase space (\\(z\\)-\\(p_z\\)) distribution within a 4D ball in the transverse plane. 1D lineouts onto the \\(z\\) axis are plotted on the bottom row. Blue = correlated, red = decorrelated initial bunch."
  },
  {
    "objectID": "posts/2024-10-13_impact/index.html#conclusion",
    "href": "posts/2024-10-13_impact/index.html#conclusion",
    "title": "The impact of phase space correlations on the beam dynamics in linear accelerators",
    "section": "Conclusion",
    "text": "Conclusion\nI think these findings are significant. It appears that measuring three orthogonal 2D projections of the 6D distribution can lead to the same predictions as direct 6D measurements. If this is true, we’ll need to direct our attention to the accelerator and physics models in our simulations. Of course, all of this needs experimental validation. Measurements at the BTF will serve as important benchmarks.\nHere is the conference paper and slides. Here are the full conference proceedings.\n\n\n\nFigure 1: Measured and simulated beam profiles in the LEDA experiment  [2].\nFigure 2: The SNS Beam Test Facility (BTF).\nFigure 3: PARMTEQ model bunch generation. An initial 2D measurement is transported through the RFQ using PARMTEQ and through the first section of the BTF using PyORBIT.\nFigure 4: Measured (black) and predicted (red) phase space distributions in the BTF.\nFigure 5: Normalized phase space distribution.\nFigure 6: Energy distribution within ellipsoidal shells in the transverse plane.\nFigure 7: Transverse charge distribution (\\(x\\)-\\(y\\)) as a function of energy (\\(p_z\\)).\nFigure 8: Diagram of the SNS accelerator. The linac is around 400 meters long.\nFigure 9: RMS beam size evolution for a correlated/decorrelated initial beam in the SNS linac. Presented at IPAC (2021).\nFigure 10: Some particles are lagging behind the bunch…\nFigure 11: RMS beam sizes after adding longitudinal apertures. Compare to Figure 9.\nFigure 12: Longitudinal phase space evolution in the linac. Each plot is the longitudinal phase space (\\(z\\)-\\(p_z\\)) distribution within a 4D ball in the transverse plane. 1D lineouts onto the \\(z\\) axis are plotted on the bottom row. Blue = correlated, red = decorrelated initial bunch."
  },
  {
    "objectID": "posts/2021-01-21_parametric-oscillators/index.html",
    "href": "posts/2021-01-21_parametric-oscillators/index.html",
    "title": "Parametric oscillators",
    "section": "",
    "text": "This post presents the solution to a general problem: the motion of a particle in one dimension in the presence of time-dependent, linear, periodic forces. This amounts to solving the following equation of motion:\n\\[\n\\frac{d^2x}{dt^2} + k(t)x = 0,\n\\tag{1}\\]\nwhere \\(k(t + T) = k(t)\\) for some \\(T\\). This is a parametric oscillator, a harmonic oscillator whose physical properties are not static. For example, the oscillations of a pendulum (in the small angle approximation) on the surface of a planet whose gravitational pull varies periodically would be described by the above equation. The solution to this equation was derived by George William Hill in 1886 to study lunar motion, and for this reason, it is known as Hill’s equation. It also finds application in areas such as condensed matter physics, quantum optics, and accelerator physics. After setting up the physical problem, we will examine the solutions and discuss their relevance to the last application, accelerator physics."
  },
  {
    "objectID": "posts/2021-01-21_parametric-oscillators/index.html#particle-accelerators-as-parametric-oscillators",
    "href": "posts/2021-01-21_parametric-oscillators/index.html#particle-accelerators-as-parametric-oscillators",
    "title": "Parametric oscillators",
    "section": "Particle accelerators as parametric oscillators",
    "text": "Particle accelerators as parametric oscillators\nParticle accelerators are machines that produce groups of charged particles (beams), increase their kinetic energy, and guide them to a target. These machines are invaluable to modern scientific research. The most famous examples are colliders, such as the LHC, in which two beams are smashed together to generate fundamental particles.\n\n\n\nFig. 1. A detector at an interaction point in the LHC.\n\n\nA lesser known fact is that the fields of condensed matter physics, material science, chemistry, and biology also benefit tremendously from accelerators; this is due to the effectiveness of scattering experiments in which the deflection of a beam after colliding with a target is used to learn information about the target. The scattered beam is composed of neutrons in spallation neutron sources such as SNS, electrons in electron scattering facilities such as CEBAF, or photons in synchrotron light sources such as APS. In addition to scientific research, accelerators find use in medicine, particularly for cancer treatment, and also in various industrial applications.\nThere are generally a few beam properties that are very important to experimentalists; in colliders, it is the energy and luminosity, in spallation sources, it is the intensity, and in light sources, it is the brightness. There is thus a constant need to push these parameters to new regions. For example, below is the famous Livingston plot which shows the energy achieved by various machines over the past century.\n\n\n\nFig. 2. Accelerator energy over time. The vertical axis is not the center of mass energy (CME) — it is the beam energy needed to produce the CME by collision with a resting proton. (source: Rasmus Ischebeck).\n\n\nThere are many physics issues associated with the optimization of these beam parameters. Accelerator physics is a field of applied physics that studies these issues. The task of the accelerator physicist is to understand, control, and measure the journey of the beam from its creation to its final destination. The difficulty of this task has grown over time; the improvement in accelerator performance has brought with it a staggering increase in size and complexity. The construction and operation of modern accelerators generally require years of planning, thousands of scientists and engineers, and hundreds of millions or even billions of dollars. Despite this complexity, the underlying physics principles are quite simple, and the single particle motion in one of these machines can be understood analytically if a few approximations are made. In the end, we will arrive at Hill’s equation.\nThere are three basic tasks an accelerator has to accomplish. First, it must increase the beam energy (acceleration). Second, it must guide the beam along a predetermined path (steering). Third, it must ensure the beam particles remain close together (focusing). It is helpful to use a coordinate system in which the s axis points along the design trajectory, and the x and y axes defined in the plane transverse to s. In this way, the motion is broken up into transverse and longitudinal dynamics.\n\n\n\nFig. 3. Curvilinear coordinate system used to describe particle motion along a reference trajectory.\n\n\nHow are these tasks accomplished? Well, particles are charged, and the force on a point charge in an electromagnetic field is given by\n\\[\n\\mathbf{F} = q\\left({\\mathbf{E} + \\mathbf{v} \\times \\mathbf{B}}\\right),\n\\tag{2}\\]\nwhere \\(q\\) is the particle charge, \\(\\mathbf{v}\\) is the particle velocity, \\(\\mathbf{E}\\) is the electric field, and \\(\\mathbf{B}\\) is the magnetic field. An accelerator consists of a series of elements, each with their own \\(\\mathbf{E}\\) and \\(\\mathbf{B}\\); the collection of these elements is called a lattice. We need to determine which electric and magnetic fields to use.\nThe first task, acceleration, is not the focus of this post. The remaining tasks, steering and focusing, concern the motion in the transverse plane. \\(\\mathbf{B}\\) fields, not \\(\\mathbf{E}\\) fields, are used since their effect grows with increased particle velocity. Any transverse magnetic field \\(\\mathbf{B} = (B_x, B_y)^T\\) can be written using a multipole expansion\n\\[\nB_y - iB_x = \\sum_{n=1}^{\\infty}\\left({b_n - ia_n}\\right)\\left(\\frac{x + iy}{r_0}\\right)^{n-1}.\n\\tag{3}\\]\nWe then have the normal multiple coefficients \\(\\{b_n\\}\\), and the skew multipole coefficients \\(\\{a_n\\}\\). The field lines corresponding to the first few terms are shown below.\n\n\n\nFig. 4. Multipole expansion of the magnetic field.\n\n\nThe dipole field \\(\\mathbf{B} \\propto \\hat{y}\\) is perfect for steering, producing the force \\(\\mathbf{F} \\propto -\\hat{x}\\) for a particle moving into the page. The quadrupole field \\(\\mathbf{B} \\propto y\\hat{x} + x\\hat{y}\\) produces the force \\(\\mathbf{F}_{quad} \\propto -x\\hat{x} + y\\hat{y}\\), which is focusing in the horizontal direction, but defocusing in the vertical direction; however, net focusing can still be achieved by alternating the direction of the quadrupoles. This is analogous to a beam of light passing through a series of converging and diverging lenses. If the spacing and curvature of the lenses are correctly chosen, net focusing can be achieved.\n\n\n\nFig. 5. Single-particle motion in the thin-lense approximation.\n\n\nThe forces which result from these fields are linear, meaning they are proportional the \\(x\\) or \\(y\\) but not \\(x^2\\), \\(y^3\\), etc., and they are uncoupled, meaning the dynamics in the \\(x\\) and \\(y\\) dimensions are independent. Now, we may ask, can we produce a perfect dipole or quadrupole field? The answer is no. In reality, there will always be higher order multipoles present in the field, but people work very hard to ensure these are much smaller than the desired multipole. This video shows a bit of the construction process for these magnets.\nFor small oscillations, the equations of motion reduce to\n\\[\n\\begin{aligned}\n    x'' \\approx -\\frac{q}{mc \\beta_s \\gamma_s} B_y(x, y, s), \\\\\n    y'' \\approx +\\frac{q}{mc \\beta_s \\gamma_s} B_x(x, y, s),\n\\end{aligned}\n\\tag{4}\\]\nwhere \\(x' = dx/ds\\), \\(m\\) is the particle mass, \\(c\\) is the speed of light in a vacuum, \\(\\beta_s\\) is the particle speed divided by \\(c\\), and \\(\\gamma_s = (1 - \\beta_s^2)^{-1/2}\\). (For simplicity, the curved coordinate system has not been taken into account). We will ignore nonlinear terms since they greatly complicate the dynamics. We will also ignore coupling between the planes. With these approximations, we arrive at the equation of motion for a single particle in the transverse plane:\n\\[\nx'' + k(s)x = 0.\n\\tag{5}\\]\nThis is Hill’s equation (Equation 1)."
  },
  {
    "objectID": "posts/2021-01-21_parametric-oscillators/index.html#solving-hills-equation",
    "href": "posts/2021-01-21_parametric-oscillators/index.html#solving-hills-equation",
    "title": "Parametric oscillators",
    "section": "Solving Hill’s equation",
    "text": "Solving Hill’s equation\nThe general solution to Hill’s equation is given by\n\\[\nx(s) = \\sqrt{2J}\\,w(s) \\cos\\left({\\mu(s) + \\delta}\\right).\n\\tag{6}\\]\nThis introduces an amplitude \\(w(s) = w(s + L)\\) which we call the envelope function, as well as a phase \\(\\mu\\), both of which depend on \\(s\\). The constants \\(J\\) and \\(\\delta\\) are determined by the initial conditions. Let’s plot this trajectory in a FODO (focus-off-defocus-off) lattice, which consists of evenly spaced focusing and defocusing quadrupoles. Here is the focusing strength within the lattice (QF is the focusing quadrupole and QD is the defocusing quadrupole):\n\n\n\nFig. 6. FODO lattice.\n\n\nFor now we can think of the lattice as repeating itself forever in the \\(s\\) direction. Each black line below is represents the trajectory for a different initial position and slope; although the individual trajectories look rather complicated, the envelope function has a very simple form.\n\n\n\nFig. 7. Envelope function and particle trajectories.\n\n\nThe particle motion becomes much easier to interpret if we observe it in position-momentum space, aka phase space. The following animation shows the evolution of the particle phase space coordinates at a single position in the lattice. The position shown is \\(s = nL/4\\), where \\(n\\) is the period number, which corresponds to the midpoint between the focusing and defocusing quadrupoles.\n\n\n\nFig. 8. Phase space coordinates after every focusing period.\n\n\nThe particle jumps around an ellipse in phase space. The shape and orientation of the ellipse will change if we look at a different position in the lattice, but its area will be the same. So, the motion is determined by the dimensions and oriention of this ellipse throughout the lattice, as well as the location of the particle on the ellipse boundary. This motivates the definition of the so-called Twiss parameters, which were first introduced by Courant and Snyder in 1958:\n\\[\n\\beta = w^2, \\quad \\alpha = -\\frac{1}{2}\\beta', \\quad \\gamma = \\frac{1 + \\alpha^2}{\\beta}.\n\\tag{7}\\]\nThe dimensions of the phase space ellipse are nicely described by these parameters:\n\n\n\nFig. 9. Courant-Snyder ellipse.\n\n\nThe maximum extent of the ellipse is determined by \\(\\beta\\) in the \\(x\\) direction and \\(\\gamma\\) in the \\(y\\) direction. \\(\\alpha\\) is proportional to the slope of the \\(\\beta\\) function, and so determines the tilt angle of the ellipse. The position of a particle on the ellipse is given by the phase \\(\\mu\\). Finally, the invariant of the motion corresponding to the ellipse area is proportional to \\(2J = \\beta {x'}^2 + 2\\alpha xx' + \\gamma x^2\\) for any \\(x\\) and \\(x'\\). The \\(\\beta\\) functions and phase advances in both dimensions are extremely important to measure and control in a real machine.\nA helpful tool to pair with the parameterization we just introduced is the transfer matrix, a matrix which connects the phase space coordinates at two different positions:\n\\[\n\\begin{bmatrix} x \\\\ x' \\end{bmatrix}_{s + L}\n= \\mathbf{M}\\begin{bmatrix} x \\\\ x' \\end{bmatrix}_{s}\n\\tag{8}\\]\nThe transfer matrix can be written as \\(\\mathbf{M} = \\mathbf{V}\\mathbf{P}\\mathbf{V}^{-1}\\), where\n\\[\\mathbf{V} = \\frac{1}{\\sqrt{\\beta}} \\begin{bmatrix} \\beta & 0 \\\\ -\\alpha & 1 \\end{bmatrix}\\] and \\[\n\\mathbf{P} = \\begin{bmatrix}\n  \\cos\\mu & \\sin\\mu \\\\ -\\sin\\mu & \\cos\\mu\n\\end{bmatrix}\n\\]\nThe effect of \\(\\mathbf{V}^{-1}\\) is to deform the phase space ellipse into a circle while preserving its area. \\(\\mathbf{P}\\) is then just a rotation in phase space, and \\(\\mathbf{V}\\) then transforms back into a tilted ellipse. This is illustrated below. \\(\\mathbf{V}\\) can be thought of as a time-dependent transformation which removes the variance in the focusing strength, turning the parametric oscillator into a simple harmonic oscillator. It is often called the Floquet transformation.\n\n\n\nFig. 10. Effect of the Floquet transformation on the Courant-Snyder ellipse."
  },
  {
    "objectID": "posts/2021-01-21_parametric-oscillators/index.html#conclusion",
    "href": "posts/2021-01-21_parametric-oscillators/index.html#conclusion",
    "title": "Parametric oscillators",
    "section": "Conclusion",
    "text": "Conclusion\nWe’ve presented the solution to Hill’s equation, which describes a parameteric oscillator. The equation pops up in multiple areas, but we focused on its application in accelerator physics, in which Hill’s equation describes the transverse motion of a single particle in an accelerator with perfectly linear magnetic fields.\nThe solution is best understood geometrically: particles move around the surface of an ellipse in phase space, the area of which is an invariant of the motion. The dimensions and orientation of the ellipse are determined by \\(\\alpha\\) and \\(\\beta\\), and the location of the particle on the ellipse boundary is determined by \\(\\mu\\). These parameters can be used to construct a time-dependent transformation (\\(\\mathbf{V}\\)) which turns the parametric oscillator into a simple harmonic oscillator.\nThe next post will examine how this treatment can be extended to include coupling between the horizontal and vertical dimensions.\n\n\n\nFig. 1. A detector at an interaction point in the LHC.\nFig. 2. Accelerator energy over time. The vertical axis is not the center of mass energy (CME) — it is the beam energy needed to produce the CME by collision with a resting proton. (source: Rasmus Ischebeck).\nFig. 3. Curvilinear coordinate system used to describe particle motion along a reference trajectory.\nFig. 4. Multipole expansion of the magnetic field.\nFig. 5. Single-particle motion in the thin-lense approximation.\nFig. 6. FODO lattice.\nFig. 7. Envelope function and particle trajectories.\nFig. 8. Phase space coordinates after every focusing period.\nFig. 9. Courant-Snyder ellipse.\nFig. 10. Effect of the Floquet transformation on the Courant-Snyder ellipse."
  },
  {
    "objectID": "posts/2024-08-12_usmcc/index.html",
    "href": "posts/2024-08-12_usmcc/index.html",
    "title": "Inaugural US Muon Collider Collaboration meeting",
    "section": "",
    "text": "The US Muon Collider Collaboration (IMCC) held their first meeting at Fermilab last week. I couldn’t attend, but I heard it was an inspiring conference. Looks like it was around 80% particle physics, 20% accelerator physics. All the presentation slides are online here."
  },
  {
    "objectID": "posts/2025-03-20_appling-6d-ment-to-data/index.html",
    "href": "posts/2025-03-20_appling-6d-ment-to-data/index.html",
    "title": "Applying 6D MENT to data",
    "section": "",
    "text": "Last year I submitted a paper called N-dimensional maximum-entropy tomography via particle sampling. (See this post.) The idea was to use Markov Chain Monte Carlo (MCMC) sampling techniques to approximate high-dimensional integrals that appear in the MENT algorithm. I demonstrated the method on a toy problem, reconstructing a 6D Gaussian Mixture (GM) distribution from its pairwise projections.\n\n\n\n6D Gaussian Mixture reconstruction. Shown are the 2D marginal distributions of the true and reconstructed distribution. (Click to enlarge.)\n\n\nI finally got around to submitting a new version. I added an example where I applied MENT to data from the Argonne Wakefield Accelerator (AWA). The data set is described in this paper by Roussel et al. and is shared in an open-source repository. It contains 36 measured images of an electron beam in the AWA beamline, each with a different set of accelerator parameters.\nThe accelerator begins with a series of quadrupole magnets, each of which provides linear focusing in one transverse plane (\\(x\\)) and defocusing in the other plane (\\(y\\)). The quadrupoles are followed by a transverse deflecting cavity (TDC). The TDC generates a sinusoidal electric field that points mainly along the \\(y\\) axis. Particles receive a vertical momentum kick dependent on arrival time, rotating the longitudinal coordinate \\(z\\) into the vertical plane. The TDC is followed by a dipole bend, which couples the horizontal position \\(x\\) to the longitudinal momentum \\(p_z\\) because of momentum dispersion. There are two beamlines after the dipole so that the beam can be measured with the dipole on or off. Scanning the quadrupoles constrains the 4D transverse phase space density \\(\\rho(x, p_x, y, p_y)\\). Quadrupole scans were repeated with the TDC turned on/off, and then with the dipole turned on/off, generating different transverse-longitudinal correlations in the distribution at the measurement screen. A full 6D reconstruction is necessary to fit the data.\n\n\n\nImages from the AWA data set. Columns correspond to different quadrupole values.\n\n\nThe authors used a differential simulation to propagate particles through the accelerator and a differentiable kernel density estimator to compute the \\(x\\)-\\(y\\) projections. The differentiable forward model enables gradient-based optimization of the phase space distribution. A generative model was used to represent the distribution, and the model parameters were varied to minimize the pixel-wise error between measured and simulated images. This is called Generative Phase Space Reconstruction (GPSR). Very cool method!\nIt’s straightforward to apply MENT to arbitrary problems using this GitHub implementation. The key step is to create a list of functions that transform input phase space coordinates to output phase space coordinates, where the coordinates are passed as NumPy arrays. The transformations do not need to be differentiable. In this case, I just wrote a wrapper function that takes a NumPy array, transforms the NumPy array using the Bmad-X accelerator (the differentiable simulation), and then converts back to NumPy at the end. One complication is that it’s best to estimate the covariance matrix of the unknown phase space distribution. This is mostly for the MCMC sampling algorithm, which performs best on “round” distributions, i.e., distributions with roughly the same variance in every direction and no linear correlations between variables. I mention a few strategies to estimate the covariance matrix in the paper. In this example, I just assumed a known covariance matrix which I obtained from the previous GPSR fit.\nI used the Metropolis-Hastings (MH) algorithm to sample particles from the MENT distribution function. I chose MH because it does not require a differentiable target distribution and is simple to code. We start with a set of points at random locations \\(x_t\\) in the phase space. Then we sample another point \\(x_*\\) from a jumping distribution \\(q\\), which we take to be a Gaussian. The new point is accepted or rejected according to the following procedure: \\[\\begin{equation}\n    {x}_{t + 1} =\n    \\begin{cases}\n        {x}_*  & \\text{if}\\ r \\leq \\pi({x}_* | {x}_t),  \\\\\n        {x}_t & \\text{otherwise},\n    \\end{cases}\n\\end{equation}\\] where \\[\\begin{equation}\n    \\pi({x}_* | {x}_t) =\n    \\text{min}\n    \\left(\n        1,\n        \\frac{ \\rho({x}_*) }{ \\rho({x}_t) }\n        \\frac{ q({x}_t | {x}_*) }{ q({x}_* | {x}_t) }\n    \\right),\n\\end{equation}\\] where \\(r \\in [0, 1]\\) is a random number.\nMH converges to the target distribution in the long run, but its performance is highly dependent on the jumping distribution. Too wide, and most points will be rejected; too narrow, and it will take forever to explore the space. Although there’s a whole literature on tuning and diagnosing MCMC, I just tuned the width of the Gaussian jumping distribution until the acceptance rate was ~0.25-0.5 and checked that the projections/distribution didn’t look too strange. I sampled around \\(5 \\times 10^5\\) particles with 500-1000 chains, and all chains gave pretty much the same result, so I was fairly confident that the chains converged. Sampling this huge number of particles can be pretty fast using a vectorized MCMC implementation.\nHere are the measured vs. simulated projections of the 6D distribution after a few MENT iterations:\n\n\n\nSimulated vs. measured images. (Click to enlarge.)\n\n\nNot bad! I don’t see any major errors. Here are the 1D and 2D marginal projections of the reconstructed 6D MENT distribution compared to GPSR:\n\n\n\n\n\n\n\n\n\n\nMENT (click to enlarge)\n\n\n\n\n\n\n\nGPSR (click to enlarge)\n\n\n\n\n\n\nThe plots look similar, with the biggest differences in the longitudinal projections, where MENT is smoother. This is some evidence that the problem is well-constrained. The fit errors are small for both models. Here are blurred histograms of 90,000 particles sampled from the MENT and GPSR reconstructions compared to the measured histograms.\n\n\n\nMENT vs. GPSR predicted images.\n\n\nI know the MCMC algorithm is not perfect, so I’m not completely convinced that all features in the MENT distribution are suggested by the data. In other words, the entropy may be slightly lower than maximum due to unconverged MCMC chains. The longitudinal distribution is very thin, which I would expect to give MH trouble. However, the distribution matches the data and has a slightly higher entropy than the GPSR reconstruction, so I think I got reasonably close to the entropy maximum. There’s room to explore other sampling methods or improvements to MH.\n\n\n\n6D Gaussian Mixture reconstruction. Shown are the 2D marginal distributions of the true and reconstructed distribution. (Click to enlarge.)\nImages from the AWA data set. Columns correspond to different quadrupole values.\nSimulated vs. measured images. (Click to enlarge.)\nMENT (click to enlarge)\nGPSR (click to enlarge)\nMENT vs. GPSR predicted images."
  },
  {
    "objectID": "posts/2021-02-22_pic/index.html",
    "href": "posts/2021-02-22_pic/index.html",
    "title": "Particle-in-cell simulation",
    "section": "",
    "text": "There are many beam physics simulation codes. A key component of these simulations is the inclusion of the electromagnetic interactions between particles in the beam, also known as space charge forces. One way to compute space charge forces is the particle-in-cell (PIC) method. This post implements a basic version of the PIC method in Python."
  },
  {
    "objectID": "posts/2021-02-22_pic/index.html#theoretical-model",
    "href": "posts/2021-02-22_pic/index.html#theoretical-model",
    "title": "Particle-in-cell simulation",
    "section": "Theoretical model",
    "text": "Theoretical model\nWe’ll use bunch to refer to a group of particles in three-dimensional (3D) space, and we’ll use a local cartesian coordinate system whose origin moves with the center of the bunch as shown below:\n\n\n\nFig. 1. Coordinate system defined in the beam rest frame.\n\n\nThe \\(s\\) coordinate specifies the position of the bunch in the accelerator, and the path can be curved. Now for a few assumptions and approximations. First, assume all particles in the bunch move at a constant velocity \\(\\beta c\\), where \\(c\\) is the speed of light. We then make the paraxial approximation. It’s conventional to use the slope \\(x' = dx/ds\\) instead of the velocity, and the paraxial approximation assumes this slope is very small. Usually we report this slope in milliradians since \\(tan\\theta \\approx \\theta\\) for small angles. Next we assume that the transverse (\\(x\\)-\\(y\\)) size of the bunch varies slowly along the \\(s\\) axis. If this is true and we look at the electric field in a transverse slice of the bunch, there won’t be much difference between the true field and the field of an infinitely long, uniform density cylinder. Our focus will be on the transverse dynamics of such a slice, so we’ll treat each “particle” as an infinite line of charge. The figure below illustrates this approximation.\n\n\n\nFig. 2. Coasting beam approximation. (Source: G. Franchetti, Space charge in circular machines, CERN Accelerator School Proceedings (2017). https://e-publishing.cern.ch/index.php/CYRSP/article/view/413)\n\n\nAnother approximation is to neglect any magnetic fields generated by the beam, which is again valid if the transverse velocities are very small relative to \\(\\beta c\\). All this being said, the equations of motion without any external forces, i.e., in free space, can be written as\n\\[\n\\mathbf{x}'' = \\frac{q}{mc^2\\beta^2\\gamma^3} \\mathbf{E},\n\\tag{1}\\]\nwhere \\(\\mathbf{x} = [x, y]^T\\) is the coordinate vector, \\(\\mathbf{E} = [E_x, E_y]^T\\) is the self-generated electric field, \\(m\\) is the particle mass, and \\(\\gamma = \\left({1 - \\beta^2}\\right)^{-1/2}\\). Let’s first address the factor \\(\\gamma^{-3}\\) in the equation of motion, which means that the space charge force goes to zero as the velocity approaches the speed of light. This is because parallel moving charges generate an attractive magnetic force which grows with velocity, completely cancelling the electric force in the limit \\(v \\rightarrow c\\).\n\n\n\nFig. 3. The magnetic force between parallel currents is attractive. (Source: OpenStax University Physics.)\n\n\nOne may ask: what about the rest frame in which there is no magnetic field? But special relativity says that electrogmagnetic fields change with reference frame. Using the transformations defined here, you can quickly prove that\n\\[\n\\mathbf{E}_{lab} = \\frac{\\mathbf{E}_{rest}}{\\gamma}.\n\\tag{2}\\]\nThis inverse relationship between velocity and the space charge force has real-life consequences. It tells us that space charge is important if 1) the beam is very intense, meaning there are many particles in a small area, or 2) the beam is very energetic, meaning it is moving extremely fast. For example, space charge can usually be ignored in electron beams, which move near the speed of light for very modest energies due to their tiny mass, but is significant in high-intensity, low-energy hadron accelerators such as FRIB, SNS, and ESS.\nWe should now address the difficulty in determining the evolution of this system: the force on a particle in an \\(n\\)-particle bunch depends on the positions of the other \\(n - 1\\) particles. The approach of statistical mechanics to this problem is to introduce a distribution function \\(f(\\mathbf{x}, \\mathbf{x}', s)\\) which gives the particle density at axial position \\(s\\) and phase space coordinates \\(\\mathbf{x}\\), \\(\\mathbf{x}'\\). The Vlasov-Poisson system of equations determines the evolution of \\(f\\) as long as we ignore collisions between particles:\n\\[\n\\frac{\\partial{f}}{\\partial{s}} +\n\\mathbf{x}'\\cdot \\frac{\\partial{f}}{\\partial{\\mathbf{x}}} +\n\\mathbf{x}'' \\cdot \\frac{\\partial{f}}{\\partial{\\mathbf{x}'}}\n= 0.\n\\tag{3}\\]\nWe know \\(\\mathbf{x''}\\) from Equation 1. The electric field is obtained from Poisson’s equation:\n\\[\n\\nabla \\cdot \\mathbf{E} = -\\nabla^2 \\phi = \\frac{\\rho}{\\varepsilon_0}.\n\\tag{4}\\]\nFinally, the transverse charge density \\(\\rho\\) is determined by\n\\[\n\\rho = q \\int{f dx'dy'}.\n\\tag{5}\\]\nAlthough these equations are easy to write down, they are generally impossible to solve analytically. We need to turn to a computer for help."
  },
  {
    "objectID": "posts/2021-02-22_pic/index.html#computational-method",
    "href": "posts/2021-02-22_pic/index.html#computational-method",
    "title": "Particle-in-cell simulation",
    "section": "Computational method",
    "text": "Computational method\nThe Vlasov equation could be solved directly, but this is difficult, especially in 2D or 3D. On the other end of the spectrum, the notion of a fluid in phase space could be abandoned and each particle could be tracked individually, computing the forces using direct sums. But this is infeasible with current hardware; the time complexity would by \\(O(n^2)\\), where \\(n\\) is the number of particles, and \\(n\\) may be on the order of \\(10^{14}\\). The particle-in-cell (PIC) method is a sort of combination of these two approaches. The idea is to track a group of macroparticles according to Equation 1, each of which represents a large number of real particles. The fields, however, are solved from Equation 4. The key step is transforming back and forth between a discrete and continuous representation of the bunch. The simulation loop for the PIC method is shown below.\n\n\n\n\nFig. 4. The particle-in-cell (PIC) loop.\n\n\n\nIn the next sections I will discuss each of these steps and implement them in Python. The hidden cell below shows all the imports needed to run the code.\n\n\nImports\nimport Cython\n%load_ext cython\n\nimport numpy as np\nfrom matplotlib import animation\nfrom matplotlib import pyplot as plt\nfrom matplotlib.lines import Line2D\nfrom matplotlib.patches import Ellipse\nimport proplot as pplt\nfrom scipy.fft import fft2\nfrom scipy.fft import ifft2\nfrom scipy.interpolate import RegularGridInterpolator\nfrom scipy.integrate import odeint\nfrom scipy.stats import truncnorm\nimport seaborn as sns \nfrom tqdm.notebook import trange \n\n\nLet’s first create a Bunch class, which is a simple container for the bunch coordinates.\n\nclass Bunch:\n    \"\"\"Container for four-dimensional phase space distribution.\n\n    Attributes\n    ----------\n    intensity : float\n        Number of physical particles in the bunch.\n    length : float\n        Length of the bunch [m].\n    mass, kin_energy : float\n        Mass [GeV/c^2], charge [C], and kinetic energy [GeV] per particle.\n    nparts : float\n        Number of macroparticles in the bunch.\n    X : ndarray, shape (nparts, 4)\n        Array of particle coordinates. Columns are [x, x', y, y']. Units are\n        meters and radians.\n    positions : ndarray, shape (nparts, 2):\n        Just the x and y positions (for convenience).\n    \"\"\"\n    def __init__(self, intensity=1e14, length=250., mass=0.938, kin_energy=1.0):\n        self.intensity, self.length = intensity, length\n        self.mass, self.kin_energy = mass, kin_energy\n        self.gamma = 1 + (kin_energy / mass) # Lorentz factor\n        self.beta = np.sqrt(1 - (1 / self.gamma)**2) # v/c\n        r0 = 1.53469e-18 # classical proton radius [m]\n        self.perveance = 2 * r0 * intensity / (length * self.beta**2 * self.gamma**3)\n        self.nparts = 0\n        self.compute_macrosize()\n        self.X, self.positions = None, None\n        \n    def compute_macrosize(self):\n        \"\"\"Update the macrosize and macrocharge.\"\"\"\n        self.macrosize = self.intensity // self.nparts if self.nparts &gt; 0 else 0\n                                \n    def fill(self, X):\n        \"\"\"Fill with particles. X is the 4D phase space coordinate array.\"\"\"\n        self.X = X if self.X is None else np.vstack([self.X, X])\n        self.positions = self.X[:, [0, 2]]\n        self.nparts = self.X.shape[0]\n        self.compute_macrosize()\n\n    def compute_extremum(self):\n        \"\"\"Get extreme x and y coorinates.\"\"\"\n        self.xmin, self.ymin = np.min(self.positions, axis=0)\n        self.xmax, self.ymax = np.max(self.positions, axis=0)\n        self.xlim, self.ylim = (self.xmin, self.xmax), (self.ymin, self.ymax)\n\n\nWeighting\nStarting from a group of macroparticles, we need to produce a charge density \\(\\rho_{i,j}\\) on a grid. The most simple approach is the nearest grid point (NGP) method, which, as the name suggests, assigns the full particle charge to the closest grid point. This is commonly called zero-order weighting; although it is very fast and easy to implement, it is not commonly used because it can lead to significant noise. A better method called cloud-in-cell (CIC) treats each particle as a rectangular, uniform density cloud of charge with dimensions equal to the grid spacing. A fractional part of the charge is assigned based on the fraction of the cloud overlapping with a given cell. This can be thought of as first-order weighting. To get a sense of what these methods are doing (in 1D), we can slide a particle across a cell and plot the resulting density of the cell at each position, thus giving an effective particle shape.\n\n\nCode\ndef shape_func(u, v, cell_width, method='NGP'):\n    S, diff = 0.0, np.abs(u - v)\n    if method.upper() == 'NGP':\n        S = 1.0 if diff &lt; (0.5 * cell_width) else 0.0\n    elif method.upper() == 'CIC':\n        S = 1.0 - diff / cell_width if diff &lt; cell_width else 0.0\n    return S / cell_width\n    \nfig, ax = pplt.subplots(figsize=(4.0, 1.5))\nxvals = np.linspace(-1.0, 1.0, 1000)\nfor i, method in enumerate(['NGP', 'CIC']):\n    densities = [shape_func(x, 0.0, 1.0, method) for x in xvals]\n    ax.plot(xvals, densities, color='black', ls=['-', ':'][i], label=method,)\nax.format(ylim=(0.0, 1.1), xlabel='($x - x_k) \\,/\\, \\Delta x$', ylabel='Density')\nax.legend(loc='r', ncols=1, framealpha=0.0)\nplt.close()\n\n\n\n\n\nFig. 5. Effective particle shape for Nearest-grid-point (NGP) and cloud-in-cell (CIC) weighting.\n\n\nThe NGP method leads to a discontinuous boundary while the CIC method leads to a continous boundary (but discontinous derivative). There are also higher order methods which lead to a smooth boundary, but I don’t cover those here.\nWe also need to perform the inverse operation: given the electric field at each grid point, interpolate the value at each particle position. The same method applies here. NGP just uses the electric field at the nearest grid point, while CIC weights the four nearest grid points. The following Grid class implements the CIC method. Notice that Cython is used in the for-loop in the distribute method. I couldn’t figure out a way to perform this operation with the loop, and in pure Python it took about 90% of the runtime for a single simulation step. Using Cython gave a significant performance boost.\n\n%%cython\nimport numpy as np\nfrom scipy.interpolate import RegularGridInterpolator\n\nclass Grid:\n    \"\"\"Class for 2D grid.\n\n    Attributes\n    ----------\n    xmin, ymin, xmax, ymax : float\n        Minimum and maximum coordinates.\n    Nx, Ny : int\n        Number of grid points.\n    dx, dy : int\n        Spacing between grid points.\n    x, y : ndarray, shape (Nx,) or (Ny,)\n        Positions of each grid point.\n    cell_area : float\n        Area of each cell.\n    \"\"\"\n    def __init__(self, xlim=(-1.0, 1.0), ylim=(-1.0, 1.0), size=(64, 64)):\n        self.xlim = xlim\n        self.ylim = ylim\n        (self.xmin, self.xmax) = self.xlim\n        (self.ymin, self.ymax) = self.ylim\n        self.size = size\n        (self.Nx, self.Ny) = size\n        self.dx = (self.xmax - self.xmin) / float(self.Nx - 1)\n        self.dy = (self.ymax - self.ymin) / float(self.Ny - 1)\n        self.cell_area = self.dx * self.dy\n        self.x = np.linspace(self.xmin, self.xmax, self.Nx)\n        self.y = np.linspace(self.ymin, self.ymax, self.Ny)\n        \n    def set_lims(self, xlim, ylim):\n        \"\"\"Set the min and max grid coordinates.\"\"\"\n        self.__init__(xlim=xlim, ylim=ylim, size=self.size)\n        \n    def zeros(self):\n        \"\"\"Create array of zeros with same size as the grid.\"\"\"\n        return np.zeros((self.size))\n\n    def distribute(self, positions):\n        \"\"\"Distribute points on the grid using the cloud-in-cell (CIC) method.\n        \n        Parameters\n        ----------\n        positions : ndarray, shape (n, 2)\n            List of (x, y) positions.\n            \n        Returns\n        -------\n        rho : ndarray, shape (Nx, Ny)\n            The value rho[i, j] gives the number of macroparticles in the i,j cell.\n        \"\"\"\n        # Compute area overlapping with 4 nearest neighbors (A1, A2, A3, A4)\n        ivals = np.floor((positions[:, 0] - self.xmin) / self.dx).astype(int)\n        jvals = np.floor((positions[:, 1] - self.ymin) / self.dy).astype(int)\n        ivals[ivals &gt; self.Nx - 2] = self.Nx - 2\n        jvals[jvals &gt; self.Ny - 2] = self.Ny - 2\n        x_i, x_ip1 = self.x[ivals], self.x[ivals + 1]\n        y_j, y_jp1 = self.y[jvals], self.y[jvals + 1]\n        _A1 = (positions[:, 0] - x_i) * (positions[:, 1] - y_j)\n        _A2 = (x_ip1 - positions[:, 0]) * (positions[:, 1] - y_j)\n        _A3 = (positions[:, 0] - x_i) * (y_jp1 - positions[:, 1])\n        _A4 = (x_ip1 - positions[:, 0]) * (y_jp1 - positions[:, 1])\n        # Distribute fractional areas\n        rho = self.zeros()\n        cdef double[:, :] rho_view = rho \n        cdef int i, j\n        for i, j, A1, A2, A3, A4 in zip(ivals, jvals, _A1, _A2, _A3, _A4):\n            rho_view[i, j] += A4\n            rho_view[i + 1, j] += A3\n            rho_view[i, j + 1] += A2\n            rho_view[i + 1, j + 1] += A1      \n        return rho / self.cell_area\n\n    def interpolate(self, grid_vals, positions):\n        \"\"\"Interpolate values from the grid using the CIC method.\n        \n        Parameters\n        ----------\n        positions : ndarray, shape (n, 2)\n            List of (x, y) positions.\n        grid_vals : ndarray, shape (n, 2)\n            Scalar value at each coordinate point.\n            \n        Returns\n        -------\n        int_vals : ndarray, shape (nparts,)\n            Interpolated value at each position.\n        \"\"\"\n        int_func = RegularGridInterpolator((self.x, self.y), grid_vals)\n        return int_func(positions)\n\n    def gradient(self, grid_vals):\n        \"\"\"Compute gradient using 2nd order centered differencing.\n        \n        Parameters\n        ----------\n        grid_vals : ndarray, shape (Nx, Ny)\n            Scalar values at each grid point.\n            \n        Returns\n        -------\n        gradx, grady : ndarray, shape (Nx, Ny)\n            The x and y gradient at each grid point.\n        \"\"\"\n        return np.gradient(grid_vals, self.dx, self.dy)\n\nIt should also be mentioned that the field interpolation method should be the same as the charge deposition method; if this is not true, it is possible for a particle to exert a force on itself! Let’s test the method with a \\(64 \\times 64\\) grid.\n\n\nCode\n# Create a 2D bunch.\nnp.random.seed(1)\nn_parts = 100000\n_X = np.random.normal(scale=1.0, size=(n_parts, 2))\nn_clusters = 4\nfor _ in range(n_clusters):\n    loc = np.random.uniform(-2.0, 2.0, size=2)\n    scale = np.random.uniform(0.5, 1.5, size=2)\n    _X = np.vstack([_X, np.random.normal(loc=loc, scale=scale, size=(n_parts, 2))])\n    \n_X = _X - np.mean(_X)\nidx, = np.where(np.sqrt(_X[:, 0]**2 + _X[:, 1]**2) &lt; 5.0)\n_X = _X[idx, :]\n_X = _X - np.mean(_X)\n\nX = np.zeros((_X.shape[0], 4))\nX[:, (0, 2)] = _X\n\nbunch = Bunch()\nbunch.fill(X)\nbunch.compute_extremum()\n\n# Distribute the particles on an x-y grid.\nn_bins = 64\ngrid = Grid(bunch.xlim, bunch.ylim, size=(n_bins, n_bins))\nrho = grid.distribute(bunch.positions) \n\n# Plot\nfig, axs = pplt.subplots(ncols=2, figwidth=7)\naxs.format(xlabel='x [mm]', ylabel='y [mm]')\n\nn_samp = 2000\nidx = np.random.choice(X.shape[0], n_samp, replace=False)\nX_samp = X[idx, :]\naxs[0].scatter(X_samp[:, 0], X_samp[:, 2], s=1, c='w', ec='None')\naxs[0].set_facecolor('k')\naxs[1].pcolormesh(grid.x, grid.y, rho.T, cmap='mono_r')\naxs.format(xlim=grid.xlim, ylim=grid.ylim)\naxs[0].format(title=f'{n_samp} random samples')\naxs[1].format(title='CIC weighting');\n\n\n\n\n\nFig. 6. Test of cloud-in-cell (CIC) particle weighting on a regular 64 x 64 grid.\n\n\n\n\n\n\nField solver\nThe workhorse in the simulation loop is the field solver. We need to solve Poisson’s equation:\n\\[\n\\left({\\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2}}\\right) = -\\frac{\\rho\\left(x, y\\right)}{\\varepsilon_0}.\n\\tag{6}\\]\nThe discretized version of the equation reads\n\\[\n\\frac{\\phi_{i+1,j} -2\\phi_{i,j} +\\phi_{i-1,j}}{{\\Delta_x}^2} + \\frac{\\phi_{i,j+1} -2\\phi_{i,j} + \\phi_{i,j-1}}{{\\Delta_y}^2} = -\\frac{\\rho_{i,j}}{\\varepsilon_0}\n\\tag{7}\\]\nfor a grid with spacing \\(\\Delta_x\\) and \\(\\Delta_y\\). There are multiple paths to a solution; we will focus on the method implemented in PyORBIT which utilizes the Fourier convolution theorem. Let’s briefly go over this method. The potential from an infinite line of elementary charges at the origin with number density \\(\\lambda\\) is\n\\[\n\\phi(\\mathbf{x}) = -\\frac{\\lambda e}{2\\pi\\varepsilon_0} \\ln{|\\mathbf{x}|} = -\\frac{\\lambda e}{2\\pi\\varepsilon_0} \\int{\\ln{|\\mathbf{x} - \\mathbf{y}|}\\delta(\\mathbf{y})d\\mathbf{y}}.\n\\tag{8}\\]\nNote that \\(\\mathbf{y}\\) is just a dummy variable. By letting \\(G(\\mathbf{x} - \\mathbf{y}) = -\\ln{|\\mathbf{x} - \\mathbf{y}|}\\) and \\(\\rho(\\mathbf{x}) = \\delta(\\mathbf{x})\\), then up to a scaling factor we have\n\\[\n\\phi(\\mathbf{x}) = \\int{G(\\mathbf{x} - \\mathbf{y})\\rho(\\mathbf{y})d\\mathbf{y}} = G(\\mathbf{x}) * \\rho(\\mathbf{x}).\n\\tag{9}\\]\nIn this form the potential is a convolution (represented by \\(*\\)) of the charge density \\(\\rho\\) with \\(G\\), which is called the Green’s function. On the grid this will look like\n\\[\n\\phi_{i, j} = \\sum_{k,l \\ne i,j}{G_{i-k, j-l} \\rho_{k, l}}.\n\\tag{10}\\]\nThis solves the problem in \\(O(N^2)\\) time complexity for \\(N\\) grid points. This is already much faster than a direct force calculation but could still get expensive for fine grids. We can speed things up by exploiting the convolution theorem, which says that the Fourier transform of a convolution of two functions is equal to the product of their Fourier transforms. The Fourier transform is defined by\n\\[\n\\hat{\\phi}(\\mathbf{k})= \\mathcal{F}\\left[\\phi(\\mathbf{x})\\right] = \\int_{-\\infty}^{\\infty}{e^{-i\\mathbf{k}\\cdot\\mathbf{x}} \\phi(\\mathbf{x}) d\\mathbf{x}}.\n\\tag{11}\\]\nThe convolution theorem then says \\[\n\\mathcal{F}\\left[\\rho * G\\right] = \\mathcal{F}\\left[\\rho\\right] \\cdot \\mathcal{F}\\left[G\\right].\n\\tag{12}\\]\nFor the discrete equation this gives\n\\[\n\\hat{\\phi}_{n, m} = \\hat{\\rho}_{n, m} \\hat{G}_{n, m},\n\\tag{13}\\]\nwhere the hat represents the discrete Fourier transform. The time complexity can be reduced to \\(O\\left(N \\log N\\right)\\) with the FFT algorithm at our disposal.\nThere is a caveat to this method: Equation 10 must be a circular convolution in order to use the FFT algorithm, which means \\(G\\) must be periodic. But the beam is in free space (we’ve neglected any conducting boundary), so this is not true. We can make it true by doubling the grid size in each dimension. We then make \\(G\\) a mirror reflection in the new quadrants so that it is periodic, and also set the charge density equal to zero in these regions. After running the method on this larger grid, the potential in the new quadrants will be unphysical; however, the potential in the original quadrant will be correct. There are also some tricks we can play to reduce the space complexity, and in the end doubling the grid size is not much of a price to pay for the gain in speed. The method is implemented in the PoissonSolver class.\n\nclass PoissonSolver:\n    \"\"\"Class to solve Poisson's equation on a 2D grid.\n    \n    Attributes\n    ----------\n    rho, phi, G : ndarray, shape (2*Nx, 2*Ny)\n        The density (rho), potential (phi), and Green's function (G) at each\n        grid point on a doubled grid. Only one quadrant (i &lt; Nx, j &lt; Ny)\n        corresponds to to the real potential.\n    \"\"\"\n    def __init__(self, grid, sign=-1.0):\n        self.grid = grid\n        new_shape = (2 * self.grid.Nx, 2 * self.grid.Ny)\n        self.rho = np.zeros(new_shape)\n        self.G = np.zeros(new_shape)\n        self.phi = np.zeros(new_shape)\n        \n    def set_grid(self, grid):\n        self.__init__(grid)\n        \n    def compute_greens_function(self):\n        \"\"\"Compute Green's function on doubled grid.\"\"\"\n        Nx, Ny = self.grid.Nx, self.grid.Ny\n        Y, X = np.meshgrid(self.grid.x - self.grid.xmin, self.grid.y - self.grid.ymin)\n        self.G[:Nx, :Ny] = -0.5 * np.log(X**2 + Y**2, out=np.zeros_like(X), \n                                         where=(X + Y &gt; 0))\n        self.G[Nx:, :] = np.flip(self.G[:Nx, :], axis=0)\n        self.G[:, Ny:] = np.flip(self.G[:, :Ny], axis=1)\n                \n    def get_potential(self, rho):\n        \"\"\"Compute the scaled electric potential on the grid.\n        \n        Parameters\n        ----------\n        rho : ndarray, shape (Nx, Ny)\n            Number of macroparticles at each grid point.\n        \n        Returns\n        -------\n        phi : ndarray, shape (Nx, Ny)\n            Scaled electric potential at each grid point.\n        \"\"\"\n        Nx, Ny = self.grid.Nx, self.grid.Ny\n        self.rho[:Nx, :Ny] = rho\n        self.compute_greens_function()\n        self.phi = ifft2(fft2(self.G) * fft2(self.rho)).real\n        return self.phi[:Nx, :Ny]\n\nRunning the algorithm gives the following potential on the doubled grid:\n\n\nCode\nsolver = PoissonSolver(grid)\nphi = solver.get_potential(rho)\n\nfig, axs = pplt.subplots(ncols=2, figwidth=8.0, share=False, space=5)\naxs[0].pcolormesh(solver.rho.T, cmap='mono_r')\naxs[1].pcolormesh(solver.phi.T, cmap='mono_r')\nfor ax in axs:\n    ax.axvline(grid.Nx - 0.5, c='w')\n    ax.axhline(grid.Ny - 0.5, c='w')\n    for xy in [(0.65, 0.75), (0.15, 0.75), (0.65, 0.25)]:\n        ax.annotate('unphysical', xy=xy, xycoords='axes fraction', c='w')\naxs[0].format(title=r'Density $\\rho$')\naxs[1].format(title=r'Potential $\\phi$');\n\n\n\n\n\nFig. 7. Electric potential on doubled grid.\n\n\n\n\nWe can then approximate the gradient of the potential using second-order centered differencing. This gives\n\\[(\\nabla\\phi)_{i,j} = \\frac{\\phi_{i+1,j} - \\phi_{i-1,j}}{2\\Delta_x} \\hat{x} + \\frac{\\phi_{i,j+1} - \\phi_{i,j-1}}{2\\Delta_y} \\hat{y}. \\tag{15}\\]\nThe following plot shows the electric field at each position.\n\n\nCode\nEx, Ey = grid.gradient(-phi)\n\nfig, axs = pplt.subplots(ncols=2, figwidth=7.5, sharey=True)\nfor ax, E in zip(axs, [Ex, Ey]):\n    m = ax.pcolormesh(\n        grid.x, grid.y, E.T / np.max(E), shading='auto', \n        vmin=-1.0, vmax=1.0, cmap='RdBu',\n    )\nfig.colorbar(m, width='1.25em')\naxs.format(xlabel='x', ylabel='y')\naxs[0].format(title=r'$E_x$ / max($E_x$)')\naxs[1].format(title=r'$E_y$ / max($E_y$)')\n\n\n\n\n\nFig. 8. Electric field on original grid.\n\n\n\n\nFinally, the value of the electric field at each particle position can be interpolated from the grid.\n\nEx_int = grid.interpolate(Ex, bunch.positions)\nEy_int = grid.interpolate(Ey, bunch.positions)\n\n\n\nParticle mover\nAll we need to do in this step is integrate the equations of motion. A common method is leapfrog integration in which the position and velocity are integrated out of phase as follows:\n\\[\nm \\left(\\frac{\\mathbf{v}_{i+1/2} - \\mathbf{v}_{i-1/2}}{\\Delta_t}\\right) = \\mathbf{F}(\\mathbf{x}_i),\n\\tag{14}\\]\n\\[\n\\frac{\\mathbf{x}_{i+1} - \\mathbf{x}_i}{\\Delta_t} = \\mathbf{v}_{i+1/2}\n\\tag{15}\\]\n\n\n\n\nFig. 9. Leapfrog integration. (Source: S. Lund.)\n\n\nA different scheme must be used when velocity-dependent forces are present. This is a symplectic integrator, which means it conserves energy. It is also second-order accurate, meaning that its error is proportional to the square of the \\(\\Delta_t\\). Finally, it is time-reversible. The only complication is that, because the velocity and position are out of phase, we need to push the velocity back one half-step before starting the simulation, and push it one half-step forward when taking a measurement."
  },
  {
    "objectID": "posts/2021-02-22_pic/index.html#putting-it-all-together",
    "href": "posts/2021-02-22_pic/index.html#putting-it-all-together",
    "title": "Particle-in-cell simulation",
    "section": "Putting it all together",
    "text": "Putting it all together\n\nSimulation loop\nWe have all the tools to implement the simulation loop. While \\(s &lt; s_{max}\\) we:\n\nCompute the charge density on the grid.\nFind the electric potential on the grid.\nInterpolate the electric field at the particle positions.\nUpdate the particle positions.\n\nWe’ll first create a History class which stores the beam moments or phase space coordinates.\n\nclass History:\n    \"\"\"Class to store bunch data over time.\n    \n    Atributes\n    ---------\n    moments : list\n        Second-order bunch moments. Each element is ndarray of shape (10,).\n    coords : list\n        Bunch coordinate arrays. Each element is ndarray of shape (nparts, 4)\n    moment_positions, coord_positions : list\n        Positions corresponding to each element of `moments` or `coords`.\n    \"\"\"\n    def __init__(self, bunch, samples='all'):\n        self.X = bunch.X\n        self.moments, self.coords = [], []\n        self.moment_positions, self.coord_positions = [], []\n        if samples == 'all' or samples &gt;= bunch.nparts:\n            self.idx = np.arange(bunch.nparts)\n        else:\n            self.idx = np.random.choice(bunch.nparts, samples, replace=False)\n        \n    def store_moments(self, s):\n        Sigma = np.cov(self.X.T)\n        self.moments.append(Sigma[np.triu_indices(4)])\n        self.moment_positions.append(s)\n        \n    def store_coords(self, s):\n        self.coords.append(np.copy(self.X[self.idx, :]))\n        self.coord_positions.append(s)\n        \n    def package(self):\n        self.moments = np.array(self.moments)\n        self.coords = np.array(self.coords)\n\nNow we’ll create a Simulation class.\n\nclass Simulation:\n    \"\"\"Class to simulate the evolution of a charged particle bunch in free space.\n        \n    Attributes\n    ----------\n    bunch : Bunch:\n        The bunch to track.\n    distance : float\n        Total tracking distance [m].\n    step_size : float\n        Distance between force calculations [m].\n    nsteps : float\n        Total number of steps = int(length / ds).\n    steps_performed : int\n        Number of steps performed so far.\n    s : float\n        Current bunch position.\n    history : History object\n        Object storing historic bunch data.\n    meas_every : dict\n        Dictionary with keys: 'moments' and 'coords'. Values correspond to the \n        number of simulations steps between storing these quantities. For\n        example, `meas_every = {'coords':4, 'moments':2}` will store the\n        moments every 4 steps and the moments every other step. Defaults to\n        storing only the initial and final positions.\n    samples : int\n        Number of bunch particles to store when measuring phase space\n        coordinates. Defaults to the entire coordinate array.\n    \"\"\"\n    def __init__(self, bunch, distance, step_size, grid_size, meas_every={}, \n                 samples='all'):\n        self.bunch = bunch\n        self.distance, self.step_size = distance, step_size \n        self.nsteps = int(distance / step_size)\n        self.grid = Grid(size=grid_size)\n        self.solver = PoissonSolver(self.grid)\n        self.fields = np.zeros((bunch.nparts, 2))\n        self.history = History(bunch, samples)  \n        self.s, self.steps_performed = 0.0, 0\n        self.meas_every = meas_every\n        self.meas_every.setdefault('moments', self.nsteps)\n        self.meas_every.setdefault('coords', self.nsteps)\n        self.sc_factor = bunch.perveance / bunch.nparts\n\n    def set_grid(self):\n        \"\"\"Set grid limits from bunch size.\"\"\"\n        self.bunch.compute_extremum()\n        self.grid.set_lims(self.bunch.xlim, self.bunch.ylim)\n        self.solver.set_grid(self.grid)\n        \n    def compute_electric_field(self):\n        \"\"\"Compute self-generated electric field.\"\"\"\n        self.set_grid()\n        rho = self.grid.distribute(self.bunch.positions)\n        phi = self.solver.get_potential(rho)\n        Ex, Ey = self.grid.gradient(-phi)\n        self.fields[:, 0] = self.grid.interpolate(Ex, self.bunch.positions)\n        self.fields[:, 1] = self.grid.interpolate(Ey, self.bunch.positions)\n                            \n    def kick(self, step_size):\n        \"\"\"Update particle slopes.\"\"\"\n        self.bunch.X[:, 1] += self.sc_factor * self.fields[:, 0] * step_size\n        self.bunch.X[:, 3] += self.sc_factor * self.fields[:, 1] * step_size\n        \n    def push(self, step_size):\n        \"\"\"Update particle positions.\"\"\"\n        self.bunch.X[:, 0] += self.bunch.X[:, 1] * step_size\n        self.bunch.X[:, 2] += self.bunch.X[:, 3] * step_size\n        \n    def store(self):\n        \"\"\"Store bunch data.\"\"\"\n        store_moments = self.steps_performed % self.meas_every['moments'] == 0\n        store_coords = self.steps_performed % self.meas_every['coords'] == 0\n        if not (store_moments or store_coords):\n            return\n        Xp = np.copy(self.bunch.X[:, [1, 3]])\n        self.kick(+0.5 * self.step_size) # sync positions/slopes\n        if store_moments:\n            self.history.store_moments(self.s)\n        if store_coords:\n            self.history.store_coords(self.s)\n        self.bunch.X[:, [1, 3]] = Xp\n        \n    def run(self, meas_every={}):\n        \"\"\"Run the simulation.\"\"\"\n        self.store()\n        self.compute_electric_field()\n        self.kick(-0.5 * self.step_size) # desync positions/slopes\n        for i in trange(self.nsteps):\n            self.compute_electric_field()\n            self.kick(self.step_size)\n            self.push(self.step_size)\n            self.s += self.step_size\n            self.steps_performed += 1\n            self.store()\n        self.history.package()\n\n\n\nBenchmark: freely expanding Vlasov equilibrium distribution\nWe need some way of checking our method’s accuracy. Luckily there is an analytic benchmark available: the Kapchinskij-Vladimirskij (KV) distribution. Without going into any detail, the beam projects to a uniform density ellipse in the \\(x\\)-\\(y\\) plane, and the space charge forces produced within this ellipse are linear (in general space charge forces are nonlinear). If we plug the KV distribution into the Vlasov equation, it can be seen that these forces will remain linear for all time if the external focusing forces are also linear. As a consequence, a set of self-consistent differential equations describing the evolution of the ellipse boundary can be written down. If we consider the beam to be an upright ellipse with semi-axis \\(a\\) along the \\(x\\) axis and \\(b\\) along the \\(y\\) axis, then without external fields the equations read:\n\\[ a'' = \\frac{2Q}{a + b} + \\frac{\\varepsilon_x}{a^3}, \\] \\[ b'' = \\frac{2Q}{a + b} + \\frac{\\varepsilon_y}{b^3}. \\tag{18}\\]\nThese are known as the KV envelope equations or simply envelope equations. \\(Q\\), called the perveance, is a dimensionless number which is proportional to the beam intensity but reduced by the beam energy. We can think of this constant as a measure of the space charge strength. The \\(\\varepsilon_x\\) and \\(\\varepsilon_y\\) terms are called the emittances and determine the area occupied by the beam in \\(x\\)-\\(x'\\) and \\(y\\)-\\(y'\\) phase space. For example, a beam with all particles sitting perfectly still in the \\(x\\)-\\(y\\) plane has no emittance, but a beam which is instead spreading out has a nonzero emittance. These emittances will also be conserved for the KV distribution. The following function integrates the envelope equations.\n\ndef track_env(X, positions, perveance=0.0):\n    \"\"\"Track beam moments (assuming KV distribution) through free space.\n    \n    Parameters\n    ----------\n    X : ndarray, shape (nparts, 4)\n        Transverse bunch coordinate array.\n    positions : list\n        List of positions at which to evaluate the equations.\n    perveance : float\n        The dimensionless space charge perveance.\n        \n    Returns\n    -------\n    ndarray, shape (len(positions), 4)\n        Each row gives [a, a', b, b'], where a and b are the beam\n        radii in the x and y dimension, respectively.\n    \"\"\"\n    Sigma = np.cov(X.T)\n    a, b = np.sqrt(Sigma[0, 0]), np.sqrt(Sigma[2, 2])\n    ap, bp = Sigma[0, 1] / a, Sigma[2, 3] / b\n    epsx = np.sqrt(np.linalg.det(Sigma[:2, :2]))\n    epsy = np.sqrt(np.linalg.det(Sigma[2:, 2:]))\n\n    def derivs(env, s):\n        a, ap, b, bp = env\n        envp = np.zeros(4)\n        envp[0], envp[2] = ap, bp\n        envp[1] = 0.5 * perveance/(a + b) + epsx**2 / a**3\n        envp[3] = 0.5 * perveance/(a + b) + epsy**2 / b**3\n        return envp\n    \n    return odeint(derivs, [a, ap, b, bp], positions, atol=1e-14)\n\nSome care must be taken in the choice of simulation parameters; we need a fine enough grid to resolve the hard edge of the beam and enough macroparticles per grid cell to collect good statistics. I chose what I thought was reasonable: 128,000 macroparticles, a step size of 2.5 cm, and a \\(128 \\times 128\\) grid. Let’s create and track four identical KV distributions, each with a different intensity.\n\n# Bunch parameters\nnparts = 128000\nbunch_length = 250.0 # [m]\nintensities = [0.0, 10.0e14, 20.0e14, 40.0e14]\n\n# Simulation parameters\ndistance = 10.0 # [m]\nstep_size = 0.025 # [m]\ngrid_size = (128, 128)\nsamples = 10000\nmeas_every = {\n    'moments': int(0.1 * distance / step_size), \n    'coords': 4\n}\n\n# Create KV bunch in normalized coordinates (surface of 4D unit sphere)\nX = np.random.normal(size=(nparts, 4))\nX = np.apply_along_axis(lambda row: row / np.linalg.norm(row), 1, X)\n\n# Scale by emittance\neps_x, eps_y = 10e-6, 10e-6\nA = 2 * np.sqrt(np.diag([eps_x, eps_x, eps_y, eps_y]))\nX = np.apply_along_axis(lambda row: np.matmul(A, row), 1, X)\n\n# Scale beam size and divergence relative to emittance\nalpha_x = alpha_y = 0.0\nbeta_x = beta_y = 20.0\nV = np.zeros((4, 4))\nV[:2, :2] = np.sqrt(1.0 / beta_x)* np.array([[beta_x, 0.0], [alpha_x, 1.0]])\nV[2:, 2:] = np.sqrt(1.0 / beta_y)* np.array([[beta_y, 0.0], [alpha_y, 1.0]])\nX = np.apply_along_axis(lambda row: np.matmul(V, row), 1, X)\n\n# Create and track bunches\nsims = []\nfor intensity in intensities:\n    bunch = Bunch(intensity, bunch_length)\n    bunch.fill(np.copy(X))\n    sim = Simulation(bunch, distance, step_size, grid_size, meas_every=meas_every, samples=samples)\n    sim.run()\n    sims.append(sim)\n\n\n\nCode\nbunch_positions = sims[0].history.moment_positions\nenv_positions = np.linspace(0, distance, 400)\nrms_sizes_lists = {'bunch':[], 'env':[]}\nfor sim in sims:\n    rms_sizes_lists['bunch'].append(np.sqrt(sim.history.moments[:, [0, 7]]))\n    rms_sizes_lists['env'].append(track_env(X, env_positions, sim.bunch.perveance)[:, [0, 2]])\n\nfig, axs = pplt.subplots(ncols=2, figsize=(7.25, 2.75), spany=False)\nalphas = np.linspace(0.4, 1.0, 4)\ncycle = pplt.Cycle(pplt.Colormap('blues', left=0.3))\ncolors = cycle.by_key()['color'][::3]\nfor key, rms_sizes_list in rms_sizes_lists.items():\n    for rms_sizes, alpha, color in zip(rms_sizes_list, alphas, colors):\n        for i, ax in enumerate(axs):\n            if key == 'env':\n                ax.plot(env_positions, 1000.0 * rms_sizes[:, i], c=color, alpha=alpha)\n            elif key == 'bunch':\n                ax.scatter(bunch_positions, 1000.0 * rms_sizes[:, i], s=8, c=color, zorder=99)\n            \nlines = [Line2D([0], [0], color=color) for color in colors]\naxs[0].legend(lines, [f'$I/I_0$ = {i}' for i in (0, 1, 2, 4)], ncols=1, fontsize=7)\naxs[0].set_title('Horizontal')\naxs[1].set_title('Vertical')\naxs.format(xlim=(-0.5, 10.5), ylabel='RMS beam size [$mm$]', xlabel='Position [m]')\n\n\nThis plot shows the horizontal and vertical beam size over time for each of the four chosen beam intensities. The solid lines are the result of integrating the envelope equations, while the dots are the result of the PIC calculation. Notice that the beam expands on its own due to the nonzero emittance and that the effect of space charge is to increase the expansion rate. It seems to be quite accurate over this distance, and the runtime is acceptable for my purposes. Here is the evolution of 10,000 randomly sampled macroparicles compared with the KV envelope.\n\n\nCode\n# Get coordinates\ncoords_list = [sim.history.coords for sim in sims]\npositions = sims[0].history.coord_positions\numax = 1.25 * max([np.max(np.max(coords, axis=1)[:, [0, 2]]) for coords in coords_list])\numax = umax * 1000.0\n\n# Create figure\nfig, axs = pplt.subplots(nrows=2, ncols=2, figwidth=5.0)\naxs.format(\n    xspineloc='bottom', yspineloc='left',\n    xlim=(-umax, umax), ylim=(-umax, umax),\n    xlabel='x [mm]', ylabel='y [mm]'\n)\naxs[1].legend([Line2D([0], [0], color='red6')], ['KV envelope'], frameon=False, loc=(0.5, 0.95))\nfor i, ax in enumerate(axs):\n    ax.annotate(f'$I/I_0$ = {i}', xy=(0.05, 0.9), xycoords='axes fraction')\nplt.close()\n\n# Create lines\nlines = []\nfor ax in axs:\n    line, = ax.plot([], [], ms=1.25, c='black', marker='.', lw=0, mew=0, fillstyle='full')\n    lines.append(line)\n        \ndef update(t):\n    for ax, coords, line, rms_sizes_list in zip(axs, coords_list, lines, rms_sizes_lists['env']):\n        line.set_data(1000.0 * coords[t, :, 0], 1000.0 * coords[t, :, 2])\n        rms_sizes_list = rms_sizes_list[::4]\n        a, b = 1000.0 * rms_sizes_list[t]\n        ax.patches = []\n        ax.add_patch(Ellipse((0.0, 0.0), 4.0 * a, 4.0 * b, color='red6', fill=False, zorder=100, lw=1))\n    axs[0].set_title('s = {:.2f} m'.format(positions[t]))\n        \nframes = len(coords_list[0]) - 1\nanim = animation.FuncAnimation(fig, update, frames=frames, interval=1000.0 / 20.0)\n\n\n\n\n\nFig. 11. Evolution of 10,000 macroparticles (black) and KV envelope (red) during free-expansion."
  },
  {
    "objectID": "posts/2021-02-22_pic/index.html#conclusion",
    "href": "posts/2021-02-22_pic/index.html#conclusion",
    "title": "Particle-in-cell simulation",
    "section": "Conclusion",
    "text": "Conclusion\nThis post implemented an electrostatic PIC solver in Python. I learned quite a bit from doing this and was happy to see my calculations agree with the theoretical benchmark. One extension of this code would be to consider the velocity-dependent force from magnetic fields. It would also be straightforward to extend the code to 3D. Finally, all the methods used here are applicable to gravitational simulations. Here are some helpful references:\n\nUSPAS course\nHockney & Eastwood\nBirdsall & Langdon\n\n\n\n\nFig. 1. Coordinate system defined in the beam rest frame.\nFig. 2. Coasting beam approximation. (Source: G. Franchetti, Space charge in circular machines, CERN Accelerator School Proceedings (2017). https://e-publishing.cern.ch/index.php/CYRSP/article/view/413)\nFig. 3. The magnetic force between parallel currents is attractive. (Source: OpenStax University Physics.)\nFig. 4. The particle-in-cell (PIC) loop.\nFig. 5. Effective particle shape for Nearest-grid-point (NGP) and cloud-in-cell (CIC) weighting.\nFig. 6. Test of cloud-in-cell (CIC) particle weighting on a regular 64 x 64 grid.\nFig. 7. Electric potential on doubled grid.\nFig. 8. Electric field on original grid.\nFig. 9. Leapfrog integration. (Source: S. Lund.)\nFig. 11. Evolution of 10,000 macroparticles (black) and KV envelope (red) during free-expansion."
  },
  {
    "objectID": "posts/2021-07-05_ontological-arguments/index.html",
    "href": "posts/2021-07-05_ontological-arguments/index.html",
    "title": "Ontological arguments",
    "section": "",
    "text": "These are my notes on Chapter 2 of Oppy’s Arguing About Gods. This chapter is about ontological arguments — arguments that seek to demonstrate the existence of God by appealing to reason alone. The first and most famous ontological argument for the existence of God was presented by Anselm in 1078 in his work, Proslogion (translated: Discourse on the Existence of God). Since then, many different philosophers have addressed Anselm’s argument, either attacking it, defending it, or reformulating it. This will be a short post; whether ontological arguments succeed is not very important to me."
  },
  {
    "objectID": "posts/2021-07-05_ontological-arguments/index.html#anselms-argument",
    "href": "posts/2021-07-05_ontological-arguments/index.html#anselms-argument",
    "title": "Ontological arguments",
    "section": "Anselm’s argument",
    "text": "Anselm’s argument\nHere’s the argument: 1. There is, in the understanding at least, a being than which no greater being can be conceived. 2. If it is even in the understanding alone, it can be thought to be in reality as well. 3. Which would be greater. 4. (Therefore) There exists, both in the understanding and in reality, a being than which no greater being can be thought.\nOne assumption is that an object’s greatness can depend on whether it exists or not. To me, it doesn’t make sense to compare the greatness of an existing thing and a non-existing thing. For example, let’s say an unwritten book \\(B\\) exists only in my understanding. Maybe the greatness of a book is a function of the words written on its pages, its length, its binding, etc. Now consider \\(B’\\), the physical realization of \\(B\\). Is \\(B’ &gt; B\\)? I think not; when I thought of \\(B\\), I thought of all the properties it would have if it were to exist.\nOne might also take issue with (1), claiming that infinity is not an understandable concept, at least not in the way Anselm intends. (I’m not sure what I think about that.) But the most obvious problem, as noted by Oppy, is that we should be able to entertain the possibility of a thing without being committed to the existence of that thing. It appears we’ve presented a version of God in which, by the very definition of God, God exists. And that’s not very helpful in determining whether God exists.\nAnselm’s argument is still interesting, though. Consider the following parodies:\nGaunilo’s Parody (The Perfect Island) 1. There is, in the understanding at least, an island than which no greater island can be thought. 2. If it is even in the understanding alone, it can be thought to be in reality as well. 3. Which would be greater. 4. (Therefore) There exists, both in the understanding and in reality, an island than which no greater island can be thought.\nThe Devil Corollary 1. There is, in the understanding at least, a being than which no worse being can be thought. 2. If it is even in the understanding alone, it can be thought to be in reality as well. 3. Which would be worse. 4. (Therefore) There exists, both in the understanding and in reality, a being than which no worse being can be thought.\nThe No-Devil Corollary 1. There is, in the understanding at least, a being than which no worse being can be thought. 2. If it exists in the understanding and in reality, it can be thought to exist in the understanding alone. 3. Which would be still worse. 4. (Therefore) There does not exist in reality a being than which no worse being can be thought.\nThe Extreme No-Devil Corollary 1. Suppose there is, in the understanding at least, a being than which no worse being can be thought. 2. If it exists in the understanding, then it is possible that it not exist in the understanding. 3. Which would be still worse. 4. (Therefore) There does not exist in the understanding a being than which no worse being can be thought.\nThe most well-known is the Perfect Island parody. There is a difficult-to-read discussion in the book about the validity/non-validity and logical structure of each parody, and about the relationships between the parodies and Anselm’s argument; for example, could there be such a thing as a maximally great island in the same way that there could be a maximally great being? I won’t summarize that discussion here; I just wanted to mention the parodies because they’re fun to think about."
  },
  {
    "objectID": "posts/2021-07-05_ontological-arguments/index.html#the-modal-ontological-argument",
    "href": "posts/2021-07-05_ontological-arguments/index.html#the-modal-ontological-argument",
    "title": "Ontological arguments",
    "section": "The modal ontological argument",
    "text": "The modal ontological argument\nThere’s a more recent ontological argument for the existence of God that is much stronger at first glance. First, some vocabulary. A possible world is a way things could be. In other words, a possible world is the conjunction of a bunch of propositions: (\\(p_1\\) & \\(p_2\\) & \\(\\dots\\)). For instance, (“dogs bark” & “cats meow”) is true in one possible world, and (“dogs meow” & “cats bark”) is true in a different possible world. This notion of possible worlds is just a useful tool to talk about possibilities — if \\(p\\) is contingently true, then \\(p\\) is true in some, but not all, possible worlds; if \\(p\\) is necessarily true, then p is true in all possible worlds; if \\(p\\) is possibly true, then \\(p\\) is true in at least one possible world. The modal ontological argument attempts to use these ideas, along with the definition of God as a necessary being, to show that God exists:\n\nIf God exists, then God exists in all possible worlds. (God would exist necessarily.)\nGod exists in some possible world. (It’s possible that God exists.)\nIf God exists in some possible world, then God exists in all possible worlds.\nIf God exists in all possible worlds, then God exists in the actual world.\nTherefore, God exists.\n\nPremise (1) is just a definition of necessary existence, and (3) follows from the idea that “possibly necessary” is the same as “necessary”. There are questions about what it means for a person to accept (2). If I say, “I could be wrong, but I don’t think God exists…” am I granting that there’s a possible world in which God exists? I don’t think so. Rather, to accept (2), I need to grant the metaphysical possibility of the existence of God. So, perhaps the argument doesn’t do much work: if I accept (2), then I likely already think that God exists in the actual world! Nonetheless, I didn’t see any obvious problem with the argument on the first pass. The issue is the following parallel argument:\n\nIf God exists, then God exists in all possible worlds. (God would exist necessarily.)\nGod exists in some, but not all possible worlds. (It’s possible that God does not exist.)\nIf God exists in some, but not all, possible worlds, then God exists in no possible worlds.\nIf God exists in no possible worlds, then God does not exist in the actual world.\nTherefore, God does not exist.\n\nI’m not aware of a way to break this symmetry."
  },
  {
    "objectID": "posts/2021-07-05_ontological-arguments/index.html#other-arguments",
    "href": "posts/2021-07-05_ontological-arguments/index.html#other-arguments",
    "title": "Ontological arguments",
    "section": "Other arguments",
    "text": "Other arguments\nThere are a couple of ontological arguments presented by Oppy that I had never heard of. One is the Mereological Ontological Argument, which argues that there is a unique thing of which everything is a part, and then calls this unique thing God. Although it seems to be a fine argument, the resulting being is just equivalent to the universe, and hence doesn’t fit very well with traditional monotheistic God. Another argument is from Kurt Gödel. This argument is logically valid, but I wasn’t sure of exactly what was meant by a few of the axioms; therefore, I don’t have many thoughts on this argument at the moment."
  },
  {
    "objectID": "posts/2021-07-05_ontological-arguments/index.html#conclusion",
    "href": "posts/2021-07-05_ontological-arguments/index.html#conclusion",
    "title": "Ontological arguments",
    "section": "Conclusion",
    "text": "Conclusion\nSkimming through the Proslogion, it’s clear that it’s not a philosophical journal article; rather, it’s a meditation at some points and a prayer at others. The meditations are mostly about how the various properties of God — such as omnipotence — could be realized. At the very least, Anselm’s ontological argument makes us think about the greatness of God and whether this greatness could even be conceived by us. And this seems to fit well with the rest of his work, even if it’s not a convincing argument for the existence of God."
  },
  {
    "objectID": "posts/2024-12-26_high-dynamic-range-benchmark/index.html",
    "href": "posts/2024-12-26_high-dynamic-range-benchmark/index.html",
    "title": "Initial halo-level particle-in-cell simulation benchmarks",
    "section": "",
    "text": "An ongoing research program at the SNS Beam Test Facility (BTF) is to predict the evolution of an intense hadron beam in a linear accelerator (linac). Specifically, we want to predict the formation of beam halo, a low-density cloud of particles surrounding the dense beam core (much like galactic halo). Halo formation is driven by nonlinear periodic forces in the accelerator combined with the self-fields of the beam, which generate chaotic trajectories with strong sensitivity to the accelerator parameters and initial phase space distribution. Particle-in-cell (PIC) codes have not been able to reproduce measurements at the halo level, which means we have no model-based approach to reducing beam loss and, therefore, a fundamental limit on the beam intensity. At the SNS BTF, we’re trying to improve these benchmarks by eliminating the uncertainty in both the accelerator model and the initial phase space distribution.\nThe BTF is a 10-meter replica of the first section of the SNS linac (which is around 400 meters long). In the first part of the BTF, a continuous stream of ions is bunched and accelerated to 2.5 MeV kinetic energy in a radiofrequency quadrupole (RFQ). The remaining beamline is a series of alternating focusing and defocusing quadrupole magnets (a “FODO” channel). Halo is expected to develop in the FODO channel, with increased halo extent for strongly mismatched beams. Our current goal is to measure the output halo distribution and reproduce it in simulation.\nPhase space measurements in the BTF utilize a set of moving slits and luminescent screens. The slits isolate a slice of the phase space distribution, and the screens record the density within the slice. We can measure the full 6D phase space distribution by scanning multiple slits in a nested loop. In high-dimensional measurements, the dynamic range, i.e., the ratio between the largest and smallest measured intensity, is only around \\(10^2\\). To observe the beam halo, we must boost the dynamic range to around \\(10^6\\) by reducing the number of slits, i.e., by decreasing the dimension of the measured phase space. 2D high-dynamic-range (HDR) measurements were pioneered at the BTF and allow us to image the beam halo in the \\(x\\)-\\(p_x\\) or \\(y\\)-\\(p_y\\) plane, where \\(x\\) and \\(y\\) are the transverse positions and \\(p_x\\) and \\(p_y\\) are the momentum.\n\n\n\n\n\n\nFigure 1: The Spallation Neutron Source (SNS) Beam Test Facility (BTF). Shown is the low-energy beam transport (LEBT), radiofrequency quadrupole (RFQ), medium-energy beam transport (MEBT) and FODO channel."
  },
  {
    "objectID": "posts/2024-12-26_high-dynamic-range-benchmark/index.html#background",
    "href": "posts/2024-12-26_high-dynamic-range-benchmark/index.html#background",
    "title": "Initial halo-level particle-in-cell simulation benchmarks",
    "section": "",
    "text": "An ongoing research program at the SNS Beam Test Facility (BTF) is to predict the evolution of an intense hadron beam in a linear accelerator (linac). Specifically, we want to predict the formation of beam halo, a low-density cloud of particles surrounding the dense beam core (much like galactic halo). Halo formation is driven by nonlinear periodic forces in the accelerator combined with the self-fields of the beam, which generate chaotic trajectories with strong sensitivity to the accelerator parameters and initial phase space distribution. Particle-in-cell (PIC) codes have not been able to reproduce measurements at the halo level, which means we have no model-based approach to reducing beam loss and, therefore, a fundamental limit on the beam intensity. At the SNS BTF, we’re trying to improve these benchmarks by eliminating the uncertainty in both the accelerator model and the initial phase space distribution.\nThe BTF is a 10-meter replica of the first section of the SNS linac (which is around 400 meters long). In the first part of the BTF, a continuous stream of ions is bunched and accelerated to 2.5 MeV kinetic energy in a radiofrequency quadrupole (RFQ). The remaining beamline is a series of alternating focusing and defocusing quadrupole magnets (a “FODO” channel). Halo is expected to develop in the FODO channel, with increased halo extent for strongly mismatched beams. Our current goal is to measure the output halo distribution and reproduce it in simulation.\nPhase space measurements in the BTF utilize a set of moving slits and luminescent screens. The slits isolate a slice of the phase space distribution, and the screens record the density within the slice. We can measure the full 6D phase space distribution by scanning multiple slits in a nested loop. In high-dimensional measurements, the dynamic range, i.e., the ratio between the largest and smallest measured intensity, is only around \\(10^2\\). To observe the beam halo, we must boost the dynamic range to around \\(10^6\\) by reducing the number of slits, i.e., by decreasing the dimension of the measured phase space. 2D high-dynamic-range (HDR) measurements were pioneered at the BTF and allow us to image the beam halo in the \\(x\\)-\\(p_x\\) or \\(y\\)-\\(p_y\\) plane, where \\(x\\) and \\(y\\) are the transverse positions and \\(p_x\\) and \\(p_y\\) are the momentum.\n\n\n\n\n\n\nFigure 1: The Spallation Neutron Source (SNS) Beam Test Facility (BTF). Shown is the low-energy beam transport (LEBT), radiofrequency quadrupole (RFQ), medium-energy beam transport (MEBT) and FODO channel."
  },
  {
    "objectID": "posts/2024-12-26_high-dynamic-range-benchmark/index.html#ldr-benchmark",
    "href": "posts/2024-12-26_high-dynamic-range-benchmark/index.html#ldr-benchmark",
    "title": "Initial halo-level particle-in-cell simulation benchmarks",
    "section": "LDR benchmark",
    "text": "LDR benchmark\nMost of the work in the last few years has focused on reproducing low-dynamic-range (LDR) measurements. In an old layout of the BTF, where the accelerator bent 180 degrees, we didn’t have much luck. Although a series of model improvements1 brought the simulations closer to the measurements, there remained gross discrepancy in the vertical phase space.2 One problem was that there seemed to be significant uncorrected dispersion in the beamline, which complicated the dynamics by coupling the longitudinal and transverse motion. Switching to a straight layout seems to have helped.3\nOld benchmarks were also trying to model a highly mismatched beam, meaning that the beam size was far from periodic in the FODO channel. Mismatched beams are expected to be more difficult to model because smaller changes in the focusing fields can generate larger changes in the beam size relative to a matched beam. After some model-based optimization, we now have a set of optics that generates a matched beam at much higher currents than previous benchmarks (52 mA vs. 25 mA).\nThe current LDR benchmark is shown below, where we compare a 5D measurement to a simulated bunch. This measurement corresponds to a matched beam. We find good agreement in all two-dimensional projections down to the $10^{-1} contour (relative to the peak density). This is great news.4\n\n\n\n\n\n\nFigure 2: Low-dynamic-range 5D simulation benchmark. Contours range from 0.01 to 1.0 as a fraction of the peak density in each frame. Black contours are measured and red contours are predicted."
  },
  {
    "objectID": "posts/2024-12-26_high-dynamic-range-benchmark/index.html#hdr-benchmark",
    "href": "posts/2024-12-26_high-dynamic-range-benchmark/index.html#hdr-benchmark",
    "title": "Initial halo-level particle-in-cell simulation benchmarks",
    "section": "HDR benchmark",
    "text": "HDR benchmark\nI’m writing this post is to share an initial benchmark at much higher dynamic range. These are the first HDR benchmarks, although we won’t publish them until ironing out some kinks in our model. The HDR measurements were performed by colleagues at the first and second measurement stations for both matched and mismatched optics.5 We then sampled \\(10^7\\) particles from the initial measurements and tracked them through the lattice.6 We use the code PyORBIT. We will eventually make our model open source.\n\nMatched optics\nHere are the simulated root-mean-square (RMS) beam sizes \\(\\tilde{x} = \\sqrt{\\langle xx \\rangle}\\) and \\(\\tilde{y} = \\sqrt{\\langle yy \\rangle}\\), where the brackets represent expected values, as a function of position in the lattice for the matched optics. I also show the maximum \\(x\\) and \\(y\\) coordinates among all particles.\n\n\n\n\n\n\nFigure 3: Simulated root-mean-square (RMS) beam sizes in the FODO channel for the case of matched optics. The faint dashed lines correspond to the maximum particle coordinates among all beam particles.\n\n\n\nThe beam core is well-matched, as expected.7 Here are the predicted and measured phase space distributions at the end of the lattice in logarithmic scale.\n\n\n\n\n\n\nFigure 4: Measured vs. predicted high-dynamic-range phase space distributions. The color scale is logarithmic (base 10).\n\n\n\nThese results are encouraging because the overall structure of the phase space distribution is not radically different than predicted. The primary difference are at the edges of the distributions, where the predicted distribution extends far beyond measured. It looks like the measured distribution is artificially cut off. This may be due to “scaping” at some point in the lattice, where the beam hits an aperture. It’s unclear where this is happening since our model predicts zero beam loss.\nNote the beautiful spiral patterns that emerge in both phase space projections. In general, spiral patterns are due to an amplitude-dependent focusing forces which rotate particles by different angles in the phase space. Although the lattice focusing is linear, space charge generates a highly nonlinear defocusing force. If you look closely, you’ll see two sets of spiral arms in the vertical distribution. I believe these correspond to different positions within the bunch (core vs. head/tail), which experience different space charge strengths and, therefore, phase advances.\n\n\nMismatched optics\nHere are the same plots for the mismatched optics.\n\n\n\n\n\n\nFigure 5: Simulated root-mean-square (RMS) beam sizes in the FODO channel for the case of mismatched optics. The faint dashed lines correspond to the maximum particle coordinates among all beam particles.\n\n\n\n\n\n\n\n\n\nFigure 6: Measured vs. predicted high-dynamic-range phase space distributions. The color scale is logarithmic (base 10).\n\n\n\nThe agreement with measurement is not quite as good as the matched case. In addition to a linear phase advance error in the horizontal plane that we haven’t been able to pinpoint, there is an asymmetry in the measured \\(x\\)-\\(x'\\) distribution that does not appear in the simulation. There is probably scraping that is unaccounted for in the simulation, but it’s unclear how this could be responsible for this asymmetry. This is the next puzzle to solve."
  },
  {
    "objectID": "posts/2024-12-26_high-dynamic-range-benchmark/index.html#contour-mapping",
    "href": "posts/2024-12-26_high-dynamic-range-benchmark/index.html#contour-mapping",
    "title": "Initial halo-level particle-in-cell simulation benchmarks",
    "section": "Contour mapping",
    "text": "Contour mapping\nThe applied electromagnetic fields in the accelerator and the self-generated fields in the beam warp the phase space via a symplectic transformation. The phase space density behaves as an incompressible fluid under such transformations. We can begin to study the transformation in the BTF by marking particles in the input and output distributions. Below, I mark particles within a thin loop in the initial phase space. The blurring of the lines may be due to the significantly different space charge effects in the core and head/tail of the bunch, which leads to different phase advances as a function of energy.\n\n\n\n\n\n\nFigure 7: Mapping of two-dimensional contours of the initial beam. Coordinates are normalized to remove linear correlations and scaled to unit variance along each dimension.\n\n\n\n\n\n\nFigure 1: The Spallation Neutron Source (SNS) Beam Test Facility (BTF). Shown is the low-energy beam transport (LEBT), radiofrequency quadrupole (RFQ), medium-energy beam transport (MEBT) and FODO channel.\nFigure 2: Low-dynamic-range 5D simulation benchmark. Contours range from 0.01 to 1.0 as a fraction of the peak density in each frame. Black contours are measured and red contours are predicted.\nFigure 3: Simulated root-mean-square (RMS) beam sizes in the FODO channel for the case of matched optics. The faint dashed lines correspond to the maximum particle coordinates among all beam particles.\nFigure 4: Measured vs. predicted high-dynamic-range phase space distributions. The color scale is logarithmic (base 10).\nFigure 5: Simulated root-mean-square (RMS) beam sizes in the FODO channel for the case of mismatched optics. The faint dashed lines correspond to the maximum particle coordinates among all beam particles.\nFigure 6: Measured vs. predicted high-dynamic-range phase space distributions. The color scale is logarithmic (base 10).\nFigure 7: Mapping of two-dimensional contours of the initial beam. Coordinates are normalized to remove linear correlations and scaled to unit variance along each dimension."
  },
  {
    "objectID": "posts/2024-12-26_high-dynamic-range-benchmark/index.html#footnotes",
    "href": "posts/2024-12-26_high-dynamic-range-benchmark/index.html#footnotes",
    "title": "Initial halo-level particle-in-cell simulation benchmarks",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe main improvements were adding an overlapping quadrupole field model a multi-bunch space charge solver.↩︎\nSee here or here for example.↩︎\nSee here and here.↩︎\nThe initial bunch was reconstructed by measuring the initial 2D phase space projections \\(\\{ f(z, p_z), f(y, p_y), f(z, p_z) \\}\\) and ignoring cross-plane correlations by setting \\(f(x, p_x, y, p_y, z, p_z) = f(x, p_x) f(y, p_y) f(z, p_z)\\). We didn’t use a full 6D measurement because 6D measurements are still extremely slow and memory-hungry; also, simulations based on simulated but realistic input beams indicate that cross-plane correlations have very little impact on the beam dynamics in the BTF. See here.↩︎\nThese measurements take a few hours and require specialize elliptical scan patterns to reduce noise and capture all low-density features.↩︎\nWith eight MPI processors, the simulations took only a few minutes.↩︎\nThe matched beam core minimizes the free energy available to drive particles into the halo.↩︎"
  },
  {
    "objectID": "posts/2024-06-06_mentflow-part1/index.html",
    "href": "posts/2024-06-06_mentflow-part1/index.html",
    "title": "Maximum-entropy phase space tomography using normalizing flows (part 1)",
    "section": "",
    "text": "Phase space tomography is a technique to reconstruct a position-momentum distribution from its projections. Entropy maximization is a mathematically rigorous way to incorporate prior information in the reconstruction, providing strong regularization when measurements are sparse. Entropy maximization is challenging when the phase space is high-dimensional. In a preprint titled High-dimensional phase space tomography using normalizing flows [arxiv], we explored the use of generative models, specifically normalizing flows, for maximum-entropy phase space tomography.\nThis post provides some background on the method in the paper. In the next post, I’ll describe some numerical experiments we used to test the method and an experimental reconstruction of a 4D phase space distribution of an intense beam in the SNS."
  },
  {
    "objectID": "posts/2024-06-06_mentflow-part1/index.html#indirectly-measuring-position-momentum-distributions",
    "href": "posts/2024-06-06_mentflow-part1/index.html#indirectly-measuring-position-momentum-distributions",
    "title": "Maximum-entropy phase space tomography using normalizing flows (part 1)",
    "section": "Indirectly measuring position-momentum distributions",
    "text": "Indirectly measuring position-momentum distributions\nTo model the behavior of a charged particle beam, we need to know how its particles are distributed in position-momentum space, or phase space. There are six phase space variables: three position coordinates (\\(x\\), \\(y\\), \\(z\\)) and three momentum coordinates (\\(x'\\), \\(y'\\), \\(z'\\)), which we wrap into a single vector \\(\\mathbf{x} = [x, x', y, y', z, z']^T\\). Since we can’t measure individual particles, we describe the beam using a probability density function \\(\\rho(\\mathbf{x})\\), where\n\\[\n\\int \\rho(\\mathbf{x}) d\\mathbf{x} = 1.\n\\tag{1}\\]\nMeasuring the phase space distribution is a major focus in accelerator physics. In a previous post, I described our efforts to measure the distribution directly. Here, I’ll describe efforts to reconstruct the distribution from partial information. I’ll assume we can only measure projections of the distribution onto position space. For example, we can measure the 1D distribution \\(\\rho(x)\\) by sweeping a conducting wire across the beam and measuring the secondary electrons emitted at each position.\nThe 1D distribution \\(\\rho(x)\\) is really an integral over the hidden momentum coordinates:\n\\[\n\\rho(x) = \\int \\rho(x, x') dx'.\n\\tag{2}\\]\nIf we could rotate the phase space coordinates, we would obtain 1D projections along different angles in the 2D phase space. This is the same problem faced in medical CT, where the detector rotates instead of the object of interest. Thus, if we could rotate the phase space coordinates, we could apply standard CT algorithms to reconstruct the 2D phase space distribution from 1D measurements.\nImagine the beam was placed in a harmonic potential well. A beautiful insight from classical mechanics is that while particles in a harmonic potential perform sinusoidal oscillations in position space, they trace circles in phase space. In other words, they rotate. Thus, measuring \\(\\rho(x)\\) at different times would be equivalent to measuring the projection of \\(\\rho(x, x')\\) along different angles in phase space. The electromagnetic focusing fields in an accelerator do not create a simple harmonic potential. But after some approximations, we can often decouple the motion in the three planes and write\n\\[\n\\begin{bmatrix}\n  x(t) \\\\ x'(t)\n\\end{bmatrix}\n=\n\\mathbf{M}\n\\begin{bmatrix}\n  x(0) \\\\ x'(0)\n\\end{bmatrix},\n\\tag{3}\\]\nwhere \\(\\mathbf{M}\\) is a symplectic matrix. And we can factor \\(\\mathbf{M}\\) into three components:\n\\[\n\\mathbf{M} =\n\\begin{bmatrix}\n    \\sqrt{\\beta (t)} & 0 \\\\\n    -\\frac{\\alpha (t)}{\\sqrt{\\beta (t)}} & \\frac{1}{\\sqrt{\\beta (t)}}\n\\end{bmatrix}\n\\begin{bmatrix}\n    \\cos{(\\phi t)} & \\sin{(\\phi t)} \\\\ -\\sin{(\\phi t)} & \\cos{(\\phi t)}\n\\end{bmatrix}\n\\begin{bmatrix}\n    \\sqrt{\\beta (0)} & 0 \\\\\n    -\\frac{\\alpha (0)}{\\sqrt{\\beta (0)}} & \\frac{1}{\\sqrt{\\beta (0)}}\n\\end{bmatrix}\n\\tag{4}\\]\nThe first and last matrices apply a scaling and shearing transformation, while the middle matrix applies a rotation in phase space. (See this post.) The rotation is the only part relevant to tomography, and we can account for the shearing and scaling so that our measurements still correspond to projections along different angles in phase space. We could measure the beam at different times by placing wire scanners at different positions along the accelerator lattice. Or we could measure the beam at one position and vary the upstream optics; both methods vary the transfer matrix, which is all that matters. Thus, by measuring the spatial density at different locations along the accelerator lattice or under different focusing optics, we can use any CT algorithm to reconstruct the phase space distribution at any point upstream of the measurements.\nThe problem becomes much more difficult when considering 6D phase space tomography from arbitrary phase space transformations. One difficulty is that we have to search the space of 6D distribution functions. In 2D, we can course-grain the distribution and search the space of images, but this does not scale well to 6D. A \\(50 \\times 50 \\times 50 \\times 50 \\times 50 \\times 50\\) grid already has \\(15 \\times 10^9\\) cells! The storage requirements for many conventional tomography algorithms are even worse because they have to store a matrix connecting the distribution to its projections. For this reason, even 4D tomography rules out the use of most conventional algorithms."
  },
  {
    "objectID": "posts/2024-06-06_mentflow-part1/index.html#entropy-maximization-for-inverse-problems",
    "href": "posts/2024-06-06_mentflow-part1/index.html#entropy-maximization-for-inverse-problems",
    "title": "Maximum-entropy phase space tomography using normalizing flows (part 1)",
    "section": "Entropy maximization for inverse problems",
    "text": "Entropy maximization for inverse problems\nAnother difficulty is that we can’t just take any measurement we wish. In medical CT, reconstructions proceed from hundreds of measurements at optimally chosen projection angles, but in accelerators, we could be limited to tens of measurements, even as few as three or four in some cases. Furthermore, the measurements may be suboptimal. In medical CT, we know that we should space the projection angles evenly over 180 degrees and can easily do this by rotating our X-ray detector. Accelerator constraints disallow many optics settings, so we can’t alsways access the full 180 degree range with equally spaced angles. And in 6D tomography, it’s not entirely clear how to determine the optimal set of transformations. All this to say, reconstructions sometimes proceed without much data, resulting in an ill-posed inverse problem. There could be various distributions consistent with the same measurements. This problem is already important in 2D and could be exponentially worse in 4D or 6D. How should we proceed if we must select a single distribution from the feasible set?\nIt seems that all we can do is rank each feasible distribution and select the distribution with the highest ranking. Equivalently, we can maximize a functional \\(H[\\rho(\\mathbf{x})]\\), subject to the measurement constraints. At this point, we’ve assumed nothing about \\(H\\). There might not be a universal functional that applies in all situations, but if there is such a functional, it should give correct results in simple cases. It turns out that a universal \\(H\\) is pinned down by four requirements  [1]:\n\nUniqueness: The maximum of \\(H\\) should be unique.\nInvariance: The maximum of \\(H\\) should not depend on the choice of coordinates.\nSubset independence: Updating the distribution in one domain should not require updating the distribution in a separate domain.\nSystem independence: If we only know the marginal distributions \\(\\rho(\\mathbf{y})\\) and \\(\\rho(\\mathbf{z})\\) and have no prior assumption of dependence between \\(\\mathbf{y}\\) and \\(\\mathbf{z}\\), then the reconstruction should not contain any such dependence: \\(\\rho(\\mathbf{y}, \\mathbf{z}) = \\rho(\\mathbf{y})\\rho(\\mathbf{z})\\).\n\nWe end up with the relative entropy:\n\\[\nH[\\rho(\\mathbf{x}), \\rho_*(\\mathbf{x})] =\n-\\int \\rho(\\mathbf{x}) \\log\\left(\\frac{\\rho(\\mathbf{x})}{\\rho_*(\\mathbf{x})}\\right) d\\mathbf{x}\n\\tag{5}\\]\nHere, \\(\\rho_*(\\mathbf{x})\\) is a prior enoding our knowledge before seeing any data. The relative entropy has a maximum at zero when \\(\\rho(\\mathbf{x}) = \\rho_*(\\mathbf{x})\\), so if there is no data, entropy maximization (ME) returns the prior. ME tells us what not to do: it tells us not to change our minds unless forced by the data, on pain of logical inconsistency.\nME is a general principle, not confined to tomography, statistical physics, or any specific problem. It sits on a firm mathematical foundation. The only arguments against ME are the following:\n\nWe have plenty of data; all algorithms give the same answer.\nIt’s too difficult to maximize entropy.\n\nI’ve argued above that particle accelerator measurements do not always tightly constrain the phase space distribution. That leaves the second consideration. Indeed, maximizing entropy is challenging, as the entropy is a highly nonlinear function of the probability density."
  },
  {
    "objectID": "posts/2024-06-06_mentflow-part1/index.html#maximum-entropy-algorithms",
    "href": "posts/2024-06-06_mentflow-part1/index.html#maximum-entropy-algorithms",
    "title": "Maximum-entropy phase space tomography using normalizing flows (part 1)",
    "section": "Maximum-entropy algorithms",
    "text": "Maximum-entropy algorithms\nI’ll now describe three approaches to our constrained optimization problem. Let’s begin by writing the measurement constraints in their most general form. Let \\(\\mathbf{x} \\in \\mathbb{R}^n\\) represent the phase space coordinates of a particle in the beam. On each measurement, particles travel through the accelerator to a measurement device. This transformation is symplectic, so we represent the \\(k\\)th transport by a symplectic map \\(\\mathcal{M}_k: \\mathcal{R}^n \\rightarrow \\mathcal{R}^n\\). Denote the transformed coordinates as\n\\[\n\\mathbf{u}_k = \\mathcal{M}_k(\\mathbf{x}).\n\\tag{6}\\]\nOnce we have the transformed coordinates, we measure the particle density on a lower dimensional plane, or projection axis, \\(\\mathbf{u}_{k_\\parallel} \\in \\mathbb{R}^m\\). To compute the projection in the particle picture, we keep these \\(m\\) dimensions and discard the rest. Or if we work with the phase space density, we can write the projection as an integral over an orthogonal integration axis \\(\\mathbf{u}_{k_\\perp} \\in \\mathbb{R}^{n - m}\\) (the unmeasured dimensions). In the following equation, \\(g(\\mathbf{u}_{k_\\parallel})\\) is the measured projected density.\n\\[\n\\begin{aligned}\n    g(\\mathbf{u}_{k_\\parallel}) &- \\int{\\rho \\left( \\mathbf{x}(\\mathbf{u}_k) \\right) d\\mathbf{u}_{k_\\perp}} = 0\\\\\n    g(\\mathbf{u}_{k_\\parallel}) &- \\int{\\rho \\left( \\mathcal{M}_k^{-1}(\\mathbf{u}_k )\\right) d\\mathbf{u}_{k_\\perp}} = 0\n\\end{aligned}\n\\tag{7}\\]\nOur task is to maximize the entropy in Equation 5, subject to the constraints in Equation 7.\n\nMENT\nAlthough the problem seems hopeless at first, we can make some progress using the method of Lagrange multipliers Lagrange multipliers. We’ll introduce a new function:\n\\[\n\\Psi\n=\nH[\\rho(\\mathbf{x}), \\rho_*(\\mathbf{x})]\n+\n\\sum_{k}^{} {\n\\int\n    \\lambda_{k}(\\mathbf{u}_{k_\\parallel})\n    \\left(\n        g_k(\\mathbf{u}_{k_\\parallel}) -\n        \\int \\rho \\left( \\mathcal{M}_k^{-1}(\\mathbf{u}_k) \\right)\n        d\\mathbf{u}_{k_\\perp}\n    \\right)\n    d\\mathbf{u}_{k_\\parallel}\n}\n\\tag{8}\\]\nWe’ve assigned a Lagrange multiplier to every point along each measurement axis. Since the measurements are continuous, the sum over Lagrange multipliers becomes an integral over Lagrange functions. The sum over \\(k\\) is just adding up all the measurements. To find the constrained maximum of \\(H\\), we need to find the stationary point of \\(\\Psi\\) with respect to the distribution \\(\\rho(\\mathbf{x})\\) and the Lagrange functions \\(\\lambda_k(\\mathbf{u}_{k_\\parallel})\\).\n\\[\n\\frac{\\delta \\Psi}{\\delta \\rho} = 0, \\frac{\\delta \\Psi}{\\delta \\lambda_k} = 0\n\\]\nI usually just treat functional derivatives as regular derivatives, and things tend to work out. For example, the derivative of the entropy is\n\\[\n\\frac{\\delta}{\\delta \\rho(\\mathbf{x})} H[\\rho(\\mathbf{x}), \\rho_*(\\mathbf{x})]\n=\n-1 - \\log\\frac{\\rho(\\mathbf{x})}{\\rho_*(\\mathbf{x})}\n\\tag{9}\\]\nWe can do a similar thing for the constraint equations. (Setting the derivative with respect to the Lagrange functions to zero returns the constraint equations). We end up with\n\\[\n\\begin{aligned}\n    \\rho(\\mathbf{x})\n    &=\n    \\rho_*(\\mathbf{x})\n    \\prod_{k} \\exp{ \\left( \\lambda_k(\\mathbf{u}_{k_\\parallel} (\\mathbf{x})) \\right) }\n\\end{aligned}\n\\tag{10}\\]\nEquation 10 parameterizes the maximum entropy distribution. We can substitute Equation 10 into Equation 7 to generate a set of highly nonlinear coupled integral equations and solve for the Lagrange functions. If successful, we would find an exact solution. No constrained optimization needed!\nMENT is an algorithm to optimize the Lagrange functions directly. I won’t describe the algorithm here. For now, I note that it’s unclear whether MENT can be efficiently implemented in 6D phase space. Thus, we wanted to know if another algorithm could find approximate maximum-entropy solutions in 6D phase space, even if we have to sacrifice some of MENT’s nice properties.\n\n\nMENT-Flow\nWe can leverage generative models to extend maximum entropy tomography to 6D phase space. Generative models represent a distribution \\(\\rho(\\mathbf{x})\\) via transformed samples:\n\\[\n\\mathbf{x} = \\mathcal{F}(\\mathbf{z}),\n\\]\nwhere \\(\\mathbf{z}\\) is a random variable from a base distribution \\(\\rho_0(\\mathbf{z})\\) and \\(\\mathcal{F}: \\mathbb{R}^{n'} \\rightarrow \\mathbb{R}^n\\) is any transformation. We’ll call \\(\\mathbf{z}\\) the normalized coordinates. If the base distribution is easy to sample from, generating iid samples from the true distribution is trivial: sample \\(\\mathbf{z}\\) and unnnormalize. A neural network can represent almost any transformation with a finite number of parameters, so we can model almost any distribution by letting a neural network represent \\(\\mathcal{F}\\).\nBased on this approach, Roussel et al.  [2] proposed a clever tomography algorithm called *Generative Phase Space Reconstruction (GPSR). Generative models are typically trained on data samples, but they don’t have to be.GPSR trains a generative model on projected densities. It samples particles from the base distribution, unnormalizes the coordinates through a neural network transformation, propagates the particles to each measurement device, and computes the projected density on the measurement planes. Then it compares the simulated projection to the measured projections, updating the network parameters until they match.\nThe trick is implementing the beam dynamics simulation \\(\\mathcal{M}_k\\). This simulation must be differentiable to backpropagate the loss through the network. Many accelerator components can be modeled in differentiable libraries such as pytorch. A second trick is in implementing the projected density estimation. Although histogram binning isn’t differentiable, kernel density estimation (KDE) is. And KDE is efficient enough for 1D and 2D data. Armed with a differentiable loss function, GPSR should be able to fit almost any \\(n\\)-dimensional distribution to 1D or 2D measurements.\nWe used a variant of GPSR to maximize the distribution’s entropy in addition to fitting the projection data. We followed Loaiza-Ganem, Gao, and Cunningham  [3], who worked on entropy maximization for a different problem with moment constraints rather than tomographic constraints. They proposed to use normalizing flows to maximize the entropy. A normalizing flow is a special type of generative model. Instead of just any unnormalizing transformation \\(\\mathcal{F}\\), we use a diffeomorphism — a smooth, invertible transformation. Think of a sheet of fabric that can stretch and compress but cannot tear. If we use such a transformation, we can track the change in probability density from the base distribution:\n\\[\n\\log\\rho(\\mathbf{x}) = \\log\\rho_0(\\mathbf{z}) - \\left| \\det \\frac{d\\mathbf{x}}{d\\mathbf{z}} \\right|\n\\]\nThe last term, called the Jacobian matrix accounts for volume change and ensures the probabilty density remains normalized.1 For \\(\\mathbf{x} = [x_1, \\dots, x_n]^T\\) and \\(\\mathbf{z} = [z_1, \\dots, z_n]^T\\)\n\\[\n\\frac{d\\mathbf{x}}{d\\mathbf{z}} =\n\\begin{bmatrix}\n    \\frac{dx_1}{dz_1} & \\dots & \\frac{dx_1}{dz_n} \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    \\frac{dx_n}{dz_1} & \\dots & \\frac{dx_n}{dz_n} \\\\\n\\end{bmatrix}\n\\tag{11}\\]\nIf we could compute Equation 11, we could easily estimate the expected value of any function \\(Q(\\mathbf{x})\\) under \\(\\rho(\\mathbf{x})\\):\n\\[\n\\begin{aligned}\n\\mathbb{E}_{\\rho(\\mathbf{x})} \\left[ Q(\\mathbf{x}) \\right]\n= \\int Q(\\mathbf{x}) \\rho(\\mathbf{x}) d\\mathbf{x}\n\\approx \\frac{1}{N} \\sum_{i=1}^{N} Q(\\mathbf{x}_i)\n\\end{aligned}\n\\tag{12}\\]\nHere, \\(\\left\\{ \\mathbf{x}_i \\right\\}\\) are samples drawn from \\(\\rho(\\mathbf{x})\\). Crucially, the entropy is an expected value!\n\\[\n\\begin{aligned}\n-H[\\rho(\\mathbf{x}), \\rho_*(\\mathbf{x})]\n&= \\int \\rho(\\mathbf{x}) \\log\\left(\\frac{\\rho(\\mathbf{x})}{\\rho_*(\\mathbf{x})}\\right) d\\mathbf{x} \\\\\n-H[\\rho(\\mathbf{x}), \\rho_*(\\mathbf{x})]\n&= \\mathbb{E}_{\\rho(\\mathbf{x})} \\left[ \\log\\rho(\\mathbf{x}) - \\log\\rho_*(\\mathbf{x}) \\right] \\\\\n-H[\\rho(\\mathbf{x}), \\rho_*(\\mathbf{x})]\n&\\approx \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\log\\rho(\\mathbf{x}_i) - \\log\\rho_*(\\mathbf{x}_i) \\right)\n\\end{aligned}\n\\tag{13}\\]\nSo, we can use normalizing flows to estimate the relative entropy. We can also maximize the entropy because the estimate in Equation 13 is differentiable. To incorporate constraints, we use the GPSR framework described above. Combining these two ingredients gives us an approximate maximum entropy phase space tomography algorithm. We ended up using a rather simple penalty method for the constrained optimization. The penalty method minimizes the following loss function:\n\\[\nL[\n    \\rho (\\mathbf{x}),\n    \\rho_*(\\mathbf{x}),\n    \\{ {g}_k(\\mathbf{u}_{k_\\parallel}) \\}\n]\n=\n- H[\\rho(\\mathbf{x}), \\rho_*(\\mathbf{x})]\n+\n\\mu \\sum_{k} {\n    D[\n        {g}_k(\\mathbf{u}_{k_\\parallel}),\n        \\tilde{g}_k(\\mathbf{u}_{k_\\parallel})\n    ]\n}\n\\tag{14}\\]\nWe minimize this loss function for a fixed penalty parameter \\(\\mu\\), starting from \\(\\mu = 0\\) (returning the prior). Then we increase \\(mu\\) and rerun the optimization, starting from the last solution.\n\n\n\n\n\n\nFigure 1: GPSR with a normalizing flow generator. The flow uses a smooth, invertible transformation to unnormalize the \\(n\\)-dimensional Gaussian base distribution, producing a phase space distribution \\(\\rho(\\mathbf{x})\\). The phase space coordinates are propagated through a series of symplectic transformatiosn \\(\\mathbf{u}_k = \\mathcal{M}_k(\\mathbf{x})\\), projected onto the \\(m\\)-dimensional measurement axes \\(\\mathbf{u}_{k_\\parallel}\\), and binned to compare to the measurements. Each step is differentiable, enabling gradient-based optimization of the flow parameters."
  },
  {
    "objectID": "posts/2024-06-06_mentflow-part1/index.html#next-steps",
    "href": "posts/2024-06-06_mentflow-part1/index.html#next-steps",
    "title": "Maximum-entropy phase space tomography using normalizing flows (part 1)",
    "section": "Next steps",
    "text": "Next steps\nThe text above sets up the maximum-entropy phase space tomography problem and suggests an approximate but scalable reconstruction model based on existing work. The primary questions we wanted to answer in this study were:\n\nIs the flow-based entropy estimate in Equation 13 sufficient? How can we judge its accuracy?\nAre normalizing flows fast and flexible enough to model complex 6D phase space distributions to projection data? It’s not obvious. Flow-based models are typically quite large and slow to train, and we require large batch sizes for GPSR.\n\nIn the next post, I’ll describe the model architecture and the numerical experiments we used to answer these questions. I’ll also describe an experiment we performed at the SNS to reconstruct the 4D phase space distribution of an intense ion beam from 1D profile measurements using MENT-Flow.\n\n\n\nFigure 1: GPSR with a normalizing flow generator. The flow uses a smooth, invertible transformation to unnormalize the \\(n\\)-dimensional Gaussian base distribution, producing a phase space distribution \\(\\rho(\\mathbf{x})\\). The phase space coordinates are propagated through a series of symplectic transformatiosn \\(\\mathbf{u}_k = \\mathcal{M}_k(\\mathbf{x})\\), projected onto the \\(m\\)-dimensional measurement axes \\(\\mathbf{u}_{k_\\parallel}\\), and binned to compare to the measurements. Each step is differentiable, enabling gradient-based optimization of the flow parameters."
  },
  {
    "objectID": "posts/2024-06-06_mentflow-part1/index.html#footnotes",
    "href": "posts/2024-06-06_mentflow-part1/index.html#footnotes",
    "title": "Maximum-entropy phase space tomography using normalizing flows (part 1)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor a symplectic transformation, the Jacobian determinant is zero (the phase space density behaves as incompressible fluid).↩︎"
  },
  {
    "objectID": "posts/2024-09-29_ment-and-bayes/index.html",
    "href": "posts/2024-09-29_ment-and-bayes/index.html",
    "title": "Maximum Entropy and Bayesian Inference",
    "section": "",
    "text": "Caticha  [1] argues that the principle of maximum relative entropy (ME) is a universal principle of inference from which Bayesian inference and conventional MaxEnt can be derived. Here’s the idea (paraphrased from  [1]).\nLet \\(q(x)\\) represent a probability distribution over \\(x\\). We wish to update \\(q(x)\\) to a new distribution \\(p(x)\\) in light of new information. This is equivalent to ranking all candidate distributions according to a functional \\(S[p(x), q(x)]\\), which we call the entropy, and selecting the highest ranked distribution. To determine the entropy functional, we use the following general principle:\nPrinciple of Minimal Updating: Beliefs should be updated only to the minimal extent required by new information.\nSo, if no information is provided, \\(p(x) = q(x)\\). We then introduce a few specific requirements:\n\nSubset independence: Probabilities conditioned on one domain should not be affected by information about a different, non-overlapping domain. This is a locality assumption. It means the contribution to the entropy from an infinitesimal region \\(x + dx\\) depends only on the distribution at \\(x\\). Consequently, the entropy must take the form:\n\n\\[\nS[p(x), q(x)] = \\int F(p(x), q(x)) dx,\n\\tag{1}\\]\nwhere \\(F(p(x), q(x))\\) is an undetermined function.\n\nSystem Independence: If we initially assume two variables are independent, and if we receive information about each variable separately, then we should not change our initial assumption. In other words, if we know \\(p(v)\\) and \\(p(w)\\) but don’t have any prior knowledge of the relationship between \\(u\\) and \\(v\\), the posterior should take the form \\(p(v, w) = p(v)p(w)\\). Enforcing this axiom determines the entropy functional:\n\n\\[\nS[p(x), q(x)] = -\\int{p(x) \\log\\left( \\frac{p(x)}{q(x)} \\right) dx}.\n\\tag{2}\\]\nME maximizes the entropy in Equation 2 subject to constraints. The constraints are typically written as integrals over the distribution; for example, the moments of the distribution can be written in this way:\n\\[\n\\langle x^n \\rangle = \\int x^n p(x) dx.\n\\]\nThis sounds like Bayesian updating if we call \\(q(x)\\) a prior and \\(p(x)\\) a posterior. Bayesian updating applies when we know a many-to-one map from \\(x\\) to \\(y\\), where \\(y\\) is a measurable variable. Additionally, we introduce the likelihood \\(p(y|x)\\), which is the probability of the data \\(y\\) given the unknown variables \\(x\\). Then Bayes rule gives:\n\\[\np(x | y) = \\frac{ p(y | x) p(x) } {p(y)},\n\\tag{3}\\]\nWe typically call \\(p(x | y)\\) the posterior. (Note that \\(p(x | y)\\) becomes the prior if used in a subsequent calculation, in which case the conditionalization on \\(y\\) would disappear.)\nLet’s see if we can connect Bayesian inference (BI) to ME. We start without any data. All we have is our prior \\(q(x)\\) and a specified likelihood \\(q(y | x)\\). The likelihood is part of our model, just like the prior, so we have an initial joint distribution over \\(x\\) and \\(y\\):\n\\[\nq(x, y) = q(x) q(y | x).\n\\tag{4}\\]\nWe then perform a measurement, finding that the variable \\(y\\) takes the value \\(y'\\). Our task is to update \\(q(x, y)\\) to a new distribution \\(p(x, y)\\) in light of this new information. We know that\n\\[\np(y) = \\int p(x, y) dx = \\delta(y - y'),\n\\tag{5}\\]\nwhere \\(\\delta\\) is the Dirac delta function. Equation 5 represents an infinite number of constraints on the joint distribution \\(p(x, y\\))—one constraint for each value of \\(y\\). The constraints do not completely determine the joint distribution, which must be of the form\n\\[\np(x, y) = p(y) p(x | y) = \\delta(y - y') p(x | y').\n\\tag{6}\\]\nThe last term, \\(p(x | y')\\), is not yet determined. We now use the ME method to update \\(q(x, y)\\) to \\(p(x, y)\\), maximizing the entropy\n\\[\nS[p(x, y), q(x, y)] = -\\int{ p(x, y) \\log{ \\left( \\frac{p(x, y)}{q(x, y)} \\right)} dxdy},\n\\tag{7}\\]\nsubject to the constraints in Equation 5. This calculation leads to\n\\[\np(x, y) = \\delta(y - y') q(x | y).\n\\tag{8}\\]\nTherefore,\n\\[\n\\begin{align}\np(x) &= \\int p(x, y) dy = \\int \\delta(y - y') q(x, y) dy \\\\\np(x) &= q(x | y') = \\frac{ q(y' | x) q(x) } {q(y')}\n\\end{align}\n\\tag{9}\\]\nDone!\n\n\n\n\nReferences\n\n[1] A. Caticha, Entropy, Information, and the Updating of Probabilities, Entropy 23, 895 (2021)."
  },
  {
    "objectID": "posts/2021-03-28_nonlinear-resonances/index.html",
    "href": "posts/2021-03-28_nonlinear-resonances/index.html",
    "title": "Nonlinear resonances",
    "section": "",
    "text": "Most of us are familiar with the experience of pushing someone else on a playground swing. We intuitively know that we should sync our pushes with the swing oscillation frequency. This strategy employs the idea of a resonance, which is an increase of the oscillation amplitude of a system for certain driving frequencies. In this post we first review the mathematics of this simple example, then extend the machinery to the nonlinear dynamics in a particle accelerator. My goal here is to write down the main results which are relevant to accelerators in order to improve my own understanding of the topic."
  },
  {
    "objectID": "posts/2021-03-28_nonlinear-resonances/index.html#resonance-in-a-driven-harmonic-oscillator",
    "href": "posts/2021-03-28_nonlinear-resonances/index.html#resonance-in-a-driven-harmonic-oscillator",
    "title": "Nonlinear resonances",
    "section": "1. Resonance in a driven harmonic oscillator",
    "text": "1. Resonance in a driven harmonic oscillator\nConsider a mass on a spring which, if left alone, oscillates at freqency \\(\\omega_0^2\\). The equation of motion for \\(x\\) is\n\\[\n\\frac{d^2}{dt^2}{x} + \\omega_0^2x = 0.\n\\tag{1}\\]\nNow consider a sinusoidal driving force \\(f(t) = f_0 \\cos(\\omega t)\\) as well as a damping term:\n\\[\n\\frac{d^2}{dt^2}{x} + b\\dot{x} + \\omega_0^2x = f_0\\cos(\\omega t)\n\\tag{2}\\]\nThe restoring, damping, and driving forces initially fight against each other, but in the end the driving force dominates and the position oscillates as\n\\[\nx(t) = A \\cos(\\omega t - \\delta)\n\\tag{3}\\]\nwhere\n\\[\nA^2 = \\frac{f_0^2}{(\\omega - \\omega_0)^2 + b\\omega^2}.\n\\tag{4}\\]\nThe figure below shows the squared amplitude as the driving frequency is varied. The maximum amplitude approaches infinity as the damping term goes to zero.\n\n\nCode\ndef get_amplitudes(f0, omegas, omega0, b):\n    return f0**2 / ((omegas - omega0)**2 + b*omegas**2)\n\nf0 = 1.0\nomega0 = 1.0\ndamping_coeffs = omega0 * np.array([0.01, 0.02, 0.04])\nomegas = np.linspace(0.0, 2.0, 1000)\ncolors = pplt.Cycle('Flare').by_key()['color']\ncolors = [colors[i] for i in [0, 4, 8]]\n\nfig, ax = pplt.subplots(figsize=(5, 2))\nfor b, color in zip(damping_coeffs, colors):\n    amplitudes = get_amplitudes(f0, omegas, omega0, b)\n    ax.plot(omegas - omega0, amplitudes, color=color, label='b = {}'.format(b))\n    ax.format(ylabel=r'$A^2$', xlabel=r'$\\omega - \\omega_0$')\nax.legend(ncols=1)\nplt.close()\n\n\n\n\n\nFig. 3. Driven, damped harmonic oscillator amplitude as a function of frequency. Three different values of the damping parameter \\(b\\) are shown.\n\n\nThe next step is to consider what happens when the driving force is not a pure sine wave. We’ll only consider periodic driving forces, and any periodic function can be written as a sum of sines and cosines of different frequencies. Assuming \\(f(t)\\) is an even function so that we can drop the sine terms in the Fourier expansion, the equation of motion becomes\n\\[\n\\ddot{x} + b\\dot{x} + \\omega_0^2 x = \\sum_{n=0}^{\\infty} {f_n\\cos(n \\omega t)}.\n\\tag{5}\\]\nThe long-term solution is found by just adding up the solutions to each term in the sum:\n\\[\nx(t) = \\sum_{n = 0}^{\\infty}{A_n \\cos{(n\\omega t - \\delta_n)}},\n\\tag{6}\\]\nwhere \\(A_n\\) is given by Eq. (3) for the frequency \\(n\\omega\\). The resonance condition will apply to each of these amplitudes individually, which means that a resonance could be excited if any component of the driving force is near the natural frequency."
  },
  {
    "objectID": "posts/2021-03-28_nonlinear-resonances/index.html#sources-of-nonlinearity",
    "href": "posts/2021-03-28_nonlinear-resonances/index.html#sources-of-nonlinearity",
    "title": "Nonlinear resonances",
    "section": "2. Sources of nonlinearity",
    "text": "2. Sources of nonlinearity\nWe’re now going to apply these ideas to a particle accelerator. We’ll assume small transverse oscillations, no acceleration, no deviation from the design momentum, and no particle-particle interactions. Under these assumptions, the transverse equation of motion of a particle with charge \\(q\\) and momentum \\(p\\) in a magnetic field \\(\\mathbf{B} = (B_x, B_y)^T\\) is\n\\[\n\\begin{aligned}\n    x'' &\\approx -\\frac{q}{mc \\beta_s \\gamma_s} B_y(x, y, s), \\\\\n    y'' &\\approx +\\frac{q}{mc \\beta_s \\gamma_s} B_x(x, y, s),\n\\end{aligned}\n\\tag{7}\\]\nRemember that \\(x' = dx/ds\\), and \\(s\\) is the position in accelerator (from now on we’ll assume a circular accelerator or “ring” of circumference \\(L\\)). Any 2D magnetic field can be expanded as the following infinite sum:\n\\[\nB_y - iB_x = \\sum_{n=1}^{\\infty}\\left({b_n - ia_n}\\right)\\left(\\frac{x + iy}{r_0}\\right)^{n-1},\n\\tag{8}\\]\nwhere \\(r_0\\) is a constant. The \\(b_n\\) and \\(a_n\\) terms are called the multipole coefficients and skew multipole coefficients, respectively. The \\(n^{th}\\) term in the expansion is the field produced by \\(2n\\) symmetrically arranged magnetic poles.\n\n\n\nFig. 4. Multipole expansion of the magnetic field up to fourth order.\n\n\nWe can see that terms with \\(n &gt; 2\\) introduce nonlinear powers of \\(x\\) and \\(y\\) on the right side of Eq. (6), while terms with \\(n \\le 2\\) introduce linear or constant terms. One may ask why we are considering a general magnetic field when in reality we use only dipoles and quadrupoles. The answer is two-fold. First, the best we can do in a real magnet is to make the \\(n &gt; 2\\) terms as small as possible; they aren’t zero and we need to know how they affect the motion. Second, sextupoles (and sometimes even octopoles) can be introduced intentionally. Their primary use is to correct for the fact that not all beam particles have the same momentum.\n\n\n\nFig. 5. An example of a sextupole electromagnet. (Source: CERN.)"
  },
  {
    "objectID": "posts/2021-03-28_nonlinear-resonances/index.html#perturbation-analysis",
    "href": "posts/2021-03-28_nonlinear-resonances/index.html#perturbation-analysis",
    "title": "Nonlinear resonances",
    "section": "3. Perturbation analysis",
    "text": "3. Perturbation analysis\nThe nonlinear terms in Eq. (6) eliminate any hope of an analytic solution. There are two options in situations such as these: 1) use a computer, or 2) use perturbation theory. The strategy of option 2 is to make approximations until an exact solution can be found, then to add in small nonlinear terms and see how the solution changes. The process can be repeated to solve the problem up to a certain order of accuracy. Usually this is infeasible beyond a few iterations, but it is a helpful tool for gaining intuition and interpreting numerical results. In particular, we’ll be looking for regions where the particle may encounter a resonance. Without many details, let’s try out the perturbation approach. Later on we’ll use a computer and see if our analysis was accurate.\n\n3.1. Floquet coordinates\nThe first step is to find an exact solution under some approximation. We’ll neglect coupling by setting \\(y = 0\\) and focus on one dimension to make things easier. Let’s denote the linear focusing from the lattice by \\(k\\), with all other terms in the field expansion folded into \\(\\Delta B\\) (there are still \\(n = 1\\) and \\(n = 2\\) terms in \\(\\Delta B\\), but they represent deviations from the design values). We’re also assuming that these variables are normalized by the ratio \\(q / p\\). This results in the equation of motion\n\\[\nx'' + k(s)x = \\Delta B.\n\\tag{9}\\]\nThis is Hill’s equation with a nonlinear driving term. The stable solution when \\(\\Delta B = 0\\) is\n\\[\nx(s) = \\sqrt{2J\\beta(s)} \\cos\\left({\\mu(s) + \\delta}\\right),\n\\tag{10}\\]\nwith the phase advance is given by\n\\[\n\\mu(s) = \\int_{0}^{s}{\\frac{ds}{\\beta(s)}}.\n\\tag{11}\\]\nThese pseudo-harmonic oscillations are still a bit difficult to visualize, so it’s helpful to perform the Floquet transformation which scales the \\(x\\) coordinate as\n\\[\nx(s) \\rightarrow u(s) = \\frac{x(s)}{\\sqrt{\\beta_x(s)}}.\n\\tag{12}\\]\nFurthermore, it is convenient to replace the \\(s\\) coordinate with\n\\[\n\\phi(s) = \\frac{1}{\\nu_0}\\int_{0}^{C}{\\frac{ds}{\\beta_x(s)}}.\n\\tag{13}\\]\nHere \\(\\nu_0\\) is the number of phase space oscillations per trip around the ring. As a result, the unperturbed equation of motion becomes (with \\(\\dot{x} = dx / d\\phi\\))\n\\[\n\\ddot{u} + \\nu_0^2 u = 0.\n\\tag{14}\\]\nBut this is just a harmonic oscillator — the trajectory in phase space is a circle, and the particle revolves once around this circle for every turn around the ring. Finally, we can write \\(\\Delta B\\) as a power series in \\(u\\) and derive the equation of motion in Floquet coordinates:\n\\[\n\\ddot{u} + \\nu_0^2 u = -\\nu_0^2 \\beta^{3/2} \\Delta B = -\\nu_0^2 \\sum_{n=0}^{\\infty} \\left({\\beta^{\\frac{n + 3}{2}} b_{n+1}}\\right) u^n.\n\\tag{15}\\]\n\n\n3.2. Fourier expansion\nThe tools to analyze driven harmonic oscillators are now available to us. Similar to Eq. (4), each term on the right hand side can be Fourier expanded, the reason being that \\(\\beta\\) (the oscillation amplitude of the unperturbed motion) and \\(b_n\\) (a multipole coefficient) depend only on the position in the ring, so of course they are periodic in \\(\\phi\\). Grouping these terms together and performing the expansion gives\n\\[\n\\ddot{u} + \\nu_0^2 u = -\\nu_0^2 \\sum_{n=0}^{\\infty}\\sum_{k=-\\infty}^{\\infty} C_{n,k} \\, u^n \\, e^{ik\\phi}.\n\\tag{16}\\]\nWe’re now going to linearize this equation. This means plugging in \\(u = u_0 + \\delta u\\), where \\(u_0\\) is the unperturbed solution and \\(\\delta_u\\) is small, and discarding all higher powers of \\(\\delta_u\\). This gives\n\\[\n\\ddot{\\delta u} + \\nu_0^2 \\delta u \\approx\n-\\nu_0^2 \\sum_{n=0}^{\\infty}\\sum_{k=-\\infty}^{\\infty} C_{n,k} \\, u_0^n \\, e^{ik\\phi}.\n\\tag{17}\\]\nThis equation tells us how the perturbation evolves with time — ideally it remains finite, but at a resonant condition it will grow without bound. The final step is to write \\(u_0^n\\) in a managable form. There is this trick involving the binomial expansion:\n\\[\nu_0^n \\propto \\cos^n(\\nu\\phi) = \\frac{1}{2^n} \\sum_{m=0}^{n} \\binom{n}{m} e^{i(n-2m)\\nu_0\\phi}.\n\\tag{18}\\]\nSo, we finally arrive at\n\\[\n\\ddot{\\delta u} + \\nu_0^2 \\delta u \\approx\n-\\nu_0^2 \\sum_{n=0}^{\\infty}\\sum_{k=-\\infty}^{\\infty} \\sum_{m=0}^{n} {n \\choose m} \\frac{C_{n,k}}{2^n} e^{i\\left[(n - 2m)\\nu_0 + k\\right]\\phi}.\n\\tag{19}\\]\nThere are a lot of indices floating around; \\(n\\) is one less than the multipole coefficient of the magnetic field, \\(k\\) is for the Fourier expansion, and \\(m\\) is just a dummy index we used to binomially expand \\(u_0^2\\).\n\n\n3.3. Resonance diagram\nEq. (18) describes a driven harmonic oscillator like Eq. (5), so we can expect a resonance condition to occur when any of the frequency components of the driving force are close to the natural frequency \\(\\nu_0\\). In other words, a resonance could occur when\n\\[\n(n - 2m)\\nu_0 + k = \\pm \\nu_0.\n\\tag{20}\\]\nIf you write out the different cases (\\(n\\) = 0, 1, 2, …), you’ll find that dipole terms (\\(n = 0\\)) forbid integer tunes, quadrupole terms forbid 1/2 integer tunes, sextupole terms forbid 1/3 integer tunes, and so on. The same thing can be done for the vertical dimension. Once coupling is included between \\(x\\) and \\(y\\), we’re lead to the definition of resonance lines:\n\\[\nM_x \\nu_x + M_y \\nu_y = N,\n\\tag{21}\\]\nwhere \\(M_x\\), \\(M_y\\), and \\(N\\) are integers and \\(|M_x| + |M_y|\\) is the order of the resonance. The reason for calling these resonance lines is because they define lines in \\(\\nu_x\\)-\\(\\nu_y\\) space (tune space).\n\n\nCode\ndef plot_resonance_lines(ax, max_order, c='k'):\n    for N in range(-max_order, max_order + 1):\n        for Mx in range(-max_order, max_order + 1):\n            for My in range(-max_order, max_order + 1):\n                order = abs(Mx) + abs(My)\n                if order &gt; 1:\n                    factor = (1 - (order - 2)/5)\n                    lw = 1.0 * factor\n                    lw = 0.4 if lw &lt; 0 else lw\n                    alpha = 1.0 * factor\n                    alpha = 0.25 if alpha &lt; 0 else alpha\n                if order &lt;= max_order:\n                    if My == 0:\n                        if Mx != 0:\n                            ax.axvline(N / Mx, c=c, alpha=alpha, lw=lw)\n                    else:\n                        ax.plot([0, 1], [N / My, (N - Mx) / My], c=c, alpha=alpha, lw=lw)\n                        \n                        \nfig, axs = pplt.subplots(ncols=2, figwidth=6.5)\naxs.format(xlim=(0, 1), ylim=(0, 1), xlabel=r'$\\nu_x$', ylabel=r'$\\nu_y$', title_kw=dict(fontsize='small'))\naxs[0].set_title('5th order')\naxs[1].set_title('10th order')\nfor ax, max_order in zip(axs, [5, 10]):\n    plot_resonance_lines(ax, max_order)\nplt.close()\n\n\n\n\n\nFig. 6. Resonance tune diagram up to fifth order (left) and tenth order (right).\n\n\nResonance strengths tend to decrease with order number, so people generally don’t consider anything beyond order 3 or 4. That being said, the machine tunes \\(\\nu_x\\) and \\(\\nu_y\\) need to be carefully chosen to avoid all low order resonance lines. Ideally, all beam particles would occupy this single point in tune space, but space charge complicates things by decreasing the tune by different amounts for each particle, possible placing them on one of the above resonance lines. This effect, called tune spread, places a fundamental limit on the number of particles in the beam."
  },
  {
    "objectID": "posts/2021-03-28_nonlinear-resonances/index.html#numerical-exploration-of-the-sextupole-resonance",
    "href": "posts/2021-03-28_nonlinear-resonances/index.html#numerical-exploration-of-the-sextupole-resonance",
    "title": "Nonlinear resonances",
    "section": "4. Numerical exploration of the sextupole resonance",
    "text": "4. Numerical exploration of the sextupole resonance\nLet’s explore the behavior of a beam under the influence of a sextupole magnet. This section recreates some figures from the book Accelerator Physics by S. Y. Lee. The easiest way to do this is to approximate the multipole as an instantaneous change to the slope of the particle’s trajectory. This is valid if the magnet isn’t too long.\n\nclass Multipole:\n    \"\"\"Class to apply multipole kick to particle. Adapted from PyORBIT tracking \n    routine in `py-orbit/src/teapotbase.cc`.\n    \n    Attributes\n    ----------\n    order : int\n        The order of the multipole term (dipole: 1, quadrupole: 2, ...).\n    strength : float\n        Integrated multipole strength [m^-(order - 1)].\n    skew : bool\n        If True, rotate the magnet 45 degrees.\n    \"\"\"\n    def __init__(self, order, strength, skew=False):\n        self.order, self.strength, self.skew = order, strength, skew\n        \n    def track_part(self, vec):\n        \"\"\"Apply transverse kick to particle slopes.\n        \n        vec : ndarray, shape (4,)\n            Transverse phase space coordinate vector [x, x', y, y'].\n        \"\"\"\n        x, xp, y, yp = vec\n        k = self.strength / np.math.factorial(self.order - 1)\n        zn = (x + 1.0j * y) ** (self.order - 1)\n        if self.skew:\n            vec[1] += k * zn.imag\n            vec[3] += k * zn.real\n        else:\n            vec[1] -= k * zn.real\n            vec[3] += k * zn.imag\n        return vec\n\nThe situation we’ll consider is a periodic lattice which is made of linear uncoupled elements + one thin sextupole. We’ll observe the beam at the location of the sextupole after each turn. A key result of the linear theory is that the details of the rest of the lattice are unimportant for this task. All we need to do is choose the Twiss parameters and tune in each dimension to form the transfer matrix, then we can just track using matrix multiplication. Recall that the transfer matrix is written as \\(\\mathbf{M} = \\mathbf{V P V^{-1}}\\), where \\(\\mathbf{V} = \\mathbf{V}(\\alpha_x, \\alpha_y, \\beta_x, \\beta_y)\\) performs the Floquet normalization and \\(\\mathbf{P} = \\mathbf{P}(\\nu_x, \\nu_y)\\) is a rotation in the \\(x\\)-\\(x'\\) and \\(y\\)-\\(y'\\) phase spaces by the angle \\(2\\pi\\nu_x\\) and \\(2\\pi\\nu_y\\), respectively. The following class implements this representation of the lattice.\n\ndef V_2D(alpha, beta):\n    \"\"\"Floquet normalization matrix in 2D phase space.\"\"\"\n    return np.array([[beta, 0.0], [-alpha, 1.0]]) / np.sqrt(beta)\n\n\ndef P_2D(tune):\n    \"\"\"Phase advance matrixmin 2D phase space.\"\"\"\n    phase_advance = 2 * np.pi * tune\n    cos, sin = np.cos(phase_advance), np.sin(phase_advance)\n    return np.array([[cos, sin], [-sin, cos]])\n\n\nclass Lattice:\n    \"\"\"Represents lattice as linear one-turn transfer matrix + multipole kick.\n    \n    Attributes\n    ----------\n    M : ndarray, shape (4, 4)\n        Linear one-turn transfer matrix.\n    aperture : float\n        Radius of cylindical boundary containing the particles [m]. \n    multipole : Multipole object\n        Must implement `track_part(vec)`, where vec = [x, xp, y, yp].\n    \"\"\"\n    def __init__(self, alpha_x, alpha_y, beta_x, beta_y, tune_x, tune_y, \n                 aperture=0.2):\n        \"\"\"Constructor.\n        \n        Parameters\n        ----------\n        alpha_x, alpha_y, beta_x, beta_y : float\n            Twiss parameters at the lattice entrance.\n        tune_x, tune_y : float\n            Number of phase space oscillations per turn.\n        \"\"\"\n        self.P = np.zeros((4, 4))\n        self.V = np.zeros((4, 4))\n        self.M = np.zeros((4, 4))\n        self.P[:2, :2] = P_2D(tune_x)\n        self.P[2:, 2:] = P_2D(tune_y)\n        self.V[:2, :2] = V_2D(alpha_x, beta_x)\n        self.V[2:, 2:] = V_2D(alpha_y, beta_y)\n        self.M = np.linalg.multi_dot([self.V, self.P, np.linalg.inv(self.V)])\n        self.aperture = aperture\n        self.multipole = None\n\n    def add_multipole(self, multipole):\n        self.multipole = multipole\n        \n    def track_part(self, vec):\n        \"\"\"Track a single particle through the lattice.\n        \n        vec : ndarray, shape (4,)\n            Transverse phase space coordinate vector [x, x', y, y'].\n        \"\"\"\n        vec = np.matmul(self.M, vec)\n        if self.multipole is not None:\n            vec = self.multipole.track_part(vec)\n        return vec\n            \n    def track_bunch(self, X):\n        \"\"\"Track a particle bunch through the lattice.\n        \n        X : ndarray, shape (nparts, 4)\n            Transverse phase space coordinate array.\n        \"\"\"\n        X = np.apply_along_axis(self.track_part, 1, X)\n        return self.collimate(X)\n        \n    def collimate(self, X):\n        \"\"\"Delete particles outside aperture.\"\"\"\n        radii = np.sqrt(X[:, 0]**2 + X[:, 2]**2)\n        return np.delete(X, np.where(radii &gt; self.aperture), axis=0)\n    \n    def get_matched_bunch(self, nparts=2000, emittance=10e-6, cut=3.0):\n        \"\"\"Generate truncated Gaussian distribution matched to the lattice.\"\"\"\n        from scipy.stats import truncnorm\n        X = truncnorm.rvs(a=4*[-cut], b=4*[cut], size=(nparts, 4))\n        A = np.sqrt(emittance) * np.identity(4)\n        V = self.V\n        X = np.apply_along_axis(lambda vec: np.matmul(A, vec), 1, X)\n        X = np.apply_along_axis(lambda vec: np.matmul(V, vec), 1, X)\n        return X\n\n\n4.1. Third-integer resonance\nWe focus first on the 1/3 integer resonance. Below, a particle is tracked over 100 turns starting from few different initial amplitudes. We set \\(y = y' = 0\\) in all cases. The \\(x\\)-\\(x'\\) trajectories should be upright ellipses in the absence of nonlinear elements. Some helper functions are defined in the following collapsed cell.\n\n\nCode\n# Define the Twiss parameters at the observation point.\nalpha_x = alpha_y = 0.0\nbeta_x = beta_y = 20.0\n\n\ndef create_lattice(tune_x=1.0, tune_y=1.0, multipole=None):\n    lattice = Lattice(alpha_x, alpha_y, beta_x, beta_y, tune_x, tune_y)\n    lattice.add_multipole(multipole)\n    return lattice\n\n\ndef get_traj(lattice, emittance, nturns=1):\n    \"\"\"Return array of shape (nturns, 4) of tracked single particle coordinates.\n    \n    The vertical coordinate and slope are set to zero.\n    \"\"\"\n    X = np.array([[np.sqrt(emittance * beta_x), 0, 0, 0]])\n    tracked_vec = [X[0]]\n    for _ in range(nturns):\n        X = lattice.track_bunch(X)\n        if X.shape[0] == 0:  # particle was deleted\n            break\n        tracked_vec.append(X[0])\n    return 1000.0 * np.array(tracked_vec)  # convert from m to mm \n\n\ndef compare_traj(tunes_x, tune_y, emittances, nturns=1, multipole=None, limits=(45, 2.5), **kws):\n    \"\"\"Compare trajectories w/ different emittances as horizontal tune is scaled.\"\"\"\n    kws.setdefault('s', 1)\n    kws.setdefault('c', 'pink8')\n    fig, axs = pplt.subplots(nrows=2, ncols=3, figwidth=7.0, figheight=4.25)\n    xlim, ylim = (-limits[0], limits[0]), (-limits[1], limits[1])\n    axs.format(xlabel=\"x [mm]\", ylabel=\"x' [mrad]\", xlim=xlim, ylim=ylim)\n    for ax, tune_x in zip(axs, tunes_x):\n        lattice = create_lattice(tune_x, tune_y, multipole)\n        for emittance in emittances:\n            tracked_vec = get_traj(lattice, emittance, nturns)\n            ax.scatter(tracked_vec[:, 0], tracked_vec[:, 1], **kws)\n            ax.annotate(\n                r'$\\nu_x = {:.3f}$'.format(tune_x), xy=(0.97, 0.97), \n                xycoords='axes fraction', \n                horizontalalignment='right', verticalalignment='top',\n                bbox=dict(fc='white', ec='black')\n            )\n    return fig, axs\n\n\ndef track_bunch(X, lattice, nturns=1):\n    \"\"\"Track and return list of coordinate array after each turn. Also return \n    the fraction of particles which were lost (exceeded aperture) at each frame.\"\"\"\n    coords, nparts, frac_lost = [X], X.shape[0], [0.0]\n    for _ in range(nturns):\n        X = lattice.track_bunch(X)\n        coords.append(X)\n        frac_lost.append(1 - X.shape[0] / nparts)\n    return [1000.0 * X for X in coords], frac_lost\n\n\ndef animate_phase_space(coords, frac_lost=None, limits=(55.0, 5.0)):\n    \"\"\"Create animation of turn-by-turn x-x' and y-y' distributions.\"\"\"\n    fig, axs = pplt.subplots(\n        ncols=2, figwidth=5.5, wspace=8.0, sharey=False, sharex=False,\n    )\n    xlim, ylim = (-limits[0], limits[0]), (-limits[1], limits[1])\n    axs.format(xlim=xlim, ylim=ylim)\n    axs[0].format(xlabel=\"x [mm]\", ylabel=\"x' [mrad]\")\n    axs[1].format(xlabel=\"y [mm]\", ylabel=\"y' [mrad]\")\n    plt.close()\n    \n    kws = dict(marker='.', color='black', ms=2.0, lw=0, mec='None')\n    line0, = axs[0].plot([], [], **kws)\n    line1, = axs[1].plot([], [], **kws)\n    \n    def update(t):\n        x, xp, y, yp = coords[t].T\n        line0.set_data(x, xp)\n        line1.set_data(y, yp)\n        axs[0].set_title('Turn {}'.format(t), fontsize='medium')\n        if frac_lost:\n            axs[1].set_title('Frac. lost = {:.3f}'.format(frac_lost[t]), fontsize='medium')\n\n    return animation.FuncAnimation(fig, update, frames=len(coords), interval=(1000.0 / 6.0))\n\n\n\n\nCode\ntunes_x = np.linspace(0.61, 0.66, 6)\ntune_y = 0.518\nemittances = 1.0e-6 * np.linspace(2.0, 10.0, 10)**2\nnturns = 100\n\nkws = dict(s=0.6, c='black', ec='none')\nfig, axs = compare_traj(tunes_x, tune_y, emittances, nturns, limits=(55.0, 3.0), **kws)\n\n\n\n\n\nFig. 7. Period-by-period phase space trajectory in a linear lattice as a function of the tune.\n\n\n\n\nNow turn on the sextupole magnet.\n\n\nCode\norder, strength = 3, 0.5\nmultipole = Multipole(order, strength, skew=False)\nfig, axs = compare_traj(tunes_x, tune_y, emittances, nturns, multipole, limits=(55.0, 3.0), **kws)\n\n\n\n\n\nFIg. 8. Period-by-period phase space trajectory in a linear lattice + sextupole as a function of the tune.\n\n\n\n\nThe initially elliptical orbits are morphed into a triangular shape as the tune approaches the resonance condition, and some of the larger orbits become unstable. It turns out that by looking at the Hamiltonian you can find a triangular region defining a separatrix between stable and unstable motion. Particles inside the triangle will oscillate forever, particles at the corner of the triangle are at unstable equilibrium points, and particles outside the triangle will eventually stream outward from the corners. This is easier to see by tracking a bunch of particles. The interesting stuff will be in the horizontal plane, but I’ll plot the vertical plane as well for comparison.\n\n\nCode\nlattice = create_lattice(tune_x=0.66, tune_y=tune_y, multipole)\nX = lattice.get_matched_bunch()\ncoords, frac_lost = track_bunch(X, lattice, nturns=50)\nanim = animate_phase_space(coords, frac_lost)\n\n\n\n\n\nFIg. 9. Phase space trajectories near the third-integer tune.\n\n\nThe triangular region of stability is clearly visible at the end of 50 turns. Interestingly, the third order resonance can be used to extract a beam from an accelerator at a much slower rate than normal. To do this, the strength and spacing of sextupole magnets must be carefully chosen to control the shape and orientation of the stability triangle, then tune is slowly moved closer to the 1/3 integer resonance value. The result is that the triangle shrinks as the stable phase space area decreases, and that more and more particles will find themselves in the unstable area and eventually stream out along the vertices.\n\n\n4.2. Integer resonance\nThe sextupole should also excite the integer resonance.\n\n\nCode\ntunes_x = np.linspace(0.96, 0.976, 6)\nfig, axs = compare_traj(tunes_x, tune_y, emittances, nturns, \n                        multipole=multipole, limits=(60, 2.5), **kws)\n\n\n\n\n\n\n\n\n\n\n\nCode\nlattice = create_lattice(tune_x=0.99, tune_y=0.18, multipole)\nanim = animate_phase_space(*track_bunch(X, lattice, nturns=50))\n\n\n\n\n\nFIg. 10. Phase space trajectories near the integer tune.\n\n\nCool pattern! The separatrix is now shaped like a tear drop. It looks like it’s evolving more slowly because the tune is close to an integer, so the particles almost return to the same location in phase space after a turn.\n\n\n4.3. Higher order resonances\nThere are also higher order resonances which a sextupole can drive. You can actually find fourth and fifth order resonances if you perform perturbation theory up to second order (at least that’s what I’m told in a textbook… I’d like to avoid carrying out such a procedure). Do these show up using our mapping equations? They are expected to be weaker, so we’ll double the sextupole strength.\n\nemittances = 1e-6 * np.array([1, 7, 15, 25, 50, 100, 150, 200, 250, 350])\ntunes_x = np.linspace(0.7496, 0.798, 6) \ntune_y = 0.23\nnturns = 1000\nmultipole = Multipole(3, 1.0)\nfig, axs = compare_traj(\n    tunes_x, tune_y, emittances, nturns, \n    multipole=multipole, limits=(150, 6),\n    **kws\n)\n\n\n\n\n\n\n\n\nThese are interesting plots. The tune near 0.75 (it’s actually 0.7496) is exciting a fourth order resonance, while the tune near 0.8 is exciting a fifth order resonance. In all the plots, the low amplitude orbits are stable ellipses. We then see the behavior change as the amplitude is increased, with the particle jumping between distinct “islands”. Eventually the trajectories once again form closed loops, but in deformed shapes. The motion is unstable at even larger amplitudes. Understanding exactly why the the plots look like they do would take more work."
  },
  {
    "objectID": "posts/2021-03-28_nonlinear-resonances/index.html#conclusion",
    "href": "posts/2021-03-28_nonlinear-resonances/index.html#conclusion",
    "title": "Nonlinear resonances",
    "section": "5. Conclusion",
    "text": "5. Conclusion\nThis post outlined the theory of nonlinear resonances driven by magnetic multipoles. The effect of a sextupole-driven resonance on the phase space trajectory was then examined using mapping equations. Taking the time to write down the steps which lead to Equation 20 — an equation often referenced in accelerator physics — was a rewarding experience and helped make the topic less mysterious to me (although I’m no expert). Here are a number of helpful references:\n\nLectures\n\nS. Lund, Transverse Particle Resonances with Application to Circular Accelerators\nE. Prebys, Resonances and Coupling\n\nTextbooks\n\nD. Edwards and M. Syphers, An introduction to the Physics of High Energy Accelerators\nH. Wiedemann, Particle Accelerator Physics\nS. Y. Lee, Accelerator Physics\nL. Reichl, The Transition to Chaos — Conservative Classical Systems and Quantum Manifestations\nJ. Taylor, Classical Mechanics\nH. Goldstein, Classical Mechanics\n\n\n\n\n\nFig. 1. Homer pushes Lisa on a swing.\nFig. 3. Driven, damped harmonic oscillator amplitude as a function of frequency. Three different values of the damping parameter \\(b\\) are shown.\nFig. 4. Multipole expansion of the magnetic field up to fourth order.\nFig. 5. An example of a sextupole electromagnet. (Source: CERN.)\nFig. 6. Resonance tune diagram up to fifth order (left) and tenth order (right).\nFig. 7. Period-by-period phase space trajectory in a linear lattice as a function of the tune.\nFIg. 8. Period-by-period phase space trajectory in a linear lattice + sextupole as a function of the tune.\nFIg. 9. Phase space trajectories near the third-integer tune.\nFIg. 10. Phase space trajectories near the integer tune."
  },
  {
    "objectID": "posts/2024-06-10_rec4d-sns-ring/index.html",
    "href": "posts/2024-06-10_rec4d-sns-ring/index.html",
    "title": "Reconstructing the 4D phase space density of a high-power proton beam from 1D measurements in the SNS ring",
    "section": "",
    "text": "In the previous post, I discussed numerical experiments to test an approximate high-dimensional maximum-entropy phase space tomography algorithm by modifying the Generative Phase Space Reconstruction (GPSR) approach. I recently applied this approach to reconstruct the 4D phase space distribution of a high-power proton beam in the SNS ring."
  },
  {
    "objectID": "posts/2024-06-10_rec4d-sns-ring/index.html#eigenpainting",
    "href": "posts/2024-06-10_rec4d-sns-ring/index.html#eigenpainting",
    "title": "Reconstructing the 4D phase space density of a high-power proton beam from 1D measurements in the SNS ring",
    "section": "Eigenpainting",
    "text": "Eigenpainting\nThe SNS accelerates H- ions to 1 GeV, strips the electrons, and injects the remaining protons into a ring. After 1000 injected turns, the accumulated pulse contains over \\(10^{14}\\) protons—over 1 MW of beam power when operating at the full 60 Hz repetition rate. Some more info on the injection scheme is here. Injecting all particles in a tiny region would lead to an enormous charge pileup and emittance growth. Thus, we slowly move the circulating beam away from the injected beam during injection to fill the phase space more uniformly.\nThe SNS starts with a large offset between the injected and circulating beams. At one position in the accelerator, the turn-by-turn particle coordinates jump around an ellipse in 2D phase space (\\(x\\)-\\(x'\\), \\(y\\)-\\(y'\\)); thus, the 2D phase space distributions \\(\\rho(x, x')\\) and \\(\\rho(y, y')\\) begin as donuts. Nonlinear and collective effects cause filamentation, filling in the donuts. We end up with a somewhat uniform density rectangular distribution in the \\(x\\)-\\(y\\) plane. Figure 1 plots a typical simulated beam using this “production” setup.\n\n\n\n\n\n\nFigure 1: Simulated beam distribution in the SNS ring.\n\n\n\nWhen we move the circulating beam away from the injected beam, we effectively change the phase space coordinates of the injected particles. In the scheme above, we use time-varying dipole magnets to adjust the position \\(x\\) (or \\(y\\)). We could also use the same magnets to change the momentum \\(x'\\) (or \\(y'\\)) by introducing an angle between the injected and circulating beams. By varying the dipole magnet currents as a function of time, we move the injection point through the \\(x\\)-\\(x'\\) phase space. Phase space painting thus seems like an appropriate name.\nWe could extend phase space painting to four phase space dimensions by introducing correlations between the horizontal and vertical dipole strengths. We then trace a line through the 4D phase space \\(x\\)-\\(x'\\)-\\(y\\)-\\(y'\\): \\(\\mathbf{x}(t)\\), where \\(t\\) represents either the time or the turn number. 4D phase space painting might enable more precise control of the beam distribution, but no one has tried it.\n\n\n\n\n\n\nFigure 2: The SNS injection region uses eight injection kickers (time-varying dipole magnets), to control the 4D phase space coordinates of the injected beam.\n\n\n\nSome time ago, Slava Danilov (ORNL) proposed a 4D phase space painting technique, which we now call eigenpainting. The idea is to create coupling in the ring using solenoid or skew quadrupole magnets, then to inject particles along an eigenvector of the \\(4 \\times 4\\) ring transfer matrix \\(\\mathbf{M}\\).\n\\[\n\\mathbf{M} \\mathbf{v}_k = e^{-2 \\pi \\nu_k} \\mathbf{v}_k,\n\\]\nwhere \\(\\mathbf{v}_k\\) are the eigenvectors, \\(\\nu_k\\) are the tunes, and \\(k = 1, 2\\). In general, particle motion is a linear combination of the eigenvectors:\n\\[\n\\mathbf{x} = \\Re \\left\\{ \\sum_k \\sqrt{2 J_k} \\mathbf{v}_k e^{\\mathbf{i \\psi_k}} \\right\\},\n\\]\nwhere \\(J_k\\) are amplitudes, \\(\\psi_k\\) are phases, and \\(\\Re\\) selects the non-imaginary component. In eigenpainting, we inject particles into the ring with \\(J_1 \\gg J_2\\). If we continuously injected particles in this way at a fixed amplitude, the particles would eventually uniformly populate an invariant closed surface in the 4D phase space. If we scaled the injection amplitude, we would fill another surface enclosing the first. It would be like adding a layer of paint to “The Rock” at the University of Michigan.\n\n\n\n\n\n\nFigure 3: Students have been adding layers of paint to the “The Rock” at the University of Michigan for many years.\n\n\n\nEach eigenvector traces an ellipse when projected onto any 2D subspace, so we could generate a uniform charge density within an ellipse in the \\(x\\)-\\(y\\) plane by scaling the injection amplitude as the square root of time.\n\\[\n\\mathbf{x}(t) = \\Re \\left\\{ \\sqrt{2 J_k} \\mathbf{v}_k e^{\\mathbf{i \\psi_k}}  \\right\\} \\sqrt{t}.\n\\]\nWe would also generate a vortex velocity field, illustrated in Figure 4. The distribution would live on a 2D surface in the 4D phase space, and the 4D phase space volume, or emittance, would be zero.\n\n\n\n\n\n\nFigure 4: Eigenpainting illustration.\n\n\n\nThings become much more complicated when particles interact through the Coloumb force. These so-called space charge interactions generally invalidate the assumption of linear dynamics. But, surprisingly, eigenpainting works even with space charge. The distribution described above is an equilibrium solution to the Vlasov-Poisson equations, which describe the self-consistent evolution of a phase space distribution under applied focusing forces and intense space charge fields. Equilibrium distributions maintain their functional form over time, even under intense space charge forces. This is not at all trivial. Our distribution, which we call the Danilov distribution, generates space charge forces that are linear in the \\(x\\) and \\(y\\) displacements, and the linearity of the space charge force is maintined for all time. It means we can still describe the particle motion using a transfer matrix as long as this transfer matrix includes the self-consistent space charge forces from the beam. Thus, eigenpainting works at any beam intensity.\nA Danilov-like distribution could have significant benefits for accelerator performance. Linear space charge forces result in the same tune shift (change in oscillation frequency) for every beam particle. Space-charge-driven tune spread is a significant intensity limitation in high-intensity rings. Paricle-core resonances driven by nonlinear space charge may also be reduced. Several authors have additionally shown that angular momentum can blur particle-core resonances and enhance beam stability. Thus, perhaps eigenpainting could bypass space charge limitations and produce higher-intensity hadron beams.\nIn addition to reduced space charge effects, the beam’s angular momentum could benefit high-energy hadron colliders. High-energy hadron colliders, such as the LHC, begin with an H- linac and injection into a low-energy ring. Generating a beam with low 4D emittance in these early stages makes it possible to flatten the beam at later stages to boost the collision luminosity. The 4D emittance is typically lower-bounded by space charge effects at low energy, but with eigenpainting, one could generate a small 4D emittance beam of arbitrary intensity.\n\n\n\n\n\n\nFigure 5: Eigenpainting could be used to generate flat beams for high-energy hadron colliders."
  },
  {
    "objectID": "posts/2024-06-10_rec4d-sns-ring/index.html#sns-experiments",
    "href": "posts/2024-06-10_rec4d-sns-ring/index.html#sns-experiments",
    "title": "Reconstructing the 4D phase space density of a high-power proton beam from 1D measurements in the SNS ring",
    "section": "SNS experiments",
    "text": "SNS experiments\nWe’ve been working on testing the eigenpainting method in the SNS ring. These experiments have been very frustrating, typically taking 6-8 hours to set up the ring and leaving little time for experiments. And we only get a maximum of one experiment per month. However, we do have some initial results that look promising. I won’t describe the details of the experiment here. Essentially, we set up the ring and programmed the injection kickers to perform the painting method as best we could under various constraints that don’t show up in simulations. Then we injected the beam.\nI’ve been focusing on how to measure the beam during injection. We’re looking to measure strong linear correlations between planes, indicating a small four-dimensional emittance, and to measure the uniformity of the charge density in the transverse plane. Our diagnostics are limited. We can’t measure the beam in the ring, but we can extract the beam on a specific turn and send it to the target. The Ring-Target Beam Transport (RTBT) line, shown in Figure 6, contains several wirescanners. Each wirescanner has a horizontal, diagonal, and vertical wire, generating vertical, horizontal, and diagonal profiles. The wirescanners run in parallel, so a single measurement generates twelve profiles. Since the wirescanners are at different locations along the beamline, each profile corresponds to a different projection axis in the 4D phase space at some point upstream of the wirescanners.\n\n\n\n\n\n\nFigure 6: Left: the accumulator ring and ring-target-beam-transport (RTBT) sections of the Spallation Neutron Source (SNS) accelerator. Right: \\(\\beta\\) functions phase advances, and focusing element and wirescanner positions at the end of the beamline. The red box outlines the wirescanner region.\n\n\n\nIn this paper, I found that these twelve profiles do not determine the 4D covariance matrix. There are constraints on the beam size and minimal control over the phase advances, but we found a slightly better set of optics for the 4D reconstruction. Although one set of measurements can generate a noisy estimate of the 4D covariance matrix, it’s better to take two or three. Unfortunately, this takes time: each measurement takes nearly five minutes! The wirescanners actually measure many different beam pulses as they slowly step through the beam and back to their starting points. If we collect three sets of optics, we’re looking at fifteen minutes per measurement. That severely limits the number of measurements we can take in our limited beam study periods. In our latest experiment, after finally getting the machine set up properly, we were able to take three good sets of wire scans, generating 36 one-dimensional profiles.\nAlthough we aimed to measure the emittance, I realized we could apply MENT-Flow to the same data to estimate the 4D phase space density. During my PhD, I tried to do this using the image of the beam on the target, but the image ended up being too noisy, so I gave up. The wirescanner measurements are more reliable, but it’s unclear if the 1D projections in the RTBT provide enough information to constrain the 4D distribution. That’s the perfect time to use entropy maximization! (Actually, MaxEnt is always the right choice, but it’s most important when data is scarce.)\nWe assumed the accelerator lattice could be described by linear focusing elements and, hence, linear transfer matrices connecting the reconstruction to the diagnostics, so it was straightforward to implement a differentiable accelerator model. I then ran MENT-Flow using a Gaussian prior based on the measured covariance matrix. Here’s the reconstruction result:\n\n\n\n\n\n\nFigure 7: Experimental MENT-Flow reconstruction of a proton beam from 1D measurements in the SNS RTBT. True/simulated profiles are plotted in red/black on the top right. The 1D and 2D projections of the reconstructed distribution are plotted on the left.\n\n\n\nThe fits aren’t perfect. After extensive testing with known ground-truth distributions, I’m confident that MENT-Flow should be able to fit the distribution to this measurement set and that the result should be close to the true distribution. However, there could be errors in our beam dynamics model or measurements. It’s unclear how to determine the problem.\nThe fits aren’t perfect, but they’re still pretty good! There are only a few bad apples. The overall good fit, combined with the entropic regularization, makes me confident that the primary features in the distribution are real. We observed two things. First, there are clear cross-plane dependencies in the reconstructed distribution. The correlations are not nearly as strong as we intended, but their presence tells us we’re moving in the right direction. Second, the beam density is not purely Gaussian; the data have pulled the reconstruction toward something more uniform. A uniform beam density is also a goal of the eigenpainting method. Although the beam density is not as uniform as we’d like it to be, it’s again encouraging that we’re moving in the right direction.\nI’m using simulations to understand how space charge and other effects degrade the beam quality during injection, blurring these correlations by the time we measure the beam. From my studies thus far, I think it’s very likely that strong space charge effects—amplified by the low 0.8 GeV beam energy and small beam size—significantly impacted the beam dynamics. 4D phase space tomography gives us a more powerful comparison to simulations and visually striking communication of our results.\n\n\n\nFigure 1: Simulated beam distribution in the SNS ring.\nFigure 2: The SNS injection region uses eight injection kickers (time-varying dipole magnets), to control the 4D phase space coordinates of the injected beam.\nFigure 3: Students have been adding layers of paint to the “The Rock” at the University of Michigan for many years.\nFigure 4: Eigenpainting illustration.\nFigure 5: Eigenpainting could be used to generate flat beams for high-energy hadron colliders.\nFigure 6: Left: the accumulator ring and ring-target-beam-transport (RTBT) sections of the Spallation Neutron Source (SNS) accelerator. Right: \\(\\beta\\) functions phase advances, and focusing element and wirescanner positions at the end of the beamline. The red box outlines the wirescanner region.\nFigure 7: Experimental MENT-Flow reconstruction of a proton beam from 1D measurements in the SNS RTBT. True/simulated profiles are plotted in red/black on the top right. The 1D and 2D projections of the reconstructed distribution are plotted on the left."
  },
  {
    "objectID": "posts/2021-07-01_space-charge-instabilities/index.html",
    "href": "posts/2021-07-01_space-charge-instabilities/index.html",
    "title": "Space charge resonances and instabilities",
    "section": "",
    "text": "This post has not been converted to Quarto format yet. It is at my old blog:\nhttps://austin-hoover.github.io/blog/physics/space%20charge/simulation/resonances/2021/07/01/space_charge_instabilities.html."
  },
  {
    "objectID": "posts/2023-05-28_btf/index.html",
    "href": "posts/2023-05-28_btf/index.html",
    "title": "High-dimensional phase space measurements",
    "section": "",
    "text": "Linear accelerators (linacs) generate powerful pulsed hadron beams for various applications. Increasing the beam power/intensity by orders of magnitude would benefit particle physics, nuclear physics, condensed matter physics, material science, biology, nuclear engineering, and other fields. Unfortunately, significantly increasing the beam intensity is not possible due to beam loss which occurs when particles hit the walls of the accelerator during their journey. The total lost beam power must be kept below 1 watt (W) per meter to avoid dangerous radiation levels. For a 1 MW beam, this limit corresponds to a fractional loss of \\(10^{-6}\\).\nSimulations struggle to predict the number and location of lost particles. The dynamics in the accelerator are complex, but the situation is perhaps unsurprising for other reasons:\n\nAccelerators are composed of thousands of components distributed over hundreds of meters. Uncertainties in each component’s position, alignment, amplitude, etc., lead to errors in the applied electromagnetic field at each position.\nThe initial distribution of particles in position-momentum space (phase space) is usually unknown.\n\nThese issues make it difficult to predict the low-order moments of the beam distribution, let alone low levels of beam loss. The second issue is critical because of the violent space charge forces that influence the beam evolution, coupling the output to the input phase space distribution [1,2].1\nTake the Spallation Neutron Source (SNS) as an example. The SNS accelerates negative hydrogen (H-) ions through a 500-meter linac. Hundreds of beam loss monitors (BLMs) record the total number of lost particles as a function of position. Phase space distribution measurements are limited to low-dimensional projections in the low-energy stages (near the boxed region in Fig. 1). Simulations based on these measurements and the accelerator’s design parameters cannot predict the measured beam loss.\n\n\n\nFig. 1. The Spallation Neutron Source (SNS) accelerator.\n\n\nThe LEDA (Low Energy Test Accelerator) experiment at Los Alamos National Laboratory attempted to predict halo formation over a much shorter distance [3]. One-dimensional (1D) density profiles of a low-energy proton beam were measured after a series of focusing quadrupole magnets. Simulations failed to reproduce the low-density “halo” surrounding the beam core — see Fig. 2. The authors attributed the failure to an inaccurate phase space distribution used to seed the simulation. (Only low-order moments of the distribution were measured.)\n\n\n\nFig. 2. Vertical beam profile measured in the LEDA experiment [3]."
  },
  {
    "objectID": "posts/2023-05-28_btf/index.html#predicting-halo-formation",
    "href": "posts/2023-05-28_btf/index.html#predicting-halo-formation",
    "title": "High-dimensional phase space measurements",
    "section": "",
    "text": "Linear accelerators (linacs) generate powerful pulsed hadron beams for various applications. Increasing the beam power/intensity by orders of magnitude would benefit particle physics, nuclear physics, condensed matter physics, material science, biology, nuclear engineering, and other fields. Unfortunately, significantly increasing the beam intensity is not possible due to beam loss which occurs when particles hit the walls of the accelerator during their journey. The total lost beam power must be kept below 1 watt (W) per meter to avoid dangerous radiation levels. For a 1 MW beam, this limit corresponds to a fractional loss of \\(10^{-6}\\).\nSimulations struggle to predict the number and location of lost particles. The dynamics in the accelerator are complex, but the situation is perhaps unsurprising for other reasons:\n\nAccelerators are composed of thousands of components distributed over hundreds of meters. Uncertainties in each component’s position, alignment, amplitude, etc., lead to errors in the applied electromagnetic field at each position.\nThe initial distribution of particles in position-momentum space (phase space) is usually unknown.\n\nThese issues make it difficult to predict the low-order moments of the beam distribution, let alone low levels of beam loss. The second issue is critical because of the violent space charge forces that influence the beam evolution, coupling the output to the input phase space distribution [1,2].1\nTake the Spallation Neutron Source (SNS) as an example. The SNS accelerates negative hydrogen (H-) ions through a 500-meter linac. Hundreds of beam loss monitors (BLMs) record the total number of lost particles as a function of position. Phase space distribution measurements are limited to low-dimensional projections in the low-energy stages (near the boxed region in Fig. 1). Simulations based on these measurements and the accelerator’s design parameters cannot predict the measured beam loss.\n\n\n\nFig. 1. The Spallation Neutron Source (SNS) accelerator.\n\n\nThe LEDA (Low Energy Test Accelerator) experiment at Los Alamos National Laboratory attempted to predict halo formation over a much shorter distance [3]. One-dimensional (1D) density profiles of a low-energy proton beam were measured after a series of focusing quadrupole magnets. Simulations failed to reproduce the low-density “halo” surrounding the beam core — see Fig. 2. The authors attributed the failure to an inaccurate phase space distribution used to seed the simulation. (Only low-order moments of the distribution were measured.)\n\n\n\nFig. 2. Vertical beam profile measured in the LEDA experiment [3]."
  },
  {
    "objectID": "posts/2023-05-28_btf/index.html#high-dimensional-phase-space-measurements",
    "href": "posts/2023-05-28_btf/index.html#high-dimensional-phase-space-measurements",
    "title": "High-dimensional phase space measurements",
    "section": "2. High-dimensional phase space measurements",
    "text": "2. High-dimensional phase space measurements\nWe are continuing experimental work on halo prediction at the SNS Beam Test Facility (BTF), a recently commissioned replica of the front end of the SNS linac [4]. A diagram of the BTF is below.\n\n\n\nFig. 3. The SNS Beam Test Facility (BTF).\n\n\nThe beam starts as a slow-moving, continuous stream of H- ions extracted from the plasma source. The low-energy beam transport (LEBT) uses electrostatic focusing to guide the beam to the radiofrequency quadrupole RFQ. The RFQ bunches and accelerates the beam to 2.5 MeV. Each bunch drifts through the medium-energy beam transport (MEBT) and finally through a series of permanent quadrupole magnets arranged in a focusing-off-defocusing-off (FODO) pattern. (If halo formation occured, it would likely be here due to the rapid oscillation of the beam core.)\nThe apparatus just after the RFQ (blue region) measures the particle density in six-dimensional phase space — three positions (\\(x\\), \\(y\\), \\(z\\)) and three momenta (\\(p_x\\), \\(p_y\\), \\(p_z\\)), with the \\(z\\) axis parallel to the beam. It works by measuring the charge within a small region \\(\\mathbf{x} \\pm \\Delta\\), where \\(\\mathbf{x} = [x, p_x, y, p_y, z, p_z]^T\\), scanning \\(\\mathbf{x}\\) through the space, and interpolating the data to obtain an estimate of the distribution function \\(f(\\mathbf{x})\\).\nThe region \\(\\mathbf{x} \\pm \\Delta\\) is selected using slits. First, a horizontal-vertical pair of slits selects a square in the \\(x\\)-\\(y\\) plane. Next, a second pair of slits select \\(p_x\\) and \\(p_y\\) (based on their positions relative to the first slits). The momentum \\(p_z\\) is selected by a vertical slit after a dipole magnet, which bends on-energy particles 90 degrees (faster particles bend less). The last to go is \\(z\\): The beam passes through a wire, generating an electron beam with the same temporal structure. The electrons are streaked in the transverse plane using an oscillating electric field such that the image of the beam on a screen gives the \\(z\\) distribution.\n\n\n\nFig. 4 Cartoon representing the six-dimensional phase space measurement apparatus.\n\n\nWe proceed by scanning the slits in a nested loop and recording the image of the beam on the screen at each setting. Each pixel in the image is the intensity at a point in phase space. This measurement is the first of its kind [5].\nOnce we reconstruct the initial distribution \\(f(\\mathbf{x})\\), we generate a set of phase space coordinates \\(\\{\\mathbf{x}_1, \\dots \\mathbf{x}_n\\}\\) by sampling from the distribution.2 These coordinates are propagated through a computational accelerator model and compared with measurements at the end of the beamline. Here we sacrifice dimensionality for dynamic range to image the beam halo. Instead of measuring the one-dimensional projections \\(\\{ f(x), f(y) \\}\\), we measure two-dimensional projections \\(\\{ f(x, p_x), f(y, p_y) \\}\\). We can reach a dynamic range of \\(10^6\\) (the dynamic range in Fig. 2 is between \\(10^3\\) and \\(10^4\\)). This high-dynamic-range measurement is the first of its kind [6].\nWhen combined with a detailed model of the accelerator and a well-tested simulation code, we hope these measurements will allow us to predict the halo dynamics in the BTF. Such a prediction would be the first step toward model-based loss prediction in a linear accelerator.\n\n\n\nFig. 5. The SNS-BTF workflow. The phase space distribution \\(f(\\mathbf{x})\\) is interpolated from raw images from high-dimensional scans. Particles are sampled from the distribution and propagated through a computational model of the BTF lattice. The output particle distribution is binned and compared with high-dynamic-range measurements of \\(\\{f(x, p_x), f(y, p_y)\\}\\) at the end of the beamline. The model is refined until the predictions are correct down to the halo level.\n\n\nOur 6D measurements are currently quite slow and fail to capture sharp features in the distribution. Dropping one dimension (\\(z\\)) increases the resolution and dynamic range while capturing most of the significant correlations in the data. The resulting 5D measurement is useful for in-depth examinations of the phase space distribution. I’ve gotten a lot of mileage out of a high-resolution 5D measurement I took in June 2022, which is publically available [7]. The scan generating over 285,082 images over 18 hours. Fig. 6. shows the change in image brightness as the slits move in and out of the beam core.\n\n\n\nFig. 6. Measured image brightness and beam current out of the RFQ during a 5D measurement.\n\n\nWe cropped and downscaled the images to decrease the size of the data set from around 250 gigabytes to 20 gigabytes. Next, we interpolated the pixel intensities to obtain a \\(69 \\times 88 \\times 69 \\times 65 \\times 55\\) image of the phase space distribution \\(f(x, x', y, y', w)\\). Here we use the energy \\(w\\) instead of the momentum \\(p_z\\). (Also \\(x'\\) and \\(y'\\) are basically the same as \\(p_x\\) and \\(p_y\\).)"
  },
  {
    "objectID": "posts/2023-05-28_btf/index.html#high-dimensional-data-visualization",
    "href": "posts/2023-05-28_btf/index.html#high-dimensional-data-visualization",
    "title": "High-dimensional phase space measurements",
    "section": "3. High-dimensional data visualization",
    "text": "3. High-dimensional data visualization\nWe have spent significant time analyzing the measured initial phase space distribution [5,8–10]. This task is straightforward in two dimensions: one simply looks at an image representing \\(f(x, p_x)\\) and extracts features from it. High-dimensional data is exponentially more challenging to visualize and interpret.\nHere it is helpful to distinguish projections, slices, and partial projections. A projection is an integral/sum that squashes the distribution onto a lower-dimensional space. For example: \\[\nf(x, p_x) = \\iiiint_{-\\infty}^{+\\infty} {f(x, p_x, y, p_y, z, p_z) dy dp_y dz dp_z}.\n\\] We must eventually project the distribution onto a two-dimensional space to view it on a page or screen. For an \\(n\\)-dimensional distribution, there are \\(n\\) one-dimensional projections and \\(n (n - 1) / 2\\) two-dimensional projections. The classic corner plot displays all these projections in a single figure.\n\n\n\nFig. 7. Corner plot showing the one-dimensional and two-dimensional projections of a measured five-dimensional distribution.\n\n\nCorner plots are valuable tools, but it is important to remember that they hide relationships between three or more dimensions. Higher dimensional correlations are only visible in slices. Most people are familiar with slices (i.e., cross sections) from three-dimensional medical CT scans. CT scans map the three-dimensional density of, say, a human brain. To examine the brain’s internal structure, medical professionals scroll through a set of two-dimensional images. Each image is the density on a two-dimensional plane intersecting the three-dimensional distribution. We call this a “planar slice”.\nAdd one more dimension. How is a slice defined now? Well, we could look at the intersection of a three-dimensional plane with the four-dimensional distribution. This is equivalent to fixing the value of one of the coordinates, leaving a conditional distribution like \\(f(x, p_x, p_y \\vert y{=}0)\\). But there is now more than one dimension that we could slice. We could take another three-dimensional plane, orthogonal to the first plane, and view the intersection of both planes and the four-dimensional distribution. We are left with a two-dimensional surface like \\(f(x, p_x \\vert y{=}p_y{=}0)\\).\nSince slices can be projected and projections can be sliced, we must distinguish between a full projection and a partial projection — the projection of a slice. More complex slicing/projection sequences are obviously possible in five- or six-dimensional spaces.\nThe “slice matrix” plot in Fig. 8 is one way to visualize the effect of slicing a four-dimensional distribution. A two-dimensional projection is on the bottom right. Three-dimensional projections — obtained by slicing only one dimension — are on the bottom and right panels. The four-dimensional distribution — obtained by slicing two dimensions — is shown on the top left.\n\n\n\nFig. 8. A four-dimensional slice of a measured five-dimensional distribution. Three- and two-dimensional projections are shown on the side panels.\n\n\nSuch figures render slowly and are limited to four-dimensional data. Complete slicing flexibility is available in an interactive widget shown in Fig. 9. This function takes one or more images or point clouds of any dimensionality as inputs. I use it extensively in my data analysis. It is available in the psdist package.\n\n\nFig. 9. Interactive slicing widget from psdist.\nFig. 9. Interactive slicing widget from psdist.\n\n\nThere are other ways to slice and project the data. For example, we could extend the concept of a radial histogram. A radial histogram takes a distribution \\(f(x_1, \\dots, x_n)\\) to a one-dimensional profile \\(f(r_n)\\), where \\(r_n = \\sqrt{x_1^2 + \\dots + x_n^2}\\), by integrating the density on spheres of radius \\(r_n\\).3 Instead of defining the spheres in the \\(n\\)-dimensional space, suppose we defined them in a subspace like the \\(x_2\\)-\\(\\dots\\)-\\(x_n\\) plane and observed the \\(x_1\\) distribution on each sphere. In other words, compute \\(f(x_1, r(x_2, \\dots, x_n))\\). The resulting projection is more difficult to interpret but preserves high-dimensional correlations in the data.\nWe could also define shells as the density contours of the distribution in the subpace — \\(f(x_2, \\dots, x_n)\\) in the above example. We could label these “contour shell slices”. Fig. 10 illustrates this concept. I applied this idea to our measured distribution to view a specific feature in a compact figure. In Fig. 11, I defined density contours in the four-dimensional transverse phase space by thresholding the projection \\(f(x, x', y, y')\\) between two density levels. I then computed the energy distribution within each shell (possible thanks to numpy).\n\n\n\nFig. 10. Concept of non-planar or “shell” slices.\n\n\n\n\n\nFig. 11. Energy (\\(w\\)) distribution within four-dimensional contour shell slices in the transverse phase space. Each slice is defined by the projected density \\(f(x, x', y, y')\\).\n\n\nNotice the drastic change in energy distribution — unimodal to bimodal — as we move from the outer to the inner core in the transverse phase space. This energy hollowing represents a five-dimensional correlation; it is also visible (although perhaps harder to see) in Fig. 8. I also plotted the transverse projections of the slice farthest from the core. Pretty bizarre-looking images result, but they make some sense; they select particles with large transverse amplitude. Some intuition is gained by considering the projections of hollow three-dimensional distribution."
  },
  {
    "objectID": "posts/2023-05-28_btf/index.html#ongoing-work",
    "href": "posts/2023-05-28_btf/index.html#ongoing-work",
    "title": "High-dimensional phase space measurements",
    "section": "4. Ongoing work",
    "text": "4. Ongoing work\nWe would like to explain the origin of all features we find in the distribution. From direct measurements at lower beam currents, we know the space charge drives the energy hollowing in Fig. 10.\n\n\n\nFig. 11. Sliced energy distribution vs. beam current. Source: Kiersten Ruisard.\n\n\nFurthermore, we are confident that the energy hollowing develops in the RFQ rather than the MEBT. I am currently using simulations to study the dynamics that drive the hollowing in the RFQ.\nAnother (more practical) question is: how do “hidden” features in the initial distribution affect the downstream dynamics? In particular, how do they affect halo formation in the BTF or the SNS linac? I am also studying this problem using simulations.\nImprovements to the high-dimensional measurements and the accelerator model will resume when the BTF comes back online this fall.\n\n\n\nFig. 1. The Spallation Neutron Source (SNS) accelerator.\nFig. 2. Vertical beam profile measured in the LEDA experiment [3].\nFig. 3. The SNS Beam Test Facility (BTF).\nFig. 4 Cartoon representing the six-dimensional phase space measurement apparatus.\nFig. 5. The SNS-BTF workflow. The phase space distribution \\(f(\\mathbf{x})\\) is interpolated from raw images from high-dimensional scans. Particles are sampled from the distribution and propagated through a computational model of the BTF lattice. The output particle distribution is binned and compared with high-dynamic-range measurements of \\(\\{f(x, p_x), f(y, p_y)\\}\\) at the end of the beamline. The model is refined until the predictions are correct down to the halo level.\nFig. 6. Measured image brightness and beam current out of the RFQ during a 5D measurement.\nFig. 7. Corner plot showing the one-dimensional and two-dimensional projections of a measured five-dimensional distribution.\nFig. 8. A four-dimensional slice of a measured five-dimensional distribution. Three- and two-dimensional projections are shown on the side panels.\nFig. 9. Interactive slicing widget from psdist.\nFig. 10. Concept of non-planar or “shell” slices.\nFig. 11. Energy (\\(w\\)) distribution within four-dimensional contour shell slices in the transverse phase space. Each slice is defined by the projected density \\(f(x, x', y, y')\\).\nFig. 11. Sliced energy distribution vs. beam current. Source: Kiersten Ruisard."
  },
  {
    "objectID": "posts/2023-05-28_btf/index.html#footnotes",
    "href": "posts/2023-05-28_btf/index.html#footnotes",
    "title": "High-dimensional phase space measurements",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSpace charge forces arise from the electric field generated by the beam. The strength of space charge forces scales inversely with the beam energy.↩︎\nSampling from a high-dimensional distribution is not trivial. We currently use a simple method that treats the distribution as a histogram. I am looking into approaches that will scale to high-dimensional distribution functions, such as normalizing flows.↩︎\nOne could just as well define ellipsoids instead of spheres by computing the covariance matrix \\(\\mathbf{\\Sigma} = \\mathbf{x}\\mathbf{x}^T\\) and setting \\(r = \\mathbf{x}^T \\mathbf{\\Sigma}^{-1} \\mathbf{x}\\).↩︎"
  },
  {
    "objectID": "posts/2021-05-27_painting-a-particle-beam/index.html",
    "href": "posts/2021-05-27_painting-a-particle-beam/index.html",
    "title": "Painting a particle beam",
    "section": "",
    "text": "There is a great scene in the otherwise disappointing movie Iron Man 2 in which Tony Stark builds a particle accelerator in his house.\nThe movie makes it look like a complicated process, but modern large-scale accelerators are immensely more complicated than the one Stark builds. In fact, I’m amazed that such machines have been built and work as expected. One example is the Spallation Neutron Source (SNS)."
  },
  {
    "objectID": "posts/2021-05-27_painting-a-particle-beam/index.html#a-quick-tour-of-the-sns",
    "href": "posts/2021-05-27_painting-a-particle-beam/index.html#a-quick-tour-of-the-sns",
    "title": "Painting a particle beam",
    "section": "A quick tour of the SNS",
    "text": "A quick tour of the SNS\n\n\n\nFig. 1. Overhead view of the SNS.\n\n\nThe goal of the SNS is to produce extremely bright, pulsed neutron beams for neutron scattering experiments. These neutrons are produced through the process of spallation by colliding a proton beam with a Mercury target. The power of the proton beam must be as high as possible to maximize the brightness of the neutron beam, and creating such a high-power beam is a multi-step process; particles must travel all the way from the ion source on the far left to the target on the far right, passing though many different sections on their journey. We’re first going to mention the basic function of each of these sections, and then we’re going to look in more detail at the relatively small but extremely important injection region of the machine. This is the point where the HEBT meets the accumulator ring in the above diagram.\n\nIon source, front end, and linac\nThe beam originates in the ion source. As explained later, the beam is not actually made of protons at this point, but is instead made of H\\(^-\\) ions (proton + two electrons). The ion source consists of a vacuum chamber filled with gas, and an oscillating electric field which ionizes the gas to form a glowing, pink plasma. The H\\(^-\\) particles are then extracted from the plasma; I asked one of the researchers at SNS (who doesn’t work on the ion source) how exactly these ions are extracted, and they said it’s “black magic”, so I guess it’s not straightforward.\n\n\n\nFig. 2. Diagram of the SNS ion source. (Source: [1].)\n\n\nThe H\\(^-\\) beam is then accelerated to around 2.5 MeV, focused, and “chopped” into 1000 minipulses. Each minipulse is about 700 nanoseconds long, and they’re separated by a gap of about 300 nanoseconds. The dynamics in this region are strongly influenced by space charge.\nEach minipulse is now ready to be accelerated. The next section of the machine is called the linac (linear accelerator), a long, straight section whose purpose is to accelerate the minipulses up to 1 GeV (around 90% of the speed of light) while maintaining an acceptable beam size. This is done using a series of normal-conducting and superconducting radio-frequency cavities. There is a lot to talk about in the linac, but I’ll stop here since I don’t have much knowledge of this area of the machine yet.\n\n\nHEBT, injection region, and accumulator ring\nThe high-energy beam transport (HEBT, pronounced “hebbet”) guides the fully accelerated minipulse from the linac to the left edge of the accumulator ring. At this point, all the ions in the minipulse are converted to protons and injected into the accumulator ring (more on this in the next section). The minpulse takes 1 microsecond to travel around the ring, at which point a second minipulse is injected and the circulating beam doubles in intensity. This repeats 1000 times over the course of one millisecond until the final beam, called a pulse, contains around \\(1.5 \\times 10^{14}\\) protons. That seems like a lot until you consider that Avagadros number is one billion times larger! 60 of these pulses contains about the same energy as a stick of dynamite.\n\n\nRTBT and target\nFinally, the entire pulse is extracted from the ring and travels down the ring-target-beam-transport (RTBT) in which it is directed to the Mercury target, producing neutrons. These neutrons are then cooled and transported to various instrumental halls for use in neutron scattering experiments.\n\n\n\nFig. 3. Protons (yellow) bombard the Mercury target to produce neutrons (blue). Source: Jill Hemman."
  },
  {
    "objectID": "posts/2021-05-27_painting-a-particle-beam/index.html#injection",
    "href": "posts/2021-05-27_painting-a-particle-beam/index.html#injection",
    "title": "Painting a particle beam",
    "section": "Injection",
    "text": "Injection\nNow we’re going to discuss the injection region in more detail (see image below). Somehow, all the negatively charged ions need to be converted to positively charge protons, and the beam from the linac needs to merge with the circulating beam in the ring without derailing its trajectory; it’s as if the ions were trying to merge onto a busy highway. The specific method used at SNS is charge exchange injection, which we discuss first. We’ll then move on to discuss phase space painting, which is used to mitigate the effects of space charge in intense beams.\n\n\n\nFig. 4. THe SNS injection region.\n\n\n\nH\\(^-\\) charge exchange\nConsider two oppositely charged beams which have the same kinetic energy but opposite charges. Also assume that we’re dealing with point particles which do not interact with each other and have no transverse velocity. If these beams are sent thought a dipole magnet, their paths will be bent in opposite directions with the same radius of curvature. If they additionally have opposite angles of incidence, there will be a point in the dipole at which both beams are moving parallel to each other. Now imagine that, at this very instance, the charge of all the particles in one of the beams changes sign. The two beams would then be identical and would continue along the same trajectory, although there may be an offset. It’s possible to also choose the horizontal and vertical positions of the two beams such that they converge and travel along identical paths.\n\n\n\nFig. 5. Oppositely charged particles are bent in different directions in a dipole magnetic field.\n\n\nThe idea is to do this with the two beams in the SNS: the negatively charged H\\(^-\\) beam the linac and the positively charged proton beam which is circulating in the ring. Russian scientists developed a method to do this in the 1960’s using a thin foil which strips the two electrons from the Hydrogen ions but leaves the protons. The foil properties need to be chosen carefully. It needs to be the right material and thick enough to have a high stripping efficiency (number of ions successfully stripped divided by total number of ions), but not so thick that most of the protons are scattered. It also needs to be able to survive high numbers of foil hits without being destroyed. Thus, the choice of foil parameters requires a knowledge of materials science. The SNS uses diamond foils as in the following images.\n\n\n\nFig. 6. Stripper foil used for charge-exchange injection at the SNS.\n\n\n\n\nDealing with extra particles\nSome of the H\\(^-\\) hold on to their electrons as they pass through the foil, and some only lose one electron, becoming H\\(^0\\). To deal with these particles, the foil is placed in a dipole field. Because many of the H\\(^0\\) particles are in excited states, it is likely that their electron will be stripped by the magnetic field soon after the foil; this is known as Lorentz stripping. So a lot of these will become protons and join the circulating beam, just a bit late to the party. The remaining non-protons continue away from the ring and encounter another foil which removes the electrons so that they can be guided to a beam dump. There is also the need to catch the stripped electrons, which can have significant kinetic energies, but I won’t discuss that here.\n\n\nIs Liouville’s theorem violated?\nThose familiar with Liouville’s theorem may object to the charge-exchange method. Liouville’s theorem applies to any system which obeys Hamilton’s equations:\n\\[ \\dot{\\mathbf{q}} = \\frac{\\partial\\mathbf{H}}{\\partial\\mathbf{p}} ,\\quad \\dot{\\mathbf{p}} = -\\frac{\\partial\\mathbf{H}}{\\partial\\mathbf{q}},\\]\nwhere \\(\\mathbf{q}\\) are the coordinates and \\(\\mathbf{p}\\) are the momenta. Imagine we took a volume of phase space and started to fill it with particles; in fact, we fill all of the infinite number of points inside the volume. Then we evolve the system in time. The final distribution of particles may have changed shape, but Liouville’s theorem states that its volume will not have changed. Mathematically, this is due to Hamilton’s equations being equivalent to a coordinate transformation whose Jacobian has a determinant equal to one. So, the objection is that the phase space volume of the entire system (linac beam + circulating beam) seems to decrease when they are merged, i.e., the linac beam is stacked directly on top of the circulating beam, and that this should be disallowed by Liouville’s theorem. Is this true?\nI read a paper by A. Ruggiero which helped to clarify this issue [2]. The key point is that Liouville’s theorem deals with distributions rather than finite numbers of particles. Any finite number of particles will not fill up every point in phase space, so there is nothing preventing another finite number of particles from being added to the empty regions. The limitation is that it’s not clear how to guide two beams of the same charge to the same position using dipole magnets; hence, charge exchange. I should note, however, that this does not seem to be the explanation put forth in the talks I’ve heard on this subject. I’m planning to discuss this with some other people in the field.\n\n\nThe future: lasers\nA major research project at the SNS is to demonstrate laser-assisted charge exchange (LACE), in which a laser is used to excite the ions and a dipole magnet is used to strip the magnets intead of a foil. This would overcome the scattering losses from foils as well as their finite lifetimes; it is a very promising approach as machines continue to increase in power [3]."
  },
  {
    "objectID": "posts/2021-05-27_painting-a-particle-beam/index.html#phase-space-painting",
    "href": "posts/2021-05-27_painting-a-particle-beam/index.html#phase-space-painting",
    "title": "Painting a particle beam",
    "section": "Phase space painting",
    "text": "Phase space painting\nIn the last section, we assumed that the particles didn’t interact with each other, but in reality, space charge is the fundamental limit on the intensity in high-power hadron accelerators. Injecting at the same position in space will cause the beam to become very dense, and the beam will then expand due to the increased space charge forces. It’s likely that this will produce a very non-uniform distribution which, as mentioned in this post, is undesirable. This is the motivation for so-called phase space painting or simply painting. The idea is to change the transverse position and momentum of the circulating beam over time in order to slowly fill or “paint” the beam in phase space and hopefully produce a more uniform density beam. Another motivation for painting is to avoid excessive foil hits, since these lead to shorter foil lifetimes and also beam scattering.\n\nTime-dependent kicker magnets\nHere is a zoomed in view of the injection region.\n\n\n\n\nFig. 7. Zoomed view of the injection region.\n\n\n\nThe blue elements which aren’t labeled in the ring are just quadrupoles used to focus the beam. This leaves the “bump” or “kicker” magnets and the “Chicane” magnets. These are both dipoles, but they are a bit different. The chicane dipoles provide a fixed horizontal bump to the closed orbit so that it is aligned with the beam from the linac. The kickers, on the other hand, can move the closed orbit horizontally or vertically, and they’re time-dependent. Regarding the latter point, the current from the magnet’s power supply, and therefore the magnetic field, can be varied during injection. Let’s take a look at the vertical closed orbit with the kickers turned on (black line).\n\n\n\nFig. 8. Magnets in the injection region.\n\n\nThe dark blue boxes are quadrupoles, the red boxes are Chicane dipoles, and the remaining elements are the horizontal (green) and vertical (yellow) kickers. Without the kickers, the closed orbit will just go straight through the center of each magnet, and a similar thing holds in the horizontal plane. Thus, we have control over the horizontal and vertical position of the circulating beam relative to the injected beam. But there is also the possibility that the trajectory is converging or diverging at the foil, so we also have control over the horizontal and vertical circulating beam slope relative to the injected beam. These eight kickers therefore give full control over the transverse phase space coordinates of the circulating beam relative to the injected beam at every point during the injection.\n\n\nProduction painting\nThe time-dependence of each kicker magnet is determined by a waveform which determines the current given to its power supply as a function of time; for example, we could have a linear waveform, square root waveform, etc. Choosing these waveforms amounts to choosing the initial and final position/slope of the circulating beam, as well as the rate of change in the position/slope. The standard “production” scheme in the SNS (as in neutron production) is to use a square root waveform so that\n\\[ x_{inj} - x_{co} = (x_{max} - x_{min}) \\sqrt{t / t_{max}} + x_{min}, \\] \\[ y_{inj} - y_{co} = (y_{max} - y_{min}) \\sqrt{t / t_{max}} + y_{min}, \\]\nwhere the co subscript means “closed orbit” and inj means “injected beam”. The slope of the circulating beam is kept at zero in this scheme.\nIt’s probably best to use some visualizations at this point. I simulated the injection painting using PyORBIT; included in this simulation are effects such as space charge, nonlinear magnetic fringe fields, scattering from the foil, etc., so the results should be somewhat similar to the real world. 260 simulation particles were injected on each turn to give a final number of 260,000, which should provide good statistics for the space charge solver which operates on a \\(128 \\times 128\\) transverse grid. The whole simulation took a few hours to run on my laptop. (The space charge solver I used makes some approximations in the longitudinal dimension; a more realistic solver will push the execution time from a few hours to a few days.)\nThe following animation shows this simulated beam at the injection point during the first 35 turns in the ring. The off-diagonal subplots show the correlations between the four phase space variables (a sample of 10,000 particles is used) and the on-diagonal subplots show the histograms for each variable. The foil location is shown by the red dot in the \\(x\\)-\\(y\\) plane.\n\n\n\nFig. 9. The first few turns of the injection process.\n\n\nKeep in mind that each little cluster is actually a bunch of particles; it’s hard to resolve because the width is small compared to the full beam. Notice that, since the circulating and injected beams are offset to begin with, the injected particles start to trace ellipses in the \\(x\\)-\\(x'\\) and \\(y\\)-\\(y'\\) projections. The frequencies at which the particles oscillate in each plane are not the same, so the path in \\(x\\)-\\(y\\) space is constantly changing, eventually filling a rectangular region. The next animation shows the beam over all 1000 turns.\n\n\n\nFig. 10. All 1000 turns of the injection process.\n\n\nNotice that the beam size is slowly increasing, and also that the density is steadily increasing; this is only apparent from the histograms since I’m using a random sample of particles in the scatter plots. Space charge, as well as nonlinear effects, tend to round the hard edges of the the originally rectangular beam. The beam also exhibits some interesting dynamics after turn 100, and again after turn 800, when it begins to tilt back and forth, which is probably due to space charge coupling the two planes. In the end, the beam has a somewhat uniform density, is not tilted, is not rotating, and is quite intense (\\(1.5 \\times 10^{14}\\) particles), so the basic goals of the painting scheme have been achieved. The beam can now be extracted and collided with the target to produce neutrons.\n\n\n\nFig. 1. Overhead view of the SNS.\nFig. 2. Diagram of the SNS ion source. (Source: [1].)\nFig. 3. Protons (yellow) bombard the Mercury target to produce neutrons (blue). Source: Jill Hemman.\nFig. 4. THe SNS injection region.\nFig. 5. Oppositely charged particles are bent in different directions in a dipole magnetic field.\nFig. 6. Stripper foil used for charge-exchange injection at the SNS.\nFig. 7. Zoomed view of the injection region.\nFig. 8. Magnets in the injection region.\nFig. 9. The first few turns of the injection process.\nFig. 10. All 1000 turns of the injection process."
  },
  {
    "objectID": "posts/2022-02-11_injection-figures/index.html",
    "href": "posts/2022-02-11_injection-figures/index.html",
    "title": "Some figures to illustrate beam injection and accumulation",
    "section": "",
    "text": "One method to create high-intensity hadron beams is charge-exchange injection: negatively charged particles are repeatedly accelerated in a linear accelerator (linac) and injected into a circular accelerator (ring), accumulating charge in the ring over time. I created a simple animation to visualize this process.\n\nimport numpy as np\nfrom scipy.stats import truncnorm\nfrom matplotlib import animation\nfrom matplotlib import pyplot as plt\nimport proplot as pplt\nimport seaborn as sns\n\nAnd some helper functions.\n\ndef rotation_matrix(angle):\n    \"\"\"Clockwise rotation matrix.\"\"\"\n    cs, sn = np.cos(angle), np.sin(angle)\n    return np.array([[cs, sn], [-sn, cs]])\n\n\ndef apply(M, X):\n    \"\"\"Apply matrix `M` to each row of `X`.\"\"\"\n    return np.apply_along_axis(lambda row: np.matmul(M, row), 1, X)\n\nThe Bunch class stores the \\(x\\), \\(x'\\), and \\(z\\) coordinates, which are measured relative to the bunch centroid. It also stores \\(s\\), the location of the bunch center along the beam line. To transport the beam, we just increase \\(s\\). We’ll let the particles perform harmonic oscillations in the transverse plane. The only tricky part is to convert to an aerial view and to bend the circulating beam around the ring.\n\nclass Bunch:\n    \"\"\"Class to store bunch coordinates.\n\n    Attributes\n    ----------\n    X : ndarray, shape (n, 3)\n        The bunch coordinates (x, x', z).\n    s : The location of the bunch center along the beamline.\n    \"\"\"\n\n    def __init__(self, n, zrms=0.1, aspect=0.1):\n        \"\"\"Constructor.\n\n        n : int\n            The number of particles in the bunch.\n        zrms : float\n            The rms width of the z distribution.\n        aspect : float\n            The rms width of the x and x' distribution relative to `zrms`.\n        \"\"\"\n        self.n, self.zrms, self.aspect = n, zrms, aspect\n        self.X = np.random.normal(\n            scale=[aspect * zrms, aspect * zrms, zrms],\n            size=(n, 3),\n        )\n        self.s = 0.0\n\n    def transport(self, distance, period_length):\n        \"\"\"Propagate the bunch along the beamline.\n\n        Parameters\n        ----------\n        distance : float\n            The distance along the beamline.\n        period_length : float\n            The period length of transverse focusing. The particles perform\n            transverse harmonic oscillations.\n        \"\"\"\n        self.X[:, :2] = apply(\n            rotation_matrix(2.0 * np.pi * (distance / period_length)),\n            self.X[:, :2],\n        )\n        self.s += distance\n\n    def global_coords_line(self, x0=0.0, y0=0.0, angle=0.0):\n        \"\"\"Return global (top-view) coordinates for straight-line transport.\n\n        Parameters\n        ----------\n        x0, y0 : float\n            Initial bunch centroid coordinates.\n        angle : float\n            The angle of the beamline.\n\n        Returns\n        -------\n        ndarray, same shape as self.X\n            The top-view coordinates.\n        \"\"\"\n        # The initial bunch is vertical when viewed from above. Point it\n        # in the right direction.\n        R = rotation_matrix(angle + 0.5 * np.pi)\n        x, y = apply(R, self.X[:, [0, 2]]).T\n        # Slide along the beamline to the correct location.\n        x += x0 + self.s * np.cos(angle)\n        y += y0 + self.s * np.sin(angle)\n        coords = np.vstack([x, y]).T\n        return coords\n\n    def global_coords_ring(self, ring_length):\n        \"\"\"Return global (top-view) coordinates for circular transport.\n\n        Parameters\n        ----------\n        ring_length : float\n            The circumference of the ring. We assume it is centered\n            on the origin.\n\n        Returns\n        -------\n        ndarray, same shape as self.X\n            The top-view coordinates.\n        \"\"\"\n        # The initial bunch is vertical when viewed from above. Make it\n        # horizontal.\n        R = rotation_matrix(0.5 * np.pi)\n        x, y = apply(R, self.X[:, [0, 2]]).T\n        # Put the bunch at the top of the ring and account for ring curvature.\n        ring_radius = ring_length / (2.0 * np.pi)\n        y += np.sqrt(ring_radius**2 - x**2)\n        # Slide along the beamline to the correct location.\n        phi = 2.0 * np.pi * (self.s % ring_length) / ring_length\n        R = rotation_matrix(phi)\n        coords = np.vstack([x, y]).T\n        coords = apply(R, coords)\n        return coords\n\nTo run the “simulation”, we will create two bunches: the first circulates in the ring, and the second is repeatedly stepped through the linac. After one revolution, the second bunch gives its particles to the first bunch.\n\n# Settings\nn_turns = 6\nn_steps = 50  # steps per turn.\nring_length = 2.0 * np.pi\nstep_length = ring_length / n_steps\nperiod_length = ring_length / 3.18\nn = 50  # size of minipulse\nx0, y0 = (-ring_length, 1.0)  # start of the linac\nzrms = 0.19  # rms bunch length\naspect = 0.09  # horizontal/vertical aspect ratio\n\n# Calculate and store the bunch coordinates.\nnp.random.seed(0)\nlbunch = Bunch(n, zrms, aspect)\nrbunch = Bunch(n, zrms, aspect)\nlcoords = [np.copy(lbunch.global_coords_line(x0, y0))]\nrcoords = [np.copy(rbunch.global_coords_ring(ring_length))]\nfor turn in range(n_turns):\n    for step in range(n_steps):\n        for bunch in [lbunch, rbunch]:\n            bunch.transport(step_length, period_length)\n        lcoords.append(lbunch.global_coords_line(x0, y0))\n        rcoords.append(rbunch.global_coords_ring(ring_length))\n    rbunch.X = np.vstack([rbunch.X, lbunch.X])\n    lbunch.__init__(n, zrms, aspect)\n\nHere is the animation:\n\n\nCode\nfig, ax = pplt.subplots(figwidth=6.7, figheight=3.6, aspect=1.0)\nkws = dict(zorder=0, color=\"black\", lw=0.75, alpha=0.06)\nax.plot([-ring_length, 0.0], [1.0, 1.0], **kws)\npsi = np.linspace(0.0, 2.0 * np.pi, 1000)\nax.plot(np.cos(psi), np.sin(psi), **kws)\nax.format(\n    xlim=(-0.5 * ring_length, 1.2),\n    ylim=(-1.2, 1.2),\n    xticks=[],\n    yticks=[],\n    yspineloc=\"neither\",\n    xspineloc=\"neither\",\n)\nplt.close()\n\nplot_kws = dict(marker=\".\", lw=0, ms=4.0, alpha=0.3, ec=\"None\")\n(line1,) = ax.plot([], [], color=\"black\", **plot_kws)\n(line2,) = ax.plot([], [], color=\"pink9\", **plot_kws)\n\n\ndef update(i):\n    line1.set_data(rcoords[i][:, 0], rcoords[i][:, 1])\n    line2.set_data(lcoords[i][:, 0], lcoords[i][:, 1])\n\n\nframes = n_steps * n_turns + 1\nfps = 0.5 * (frames / n_turns)\nanim = animation.FuncAnimation(fig, update, frames=frames, interval=(1000.0 / fps))\n\n\n\n\n\n\n\nIt could be interesting to display the results of a simulation in this way — usually they are displayed in the frame of the beam centroid."
  },
  {
    "objectID": "posts/2022-02-11_injection-figures/index.html#charge-exchange-injection",
    "href": "posts/2022-02-11_injection-figures/index.html#charge-exchange-injection",
    "title": "Some figures to illustrate beam injection and accumulation",
    "section": "",
    "text": "One method to create high-intensity hadron beams is charge-exchange injection: negatively charged particles are repeatedly accelerated in a linear accelerator (linac) and injected into a circular accelerator (ring), accumulating charge in the ring over time. I created a simple animation to visualize this process.\n\nimport numpy as np\nfrom scipy.stats import truncnorm\nfrom matplotlib import animation\nfrom matplotlib import pyplot as plt\nimport proplot as pplt\nimport seaborn as sns\n\nAnd some helper functions.\n\ndef rotation_matrix(angle):\n    \"\"\"Clockwise rotation matrix.\"\"\"\n    cs, sn = np.cos(angle), np.sin(angle)\n    return np.array([[cs, sn], [-sn, cs]])\n\n\ndef apply(M, X):\n    \"\"\"Apply matrix `M` to each row of `X`.\"\"\"\n    return np.apply_along_axis(lambda row: np.matmul(M, row), 1, X)\n\nThe Bunch class stores the \\(x\\), \\(x'\\), and \\(z\\) coordinates, which are measured relative to the bunch centroid. It also stores \\(s\\), the location of the bunch center along the beam line. To transport the beam, we just increase \\(s\\). We’ll let the particles perform harmonic oscillations in the transverse plane. The only tricky part is to convert to an aerial view and to bend the circulating beam around the ring.\n\nclass Bunch:\n    \"\"\"Class to store bunch coordinates.\n\n    Attributes\n    ----------\n    X : ndarray, shape (n, 3)\n        The bunch coordinates (x, x', z).\n    s : The location of the bunch center along the beamline.\n    \"\"\"\n\n    def __init__(self, n, zrms=0.1, aspect=0.1):\n        \"\"\"Constructor.\n\n        n : int\n            The number of particles in the bunch.\n        zrms : float\n            The rms width of the z distribution.\n        aspect : float\n            The rms width of the x and x' distribution relative to `zrms`.\n        \"\"\"\n        self.n, self.zrms, self.aspect = n, zrms, aspect\n        self.X = np.random.normal(\n            scale=[aspect * zrms, aspect * zrms, zrms],\n            size=(n, 3),\n        )\n        self.s = 0.0\n\n    def transport(self, distance, period_length):\n        \"\"\"Propagate the bunch along the beamline.\n\n        Parameters\n        ----------\n        distance : float\n            The distance along the beamline.\n        period_length : float\n            The period length of transverse focusing. The particles perform\n            transverse harmonic oscillations.\n        \"\"\"\n        self.X[:, :2] = apply(\n            rotation_matrix(2.0 * np.pi * (distance / period_length)),\n            self.X[:, :2],\n        )\n        self.s += distance\n\n    def global_coords_line(self, x0=0.0, y0=0.0, angle=0.0):\n        \"\"\"Return global (top-view) coordinates for straight-line transport.\n\n        Parameters\n        ----------\n        x0, y0 : float\n            Initial bunch centroid coordinates.\n        angle : float\n            The angle of the beamline.\n\n        Returns\n        -------\n        ndarray, same shape as self.X\n            The top-view coordinates.\n        \"\"\"\n        # The initial bunch is vertical when viewed from above. Point it\n        # in the right direction.\n        R = rotation_matrix(angle + 0.5 * np.pi)\n        x, y = apply(R, self.X[:, [0, 2]]).T\n        # Slide along the beamline to the correct location.\n        x += x0 + self.s * np.cos(angle)\n        y += y0 + self.s * np.sin(angle)\n        coords = np.vstack([x, y]).T\n        return coords\n\n    def global_coords_ring(self, ring_length):\n        \"\"\"Return global (top-view) coordinates for circular transport.\n\n        Parameters\n        ----------\n        ring_length : float\n            The circumference of the ring. We assume it is centered\n            on the origin.\n\n        Returns\n        -------\n        ndarray, same shape as self.X\n            The top-view coordinates.\n        \"\"\"\n        # The initial bunch is vertical when viewed from above. Make it\n        # horizontal.\n        R = rotation_matrix(0.5 * np.pi)\n        x, y = apply(R, self.X[:, [0, 2]]).T\n        # Put the bunch at the top of the ring and account for ring curvature.\n        ring_radius = ring_length / (2.0 * np.pi)\n        y += np.sqrt(ring_radius**2 - x**2)\n        # Slide along the beamline to the correct location.\n        phi = 2.0 * np.pi * (self.s % ring_length) / ring_length\n        R = rotation_matrix(phi)\n        coords = np.vstack([x, y]).T\n        coords = apply(R, coords)\n        return coords\n\nTo run the “simulation”, we will create two bunches: the first circulates in the ring, and the second is repeatedly stepped through the linac. After one revolution, the second bunch gives its particles to the first bunch.\n\n# Settings\nn_turns = 6\nn_steps = 50  # steps per turn.\nring_length = 2.0 * np.pi\nstep_length = ring_length / n_steps\nperiod_length = ring_length / 3.18\nn = 50  # size of minipulse\nx0, y0 = (-ring_length, 1.0)  # start of the linac\nzrms = 0.19  # rms bunch length\naspect = 0.09  # horizontal/vertical aspect ratio\n\n# Calculate and store the bunch coordinates.\nnp.random.seed(0)\nlbunch = Bunch(n, zrms, aspect)\nrbunch = Bunch(n, zrms, aspect)\nlcoords = [np.copy(lbunch.global_coords_line(x0, y0))]\nrcoords = [np.copy(rbunch.global_coords_ring(ring_length))]\nfor turn in range(n_turns):\n    for step in range(n_steps):\n        for bunch in [lbunch, rbunch]:\n            bunch.transport(step_length, period_length)\n        lcoords.append(lbunch.global_coords_line(x0, y0))\n        rcoords.append(rbunch.global_coords_ring(ring_length))\n    rbunch.X = np.vstack([rbunch.X, lbunch.X])\n    lbunch.__init__(n, zrms, aspect)\n\nHere is the animation:\n\n\nCode\nfig, ax = pplt.subplots(figwidth=6.7, figheight=3.6, aspect=1.0)\nkws = dict(zorder=0, color=\"black\", lw=0.75, alpha=0.06)\nax.plot([-ring_length, 0.0], [1.0, 1.0], **kws)\npsi = np.linspace(0.0, 2.0 * np.pi, 1000)\nax.plot(np.cos(psi), np.sin(psi), **kws)\nax.format(\n    xlim=(-0.5 * ring_length, 1.2),\n    ylim=(-1.2, 1.2),\n    xticks=[],\n    yticks=[],\n    yspineloc=\"neither\",\n    xspineloc=\"neither\",\n)\nplt.close()\n\nplot_kws = dict(marker=\".\", lw=0, ms=4.0, alpha=0.3, ec=\"None\")\n(line1,) = ax.plot([], [], color=\"black\", **plot_kws)\n(line2,) = ax.plot([], [], color=\"pink9\", **plot_kws)\n\n\ndef update(i):\n    line1.set_data(rcoords[i][:, 0], rcoords[i][:, 1])\n    line2.set_data(lcoords[i][:, 0], lcoords[i][:, 1])\n\n\nframes = n_steps * n_turns + 1\nfps = 0.5 * (frames / n_turns)\nanim = animation.FuncAnimation(fig, update, frames=frames, interval=(1000.0 / fps))\n\n\n\n\n\n\n\nIt could be interesting to display the results of a simulation in this way — usually they are displayed in the frame of the beam centroid."
  },
  {
    "objectID": "posts/2022-02-11_injection-figures/index.html#phase-space-painting",
    "href": "posts/2022-02-11_injection-figures/index.html#phase-space-painting",
    "title": "Some figures to illustrate beam injection and accumulation",
    "section": "Phase space painting",
    "text": "Phase space painting\nIf there is significant overlap between the injected and circulating beams, a huge charge density will be created and the beam size will blow up due to the electric forces between the particles. To avoid this, the relative distance and angle between the beams is varied over time. The idea is to inject new particles into unoccupied regions of the transverse phase space (\\(x\\)-\\(x'\\)-\\(y\\)-\\(y'\\)) of the circulating beam. This process is called phase space painting, or simply painting.\nThere are many possible painting methods. The two most common methods are correlated painting and anti-correlated painting. In correlated painting, initial particles are injected directly onto the closed-orbit in the ring, then moved away from the origin at a rate proportional to the square root of time. The angle between the beams is always zero. In other words,\n\\[\n\\begin{aligned}\n    x(t) &= x_{max} \\sqrt{t}, \\\\\n    y(t) &= y_{max} \\sqrt{t}, \\\\\n    x'(t) &= 0, \\\\\n    y'(t) &= 0.\n\\end{aligned}\n\\tag{1}\n\\]\nHere \\(t\\) is time, normalized to the range [0, 1]. Anti-correlated painting reverses this process in one of the planes:\n\\[\n\\begin{aligned}\n    x(t) &= x_{max} \\sqrt{t}, \\\\\n    y(t) &= y_{max} \\sqrt{1 - t}, \\\\\n    x'(t) &= 0, \\\\\n    y'(t) &= 0\n\\end{aligned}\n\\tag{2}\n\\]\nRealistic painting simulations take a long time. I sometimes just want to see the turn-by-turn distribution without particle interactions or nonlinear effects. In this case, propagating the beam through the ring reduces to matrix multiplication, and if we view the distribution in normalized \\(x\\)-\\(x'\\) and \\(y\\)-\\(y'\\) phase space, the transfer matrix reduces to a rotation matrix in those planes. The rotation angle is \\(2\\pi\\nu_x\\) in the \\(x\\)-\\(x'\\) plane and \\(2\\pi\\nu_y\\) in the vertical plane, where \\(\\nu_{x}\\) and \\(\\nu_y\\) are the number of phase space oscillations per turn in either plane.\nThis can still take a while if many particles are used — the circulating beam needs to be rotated in the transverse plane on each turn. We can reduce the time in the following way. Assume we want to know the distribution on turn \\(t\\). First, generate \\(t\\) minipulses from the linac and put them at the origin. Second, slide each minipulse to its final amplitude using Eq.\\(~\\)(1) or Eq.\\(~\\)(2). Third, rotate each minipulse in \\(x\\)-\\(x'\\) and \\(y\\)-\\(y'\\) according to when it was injected: the first minipulse does \\(t\\) turns, the second does \\(t - 1\\) turns, etc. until the last minipulse, which doesn’t move. The following Painter class performs these steps.\n\nclass Painter:\n    \"\"\"Class to illustrate phase space painting.\n\n    (Linear approximation, non-interacting particles, normalized phase space.)\n    \"\"\"\n\n    def __init__(self, n_turns=3000, n_inj=500, inj_rms=0.15, inj_cut=3.0):\n        \"\"\"Constructor.\n\n        n_turns : int\n            The number of turns before accumulation is complete.\n        n_inj : int\n            The number of particles injected per turn.\n        inj_rms, inj_cut : float\n            The transverse rms width and cut-off of each Gaussian minipulse.\n        \"\"\"\n        self.n_inj = n_inj\n        self.n_turns = n_turns\n        self.inj_rms = inj_rms\n        self.inj_cut = np.repeat(inj_cut, 4)\n        self.times = np.linspace(0.0, 1.0, n_turns)\n        self.set_xmax()\n        self.is_initialized = False\n\n    def set_xmax(self, xmax=None):\n        \"\"\"Set the final injection point in phase space (x, x', y, y').\"\"\"\n        if xmax is None:\n            xmax = [1.0, 0.0, 1.0, 0.0]\n        self.xmax = np.array(xmax)\n\n    def inj_point(self, turn, method=\"correlated\"):\n        \"\"\"Return the phase space coordinates of the injection point.\"\"\"\n        t = self.times[turn]\n        if method == \"correlated\":\n            return self.xmax * np.sqrt(t)\n        elif method == \"anti-correlated\":\n            tau1 = np.sqrt(t)\n            tau2 = np.sqrt(1.0 - t)\n            return np.multiply(self.xmax, [tau1, tau1, tau2, tau2])\n        else:\n            raise ValueError(\"Invalid method\")\n\n    def generate_minipulse(self):\n        \"\"\"Generate a minipulse at the origin.\"\"\"\n        X = truncnorm.rvs(\n            scale=self.inj_rms,\n            size=(self.n_inj, 4),\n            a=-self.inj_cut,\n            b=+self.inj_cut,\n        )\n        return X\n\n    def initialize(self):\n        \"\"\"Initialize all minipulses at the origin.\"\"\"\n        self.coords0 = [self.generate_minipulse() for _ in range(self.n_turns)]\n        self.is_initialized = True\n\n    def paint(self, nux, nuy, turns=1, method=\"correlated\"):\n        \"\"\"Return the painted distribution.\n\n        Parameters\n        ----------\n        nux, nux : float\n            Horizontal and vertical tunes (oscillations per turn).\n        turns : int\n            Paint this many turns.\n        method : {'correlated', 'anti-correlated'}\n            Correlated painting paints min-to-max in both x-x' and y-y'.\n            Anti-correlated painting paints min-to-max in x-x' and max-to-min\n            in y-y'.\n\n        Returns\n        -------\n        X : ndarray, shape (turns * n, 4)\n            The painted phase space distribution.\n        \"\"\"\n        if not self.is_initialized:\n            self.initialize()\n        coords0 = np.copy(self.coords0[:turns])\n        # Move each minipulse to its final amplitude.\n        for turn in range(turns):\n            coords0[turn] += self.inj_point(turn, method)\n        # Rotate each minipulse by the appropriate amount of turns.\n        X = []\n        for turn, X0 in enumerate(coords0):\n            M = np.zeros((4, 4))\n            _turns = turns - turn + 1\n            M[:2, :2] = rotation_matrix(2.0 * np.pi * nux * _turns)\n            M[2:, 2:] = rotation_matrix(2.0 * np.pi * nuy * _turns)\n            X.append(apply(M, X0))\n        return np.vstack(X)\n\nWe’ll also need a method to plot the 2D projections of the 4D phase space distribution during injection.\n\ndef plot_projections(coords, method, painter):\n    limits = 4 * [(-1.5, 1.5)]\n    indices = [(0, 2), (0, 1), (2, 3), (0, 3), (2, 1), (1, 3)]\n    line_kws = dict(color=\"black\", lw=0.4, alpha=0.1)\n    fig, axs = pplt.subplots(\n        nrows=len(coords),\n        ncols=6,\n        figwidth=6.7,\n        space=0,\n        spanx=False,\n        spany=False,\n        sharex=False,\n        sharey=False,\n    )\n    axs.format(xticks=[], yticks=[], xticklabels=[], yticklabels=[])\n    for col in range(axs.shape[1]):\n        i, j = indices[col]\n        axs[:, col].format(xlim=limits[i], ylim=limits[j])\n        dims = {0: \"x\", 1: \"x'\", 2: \"y\", 3: \"y'\"}\n        axs[0, col].set_title(\"{}-{}\".format(dims[i], dims[j]))\n    for row, (turn, X) in enumerate(zip(turns_list, coords)):\n        xinj = painter.inj_point(turn, method)\n        time = painter.times[turn]\n        axs[row, 0].set_ylabel(\"t = {:.2f}\".format(time), fontsize=\"small\")\n        for col in range(axs.shape[1]):\n            ax = axs[row, col]\n            i, j = indices[col]\n            x, y = X[:, i], X[:, j]\n            bins = 125 if row == 0 else \"auto\"\n            sns.histplot(\n                ax=ax,\n                x=x,\n                y=y,\n                ec=\"None\",\n                cmap=\"mono\",\n                binrange=(limits[i], limits[j]),\n                bins=bins,\n            )\n            ax.axvline(xinj[i], **line_kws)\n            ax.axhline(xinj[j], **line_kws)\n    return axs\n\nLet’s first look at correlated painting. We’ll assume that the lattice is uncoupled and has unequal tunes.\n\nnux = 0.1810201\nnuy = nux - 0.143561\nn_turns = 3000\nturns_list = np.linspace(100, n_turns - 1, 5).astype(int)\npainter = Painter(n_turns)\npainter.set_xmax([1.0, 0.0, 0.0, 1.0])\n\nmethod = 'correlated'\ncoords = [painter.paint(nux, nuy, turns, method) for turns in turns_list]\naxs = plot_projections(coords, method, painter)\naxs.format(suptitle='Correlated painting')\n\n\n\n\n\n\n\n\nAll plots are shown in the rest frame of the circulating beam at a fixed position in the ring; new particles are injected at the intersection of the faint horizontal and vertical lines. The reason for the square root time-dependence was to create the uniform density ellipses in \\(x\\)-\\(x'\\) and \\(y\\)-\\(y'\\). The \\(x\\)-\\(y\\) distriubtion is rectangular because of the unequal tunes. Interestingly, there are nonzero higher-order correlations between \\(x\\) and \\(y\\), as seen in the higher density cross. The distribution will not look exactly like this during real injection since space charge and other nonlinear effects will perturb the particle oscillations; however, this large peak density is not ideal. For example, at the Spallation Neutron Source (SNS), it is important to minimize the peak density of the beam when it collides with the liquid mercury target. Also, the electric field within the beam in the figure has a nonlinear dependence on the particle coordinates, leading to other undesirable effects. Therefore, the SNS uses correlated painting with an initial offset, creating a hollow beam that eventually fills in during accumulation. The final beam has a more uniform density.\nLet’s now look at anti-correlated painting.\n\n#collapse\nmethod = 'anti-correlated'\ncoords = [painter.paint(nux, nuy, turns, method) for turns in turns_list]\naxs = plot_projections(coords, method, painter)\naxs.format(suptitle='Anti-correlated painting')\n\n\n\n\n\n\n\n\nIncredibly, although the distribution has a strange shape in the \\(x\\)-\\(y\\) plane throughout injection, the end result is a uniform density ellipse. In fact, it is an approximate KV distribution in which particles are uniformly spaced on a 4D ellipsoid in phase space. Since the KV distribution linearizes the space charge force, this painting method seems very attractive. But when space charge is included during injection, the nonlinear forces at early times during injection will not preserve the KV structure. Nonetheless, anti-correlated painting can have benefits over correlated painting.\nWe have been studying an alternative painting method called elliptical painting. In elliptical painting, the injected coordinates are scaled along an eigenvector of the ring transfer matrix.\n\\[\n\\mathbf{x}(t) = Re \\{ \\sqrt{2 J_l} \\mathbf{v}_l e^{-i\\psi_l}  \\} \\sqrt{t}.\n\\]\nHere \\(\\mathbf{x} = (x, x', y, y')^T\\), \\(\\mathbf{v}_l\\) is the eigenvector, \\(J_l\\) is an amplitude, \\(\\psi_l\\) is a phase, \\(Re\\{z\\}\\) selects the real component of \\(z\\), and \\(l = 1,2\\). This method takes advantage of coupled motion in the ring and generates a Danilov distribution, which has many of the same attractive properties as the KV distribution. The main difference is that it has angular momentum.\nThere is more to say about this method, but for now, let’s just look at the distribution it creates in the linear approximation. One way to perform elliptical painting is by setting equal tunes in the ring. In this case, one eigenvector is \\(\\mathbf{v} = (x_{max}, 0, 0, y_{max}')^T\\).\n\nmethod = 'correlated'\npainter.set_xmax([1.0, 0.0, 0.0, 1.0])\ncoords = [painter.paint(nux, nux, turns, method) for turns in turns_list]\naxs = plot_projections(coords, method, painter)\n\n\n\n\n\n\n\n\nThe big change is that we are now varying the angle between the beams — \\(y'\\) in this case — not just the distance. This produces rotation; the beam is rigidly rotating counterclockwise in the above figure. Again, the end result is a uniform density ellipse in the \\(x\\)-\\(y\\) plane. But notice the difference from anti-correlated painting: the uniform density ellipse is maintained at all times. In fact, the Danilov structure is maintained at all times. For this reason, an approximate Danilov distribution would be produced even with space charge. In fact, realistic simulations predict that this would remain true even with realistic nonlinear effects. This is good news because the Danilov distribution has even more attractive properties than the KV distribution. We are in the midst of testing this prediction at the SNS."
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "Austin's Blog",
    "section": "",
    "text": "The purpose of this blog is to share my research in accelerator physics and learn about other topics. Outside of physics, I’m especially interested in philosophy of science, philosophy of mind, and philosophy of religion.\nLet me know if you think of a better title…\nHomepage: https://austin-hoover.github.io"
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "Links",
    "section": "",
    "text": "Blogs\n\n\nAlexander Pruss\nAsymptotia\nBackreaction\nBactra\nChina Talk\nCondensed Concepts\nEdward Feser\nex-apologist\nFake Nous\nlillog\nNanoscale Views\nNot Even Wrong\nNotes from a data witch\nOf particular significance\nPhysics Matt\nPreposteroous Universe\nScott Hawley\nShtetl-Optimized\nThe Splintered Mind\nStatistical Modeling\nDanny Yee’s Book Reviews\n\nPeople\n\nAlexander Pruss\nAdrian Oeftiger\nBrian Cutter\nCharles Sebens\nChristopher Gregory Weaver\nDavid Kipping\nEddie Chen\nGraham Oppy\nHairong Qi\nHelen Seward\nIsaac Wilhelm\nJacob Barandes\nJeff Holmes\nJohn Norton\nJoseph Schmidt\nJustin Mooney\nKerry McKenzie\nMelanie Mitchell\nMichael Heumer\nNick Bostrom\nPhilip Goff\nSimono Friedrich\nTaylor Cyr\nThomas Metcalf\nTim Oconor\nTyler Hildebrand\n\nPlaces\n\nCERN\nORNL\nSNS\nUTK physics\n\nPhysics journals\n\nChaos\nEntropy\nNature\nPRAB\nPRE\nPRL\nScience\n\nPreprint servers\n\nArXiv\nArXiv (accelerator physics)\nArXiv (astrophysics of galaxies)\nArXiv (chaotic dynamics)\nArXiv (computational physics)\nArXiv (data analysis, statistics, and probability)\nArXiv (history and philosophy of physics)\nPhilSci\nphilpeople.org\n\nBibliography on the problem of evil\nlesswrong\nLittle debates\nEinstein papers\nProbability theory as extended logic\nArticles\n\nWhy academics stink at writing\nThe preprint revolution\nRunescape helps Venezuelans survive\nZombie nouns\nWhat is it like to be a philosopher?"
  },
  {
    "objectID": "notes/books/index.html",
    "href": "notes/books/index.html",
    "title": "Books",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nEichmann in Jerusalem: A Report on the Banality of Evil\n\n\nHannah Arendt\n\n\n\n\nPhysics of Collective Beam Instabilities in High Energy Accelerators\n\n\nA. Chao\n\n\n\n\nThe Sense of Style\n\n\nS. Pinker\n\n\n\n\nDeep Learning: Foundations and Concepts\n\n\nC. Bishop, H. Bishop\n\n\n\n\nEntropic Physics\n\n\nAriel Caticha\n\n\n\n\nProbability Theory: The Logic of Science\n\n\nE. T. Jaynes\n\n\n\n\nThe Ethics of Abortion\n\n\nChristopher Kaczor\n\n\n\n\nData Analysis: A Bayesian Tutorial\n\n\nD. Sivia, J. Skilling\n\n\n\n\nBayesian Data Analysis\n\n\nA. Gelman, J. Carlin, H. Stern, D. Rubin\n\n\n\n\nStalin: Paradoxes of Power, 1878-1928\n\n\nStephen Kotkin\n\n\n\n\nThe Character Gap\n\n\nChristian Miller\n\n\n\n\nExhalation\n\n\nTed Chiang\n\n\n\n\nWealth, Poverty, and Politics\n\n\nThomas Sowell\n\n\n\n\nThe Disappearance of Childhood\n\n\nNiel Postman\n\n\n\n\nMere Christianity\n\n\nC. S. Lewis\n\n\n\n\nStatistical Physics of Particles\n\n\nM. Kardar\n\n\n\n\nIntroduction to Thermal Physics\n\n\nD. Schroeder\n\n\n\n\nOptical Coherence and Quantum Optics\n\n\nI. Mandel and E. Wolf\n\n\n\n\nQuantum Mechanics: Concepts and Applications\n\n\nN. Zettili\n\n\n\n\nModern Quantum Mechanics\n\n\nJ. Sakurai\n\n\n\n\nWhat Is Real?: The Unfinished Quest for the Meaning of Quantum Physics\n\n\nAdam Becker\n\n\n\n\nModern Elementary Particle Physics\n\n\nG. Kane\n\n\n\n\nIntroduction to High Energy Physics\n\n\nD. Perkins\n\n\n\n\nIntroduction to Elementary Particle Physics\n\n\nR. Griffiths\n\n\n\n\nClassical Mechanics\n\n\nJ. Taylor\n\n\n\n\nClassical Mechanics\n\n\nS. Goldstein\n\n\n\n\nIntroduction to Electrodynamics\n\n\nR. Griffiths\n\n\n\n\nClassical Electromagnetism\n\n\nJ. Franklin\n\n\n\n\nClassical Electrodynamics\n\n\nJ. Jackson\n\n\n\n\nAstrophysics in a Nutshell\n\n\nD. Maoz\n\n\n\n\nTheory and Design of Charged Particle Beams\n\n\nM. Reiser\n\n\n\n\nSpace Charge Physics for Particle Accelerators\n\n\nI. Hofmann\n\n\n\n\nRF Linear Accelerators\n\n\nT. Wangler\n\n\n\n\nParticle Accelerator Physics\n\n\nH. Wiedemann\n\n\n\n\nMeasurement and Control of Charged Particle Beams\n\n\nM. Minty and F. Zimmerman\n\n\n\n\nIntroduction to the Physics of High Energy Accelerators\n\n\nD. Edwards and M. Syphers\n\n\n\n\nAccelerator Physics\n\n\nS. Y. Lee\n\n\n\n\nTime and Chance\n\n\nDavid Albert\n\n\n\n\nPhilosophy of Physics: Space and Time\n\n\nTim Maudlin\n\n\n\n\nPhilosophy of Physics: Quantum Theory\n\n\nTim Maudlin\n\n\n\n\nThe Emergent Multiverse\n\n\nDavid Wallace\n\n\n\n\nTaking Pascal’s Wager\n\n\nMichael Rota\n\n\n\n\nFive Proofs of the Existence of God\n\n\nEdward Feser\n\n\n\n\nIs God the Best Explanation of Things? A Dialogue\n\n\nJoshua Rasmussen and Felipe Leon\n\n\n\n\nArguing About Gods\n\n\nGraham Oppy\n\n\n\n\nNecessary Existence\n\n\nAlexander Pruss and Joshua Rasmussen\n\n\n\n\nInfinity, Causation, and Paradox\n\n\nAlexander Pruss\n\n\n\n\nKnowledge and Christian Belief\n\n\nAlvin Plantinga\n\n\n\n\nChange in View\n\n\nGilbert Harman\n\n\n\n\nEnvisioning Information\n\n\nEdward Tufte\n\n\n\n\nMathematics of Classical and Quantum Physics\n\n\nF. Byron and R. Fuller\n\n\n\n\nMathematical Methods for Physicists\n\n\nG. Arfken, F. Weber, and F. Harris\n\n\n\n\nStories of Your Life and Others\n\n\nTed Chiang\n\n\n\n\nThe Road\n\n\nCormac McCarthy\n\n\n\n\nNo Country for Old Men\n\n\nCormac McCarthy\n\n\n\n\nMoby Dick\n\n\nHerman Melville\n\n\n\n\nFahrenheit 451\n\n\nRay Bradbury\n\n\n\n\nCrime and Punishment\n\n\nFyodor Dostoevsky\n\n\n\n\nThe Alchemist\n\n\nPaulo Coelho\n\n\n\n\nPattern Recognition and Machine Learning\n\n\nC. Bishop\n\n\n\n\nNeural Networks and Deep Learning\n\n\nM. Nielsen\n\n\n\n\nElements of Programming Interviews\n\n\nA. Aziz, T. Lee, and A. Prakash\n\n\n\n\nCry, the Beloved Country\n\n\nAlan Paton\n\n\n\n\nAlgorithms\n\n\nR. Sedgewick and K. Wayne\n\n\n\n\n\nNo matching items"
  }
]